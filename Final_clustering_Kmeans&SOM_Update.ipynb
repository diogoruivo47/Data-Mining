{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/diogoruivo47/Data-Mining/blob/main/Final_clustering_Kmeans%26SOM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nYanJt86V9PU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, r2_score\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_w-L3h9ClsV"
   },
   "source": [
    "Optuna was installed for grid search via bayesian probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HHE2jxOG6pb9"
   },
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueK3IdHCzoxK"
   },
   "source": [
    "# current plan to merge the data\n",
    "\n",
    "each DF should include cluster labels (C1 + C2.... etc)\n",
    "a unique Identifier\n",
    "\n",
    "Centroids of cluster\n",
    "\n",
    "formate data for aligbments mean or median of clusters and variability/STDV\n",
    "\n",
    "compare pairwise similary,\n",
    "  check numerical w euclidian cosine similary to see represent directionality\n",
    "    present in a distance matrix with each row and column corresponding to clusters\n",
    "\n",
    "after score silhouette etc and R2 for totaldataset\n",
    "use T-sne\n",
    "\n",
    "start grunt work for report, use labs if needed\n",
    "--------------------------------------------------\n",
    "\n",
    "mathematically speaking the merge via hierarchical works via the profile describing a cluster, meaning, the centroids, the distribution of statistics and cluster proportions.\n",
    "\n",
    "combining these will create your final DF merged non refull clustered blablablabla, so follow steps above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCQjgM-nWuwD"
   },
   "source": [
    "------------------------------------------------------------------------------\n",
    "plan when improving data for clustering in NOV:\n",
    "\n",
    "split numericals from categoricals,\n",
    "\n",
    "reduce feature amounts in preferences and shopping,\n",
    "\n",
    "improve silhouette overall and find ideal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r9nNOls0Vg7l"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_clean2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_JGTfDOnWRVX"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = df.drop(columns=['Unnamed: 0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fBjJ8n_wWVfn"
   },
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "categorical_cols = ['last_promo', 'payment_method', 'customer_region_0', 'customer_region_1', 'customer_region_2', 'customer_region_3']\n",
    "cat_df = df_copy[categorical_cols].copy()\n",
    "numerical_cols = df_copy.columns.difference(categorical_cols)\n",
    "num_df = df_copy[numerical_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvdVKt3cFVP6",
    "outputId": "36087e52-bd4b-447c-e33e-d0f3ef8cdf1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUANTUM FEATURE VERIFICATION:\n",
      "Input shape: (31737, 48)\n",
      "Output shape: (31737, 42)\n",
      "\n",
      "FEATURE CONSERVATION CHECK:\n",
      "All features preserved: False\n"
     ]
    }
   ],
   "source": [
    "def scale_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scale features with QUANTUM-PRECISE type handling.\n",
    "    CRITICAL: Convert categorical types before statistical operations!\n",
    "    \"\"\"\n",
    "    # FUNDAMENTAL STEP: Convert DataFrame to float64 with EXTREME PRECISION\n",
    "    df = df.copy()\n",
    "\n",
    "    # CRITICAL TYPE CONVERSION WITH VERIFICATION\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype.name == 'category':\n",
    "            df[column] = df[column].astype('float64')\n",
    "\n",
    "    scaled_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # 1. Binary features\n",
    "    binary_cols = ['DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6',\n",
    "                    'CLV_Score', 'RFM_Score' , 'Loyalty']\n",
    "    for col in binary_cols:\n",
    "        scaled_df[col] = df[col]\n",
    "\n",
    "    # 2. CUI features\n",
    "    cui_cols = [col for col in df.columns if col.startswith('CUI_')]\n",
    "    for col in cui_cols:\n",
    "        nonzero_mask = df[col] != 0\n",
    "        scaled = np.zeros(len(df))\n",
    "        if nonzero_mask.any():\n",
    "            nonzero_values = df.loc[nonzero_mask, col].values.reshape(-1, 1)\n",
    "            scaled[nonzero_mask] = StandardScaler().fit_transform(nonzero_values).ravel()\n",
    "        scaled_df[col] = scaled\n",
    "\n",
    "    # 3. Ordinal features\n",
    "    ordinal_cols = ['is_chain','Orders_Night', \t'Orders_Dawn',\t'Orders_Morning',\t'Orders_Afternoon'\t,'Orders_Evening'\t,'Orders_Dusk', 'Age_Group']\n",
    "    for col in ordinal_cols:\n",
    "        values = df[col].values.reshape(-1, 1)\n",
    "        scaled_df[col] = MinMaxScaler().fit_transform(values).ravel()\n",
    "\n",
    "    # 4. Continuous features\n",
    "    continuous_cols = ['vendor_count', 'product_count', 'Total_Orders_Per_Client',\n",
    "                      'mnt', 'mnt_Per_Order', 'Items_Per_Order', 'frq', 'rcn',\n",
    "                      'activity']\n",
    "\n",
    "    for col in continuous_cols:\n",
    "        values = df[col].values\n",
    "        if np.std(values) == 0:\n",
    "            scaled_df[col] = values\n",
    "            continue\n",
    "\n",
    "        # MATHEMATICALLY PRECISE skewness handling\n",
    "        if pd.Series(values, dtype='float64').skew() > 1:\n",
    "            min_val = values.min()\n",
    "            if min_val < 0:\n",
    "                values = values - min_val + 1e-10\n",
    "            values = np.log1p(values)\n",
    "\n",
    "        scaled_df[col] = StandardScaler().fit_transform(values.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # QUANTUM VALIDATION\n",
    "    missing_cols = set(df.columns) - set(scaled_df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"VIOLATION OF CONSERVATION OF FEATURES: Missing columns: {missing_cols}\")\n",
    "\n",
    "    return scaled_df\n",
    "\n",
    "# Execute with SUPERNOVA PRECISION\n",
    "num_df = scale_features(num_df)\n",
    "\n",
    "# VALIDATE WITH HADRON COLLIDER PRECISION\n",
    "print(\"\\nQUANTUM FEATURE VERIFICATION:\")\n",
    "print(f\"Input shape: {df.shape}\")\n",
    "print(f\"Output shape: {num_df.shape}\")\n",
    "print(\"\\nFEATURE CONSERVATION CHECK:\")\n",
    "print(\"All features preserved:\", set(df.columns) == set(num_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "dp-CXE5uXQjZ",
    "outputId": "915536bf-1974-4204-a127-d5e991a41ee6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOW_0</th>\n",
       "      <th>DOW_1</th>\n",
       "      <th>DOW_2</th>\n",
       "      <th>DOW_3</th>\n",
       "      <th>DOW_4</th>\n",
       "      <th>DOW_5</th>\n",
       "      <th>DOW_6</th>\n",
       "      <th>CLV_Score</th>\n",
       "      <th>RFM_Score</th>\n",
       "      <th>Loyalty</th>\n",
       "      <th>CUI_American</th>\n",
       "      <th>CUI_Asian</th>\n",
       "      <th>CUI_Beverages</th>\n",
       "      <th>CUI_Cafe</th>\n",
       "      <th>CUI_Chicken_Dishes</th>\n",
       "      <th>CUI_Chinese</th>\n",
       "      <th>CUI_Desserts</th>\n",
       "      <th>CUI_Healthy</th>\n",
       "      <th>CUI_Indian</th>\n",
       "      <th>CUI_Italian</th>\n",
       "      <th>CUI_Japanese</th>\n",
       "      <th>CUI_Noodle_Dishes</th>\n",
       "      <th>CUI_OTHER</th>\n",
       "      <th>CUI_Street_Food/Snacks</th>\n",
       "      <th>CUI_Thai</th>\n",
       "      <th>is_chain</th>\n",
       "      <th>Orders_Night</th>\n",
       "      <th>Orders_Dawn</th>\n",
       "      <th>Orders_Morning</th>\n",
       "      <th>Orders_Afternoon</th>\n",
       "      <th>Orders_Evening</th>\n",
       "      <th>Orders_Dusk</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>vendor_count</th>\n",
       "      <th>product_count</th>\n",
       "      <th>Total_Orders_Per_Client</th>\n",
       "      <th>mnt</th>\n",
       "      <th>mnt_Per_Order</th>\n",
       "      <th>Items_Per_Order</th>\n",
       "      <th>frq</th>\n",
       "      <th>rcn</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>31737.000000</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "      <td>3.173700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.558339</td>\n",
       "      <td>0.570029</td>\n",
       "      <td>0.593503</td>\n",
       "      <td>0.622081</td>\n",
       "      <td>0.680814</td>\n",
       "      <td>0.655796</td>\n",
       "      <td>0.707471</td>\n",
       "      <td>0.066017</td>\n",
       "      <td>0.065291</td>\n",
       "      <td>0.161565</td>\n",
       "      <td>1.432862e-17</td>\n",
       "      <td>-2.149293e-17</td>\n",
       "      <td>7.500136e-18</td>\n",
       "      <td>2.910501e-18</td>\n",
       "      <td>8.955386e-19</td>\n",
       "      <td>3.358270e-18</td>\n",
       "      <td>-1.119423e-18</td>\n",
       "      <td>2.238847e-18</td>\n",
       "      <td>-3.134385e-18</td>\n",
       "      <td>-1.791077e-18</td>\n",
       "      <td>2.955277e-17</td>\n",
       "      <td>-4.477693e-19</td>\n",
       "      <td>1.432862e-17</td>\n",
       "      <td>-7.612078e-18</td>\n",
       "      <td>-4.477693e-18</td>\n",
       "      <td>0.375482</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.015778</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.024953</td>\n",
       "      <td>0.027269</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>0.354350</td>\n",
       "      <td>-2.865724e-16</td>\n",
       "      <td>1.432862e-16</td>\n",
       "      <td>8.597171e-17</td>\n",
       "      <td>3.725441e-16</td>\n",
       "      <td>1.862720e-16</td>\n",
       "      <td>-5.415770e-16</td>\n",
       "      <td>-2.865724e-17</td>\n",
       "      <td>-5.731447e-17</td>\n",
       "      <td>-2.865724e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.015171</td>\n",
       "      <td>1.045785</td>\n",
       "      <td>1.047444</td>\n",
       "      <td>1.071099</td>\n",
       "      <td>1.089727</td>\n",
       "      <td>1.071256</td>\n",
       "      <td>1.169202</td>\n",
       "      <td>0.040343</td>\n",
       "      <td>0.037089</td>\n",
       "      <td>0.228829</td>\n",
       "      <td>5.831157e-01</td>\n",
       "      <td>6.122438e-01</td>\n",
       "      <td>4.136793e-01</td>\n",
       "      <td>2.074671e-01</td>\n",
       "      <td>3.199130e-01</td>\n",
       "      <td>3.331337e-01</td>\n",
       "      <td>2.520398e-01</td>\n",
       "      <td>2.614290e-01</td>\n",
       "      <td>3.296154e-01</td>\n",
       "      <td>4.507507e-01</td>\n",
       "      <td>4.455477e-01</td>\n",
       "      <td>2.648418e-01</td>\n",
       "      <td>4.710219e-01</td>\n",
       "      <td>3.656887e-01</td>\n",
       "      <td>2.737347e-01</td>\n",
       "      <td>0.325380</td>\n",
       "      <td>0.045378</td>\n",
       "      <td>0.045065</td>\n",
       "      <td>0.036090</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>0.047679</td>\n",
       "      <td>0.023012</td>\n",
       "      <td>0.158256</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "      <td>1.000016e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011496</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.624023e-01</td>\n",
       "      <td>-8.129453e-01</td>\n",
       "      <td>-8.003939e-01</td>\n",
       "      <td>-7.191284e-01</td>\n",
       "      <td>-8.038850e-01</td>\n",
       "      <td>-5.848230e-01</td>\n",
       "      <td>-8.514873e-01</td>\n",
       "      <td>-7.593775e-01</td>\n",
       "      <td>-8.314770e-01</td>\n",
       "      <td>-7.621574e-01</td>\n",
       "      <td>-7.985325e-01</td>\n",
       "      <td>-6.980246e-01</td>\n",
       "      <td>-7.789587e-01</td>\n",
       "      <td>-8.852547e-01</td>\n",
       "      <td>-8.938746e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.177017e+00</td>\n",
       "      <td>-1.437495e+00</td>\n",
       "      <td>-1.028486e+00</td>\n",
       "      <td>-3.453503e+00</td>\n",
       "      <td>-1.522614e+00</td>\n",
       "      <td>-8.502307e-01</td>\n",
       "      <td>-9.278250e-01</td>\n",
       "      <td>-1.174464e+00</td>\n",
       "      <td>-1.205454e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>0.031640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.531087e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.177017e+00</td>\n",
       "      <td>-7.825958e-01</td>\n",
       "      <td>-6.457117e-01</td>\n",
       "      <td>-6.840819e-01</td>\n",
       "      <td>-7.672763e-01</td>\n",
       "      <td>-8.502307e-01</td>\n",
       "      <td>-7.265809e-01</td>\n",
       "      <td>-8.257126e-01</td>\n",
       "      <td>-1.103443e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055885</td>\n",
       "      <td>0.056626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-3.174828e-01</td>\n",
       "      <td>-3.179376e-01</td>\n",
       "      <td>-2.629377e-01</td>\n",
       "      <td>1.415649e-02</td>\n",
       "      <td>-2.610392e-01</td>\n",
       "      <td>-2.909657e-01</td>\n",
       "      <td>-5.260947e-01</td>\n",
       "      <td>-3.025859e-01</td>\n",
       "      <td>-4.932618e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093041</td>\n",
       "      <td>0.096610</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>7.654022e-01</td>\n",
       "      <td>8.016196e-01</td>\n",
       "      <td>5.026102e-01</td>\n",
       "      <td>7.354295e-01</td>\n",
       "      <td>4.942987e-01</td>\n",
       "      <td>7.088916e-01</td>\n",
       "      <td>8.017729e-01</td>\n",
       "      <td>6.128859e-01</td>\n",
       "      <td>9.027790e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.186393</td>\n",
       "      <td>0.194064</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.637759e+01</td>\n",
       "      <td>2.695986e+01</td>\n",
       "      <td>1.311239e+01</td>\n",
       "      <td>1.225216e+01</td>\n",
       "      <td>2.368007e+01</td>\n",
       "      <td>3.395328e+01</td>\n",
       "      <td>1.355035e+01</td>\n",
       "      <td>1.356378e+01</td>\n",
       "      <td>1.671358e+01</td>\n",
       "      <td>2.202824e+01</td>\n",
       "      <td>3.746285e+01</td>\n",
       "      <td>1.882122e+01</td>\n",
       "      <td>2.075998e+01</td>\n",
       "      <td>1.300769e+01</td>\n",
       "      <td>1.036615e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.011435e+00</td>\n",
       "      <td>1.816936e+00</td>\n",
       "      <td>2.033706e+00</td>\n",
       "      <td>1.587099e+00</td>\n",
       "      <td>2.386661e+00</td>\n",
       "      <td>2.542052e+00</td>\n",
       "      <td>2.342243e+00</td>\n",
       "      <td>2.748987e+00</td>\n",
       "      <td>1.854884e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DOW_0         DOW_1         DOW_2         DOW_3         DOW_4  \\\n",
       "count  31737.000000  31737.000000  31737.000000  31737.000000  31737.000000   \n",
       "mean       0.558339      0.570029      0.593503      0.622081      0.680814   \n",
       "std        1.015171      1.045785      1.047444      1.071099      1.089727   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "max       16.000000     17.000000     15.000000     17.000000     16.000000   \n",
       "\n",
       "              DOW_5         DOW_6     CLV_Score     RFM_Score       Loyalty  \\\n",
       "count  31737.000000  31737.000000  31737.000000  31737.000000  31737.000000   \n",
       "mean       0.655796      0.707471      0.066017      0.065291      0.161565   \n",
       "std        1.071256      1.169202      0.040343      0.037089      0.228829   \n",
       "min        0.000000      0.000000      0.011496      0.012048      0.000000   \n",
       "25%        0.000000      0.000000      0.030806      0.031640      0.000000   \n",
       "50%        0.000000      0.000000      0.055885      0.056626      0.000000   \n",
       "75%        1.000000      1.000000      0.093041      0.096610      0.333333   \n",
       "max       20.000000     20.000000      0.186393      0.194064      0.833333   \n",
       "\n",
       "       CUI_American     CUI_Asian  CUI_Beverages      CUI_Cafe  \\\n",
       "count  3.173700e+04  3.173700e+04   3.173700e+04  3.173700e+04   \n",
       "mean   1.432862e-17 -2.149293e-17   7.500136e-18  2.910501e-18   \n",
       "std    5.831157e-01  6.122438e-01   4.136793e-01  2.074671e-01   \n",
       "min   -8.624023e-01 -8.129453e-01  -8.003939e-01 -7.191284e-01   \n",
       "25%    0.000000e+00 -5.531087e-02   0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00   0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00   0.000000e+00  0.000000e+00   \n",
       "max    1.637759e+01  2.695986e+01   1.311239e+01  1.225216e+01   \n",
       "\n",
       "       CUI_Chicken_Dishes   CUI_Chinese  CUI_Desserts   CUI_Healthy  \\\n",
       "count        3.173700e+04  3.173700e+04  3.173700e+04  3.173700e+04   \n",
       "mean         8.955386e-19  3.358270e-18 -1.119423e-18  2.238847e-18   \n",
       "std          3.199130e-01  3.331337e-01  2.520398e-01  2.614290e-01   \n",
       "min         -8.038850e-01 -5.848230e-01 -8.514873e-01 -7.593775e-01   \n",
       "25%          0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%          0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%          0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max          2.368007e+01  3.395328e+01  1.355035e+01  1.356378e+01   \n",
       "\n",
       "         CUI_Indian   CUI_Italian  CUI_Japanese  CUI_Noodle_Dishes  \\\n",
       "count  3.173700e+04  3.173700e+04  3.173700e+04       3.173700e+04   \n",
       "mean  -3.134385e-18 -1.791077e-18  2.955277e-17      -4.477693e-19   \n",
       "std    3.296154e-01  4.507507e-01  4.455477e-01       2.648418e-01   \n",
       "min   -8.314770e-01 -7.621574e-01 -7.985325e-01      -6.980246e-01   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00       0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00       0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00       0.000000e+00   \n",
       "max    1.671358e+01  2.202824e+01  3.746285e+01       1.882122e+01   \n",
       "\n",
       "          CUI_OTHER  CUI_Street_Food/Snacks      CUI_Thai      is_chain  \\\n",
       "count  3.173700e+04            3.173700e+04  3.173700e+04  31737.000000   \n",
       "mean   1.432862e-17           -7.612078e-18 -4.477693e-18      0.375482   \n",
       "std    4.710219e-01            3.656887e-01  2.737347e-01      0.325380   \n",
       "min   -7.789587e-01           -8.852547e-01 -8.938746e-01      0.000000   \n",
       "25%    0.000000e+00            0.000000e+00  0.000000e+00      0.166667   \n",
       "50%    0.000000e+00            0.000000e+00  0.000000e+00      0.333333   \n",
       "75%    0.000000e+00            0.000000e+00  0.000000e+00      0.500000   \n",
       "max    2.075998e+01            1.300769e+01  1.036615e+01      1.000000   \n",
       "\n",
       "       Orders_Night   Orders_Dawn  Orders_Morning  Orders_Afternoon  \\\n",
       "count  31737.000000  31737.000000    31737.000000      31737.000000   \n",
       "mean       0.014261      0.015778        0.018593          0.024953   \n",
       "std        0.045378      0.045065        0.036090          0.043672   \n",
       "min        0.000000      0.000000        0.000000          0.000000   \n",
       "25%        0.000000      0.000000        0.000000          0.000000   \n",
       "50%        0.000000      0.000000        0.000000          0.000000   \n",
       "75%        0.000000      0.000000        0.017241          0.023810   \n",
       "max        1.000000      1.000000        1.000000          1.000000   \n",
       "\n",
       "       Orders_Evening   Orders_Dusk     Age_Group  vendor_count  \\\n",
       "count    31737.000000  31737.000000  31737.000000  3.173700e+04   \n",
       "mean         0.027269      0.007183      0.354350 -2.865724e-16   \n",
       "std          0.047679      0.023012      0.158256  1.000016e+00   \n",
       "min          0.000000      0.000000      0.000000 -1.177017e+00   \n",
       "25%          0.000000      0.000000      0.200000 -1.177017e+00   \n",
       "50%          0.020408      0.000000      0.400000 -3.174828e-01   \n",
       "75%          0.040816      0.000000      0.400000  7.654022e-01   \n",
       "max          1.000000      1.000000      1.000000  2.011435e+00   \n",
       "\n",
       "       product_count  Total_Orders_Per_Client           mnt  mnt_Per_Order  \\\n",
       "count   3.173700e+04             3.173700e+04  3.173700e+04   3.173700e+04   \n",
       "mean    1.432862e-16             8.597171e-17  3.725441e-16   1.862720e-16   \n",
       "std     1.000016e+00             1.000016e+00  1.000016e+00   1.000016e+00   \n",
       "min    -1.437495e+00            -1.028486e+00 -3.453503e+00  -1.522614e+00   \n",
       "25%    -7.825958e-01            -6.457117e-01 -6.840819e-01  -7.672763e-01   \n",
       "50%    -3.179376e-01            -2.629377e-01  1.415649e-02  -2.610392e-01   \n",
       "75%     8.016196e-01             5.026102e-01  7.354295e-01   4.942987e-01   \n",
       "max     1.816936e+00             2.033706e+00  1.587099e+00   2.386661e+00   \n",
       "\n",
       "       Items_Per_Order           frq           rcn      activity  \n",
       "count     3.173700e+04  3.173700e+04  3.173700e+04  3.173700e+04  \n",
       "mean     -5.415770e-16 -2.865724e-17 -5.731447e-17 -2.865724e-17  \n",
       "std       1.000016e+00  1.000016e+00  1.000016e+00  1.000016e+00  \n",
       "min      -8.502307e-01 -9.278250e-01 -1.174464e+00 -1.205454e+00  \n",
       "25%      -8.502307e-01 -7.265809e-01 -8.257126e-01 -1.103443e+00  \n",
       "50%      -2.909657e-01 -5.260947e-01 -3.025859e-01 -4.932618e-02  \n",
       "75%       7.088916e-01  8.017729e-01  6.128859e-01  9.027790e-01  \n",
       "max       2.542052e+00  2.342243e+00  2.748987e+00  1.854884e+00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9jMo2ePFXg4R",
    "outputId": "35f2dc24-5ff3-481d-ff09-896c3c9b75e4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOW_0</th>\n",
       "      <th>DOW_1</th>\n",
       "      <th>DOW_2</th>\n",
       "      <th>DOW_3</th>\n",
       "      <th>DOW_4</th>\n",
       "      <th>DOW_5</th>\n",
       "      <th>DOW_6</th>\n",
       "      <th>CLV_Score</th>\n",
       "      <th>RFM_Score</th>\n",
       "      <th>Loyalty</th>\n",
       "      <th>CUI_American</th>\n",
       "      <th>CUI_Asian</th>\n",
       "      <th>CUI_Beverages</th>\n",
       "      <th>CUI_Cafe</th>\n",
       "      <th>CUI_Chicken_Dishes</th>\n",
       "      <th>CUI_Chinese</th>\n",
       "      <th>CUI_Desserts</th>\n",
       "      <th>CUI_Healthy</th>\n",
       "      <th>CUI_Indian</th>\n",
       "      <th>CUI_Italian</th>\n",
       "      <th>CUI_Japanese</th>\n",
       "      <th>CUI_Noodle_Dishes</th>\n",
       "      <th>CUI_OTHER</th>\n",
       "      <th>CUI_Street_Food/Snacks</th>\n",
       "      <th>CUI_Thai</th>\n",
       "      <th>is_chain</th>\n",
       "      <th>Orders_Night</th>\n",
       "      <th>Orders_Dawn</th>\n",
       "      <th>Orders_Morning</th>\n",
       "      <th>Orders_Afternoon</th>\n",
       "      <th>Orders_Evening</th>\n",
       "      <th>Orders_Dusk</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>vendor_count</th>\n",
       "      <th>product_count</th>\n",
       "      <th>Total_Orders_Per_Client</th>\n",
       "      <th>mnt</th>\n",
       "      <th>mnt_Per_Order</th>\n",
       "      <th>Items_Per_Order</th>\n",
       "      <th>frq</th>\n",
       "      <th>rcn</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DOW_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.239451</td>\n",
       "      <td>0.234692</td>\n",
       "      <td>0.225882</td>\n",
       "      <td>0.176707</td>\n",
       "      <td>0.130844</td>\n",
       "      <td>0.144047</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>0.015459</td>\n",
       "      <td>0.309971</td>\n",
       "      <td>-0.007948</td>\n",
       "      <td>0.031864</td>\n",
       "      <td>-0.023641</td>\n",
       "      <td>-0.009666</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>-0.012840</td>\n",
       "      <td>-0.020678</td>\n",
       "      <td>-0.009510</td>\n",
       "      <td>-0.021093</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>-0.032283</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>0.014130</td>\n",
       "      <td>-0.016510</td>\n",
       "      <td>-0.011773</td>\n",
       "      <td>0.380964</td>\n",
       "      <td>0.117614</td>\n",
       "      <td>0.112148</td>\n",
       "      <td>0.289692</td>\n",
       "      <td>0.270555</td>\n",
       "      <td>0.209825</td>\n",
       "      <td>0.166647</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>0.444394</td>\n",
       "      <td>0.467621</td>\n",
       "      <td>0.487078</td>\n",
       "      <td>0.367186</td>\n",
       "      <td>-0.095872</td>\n",
       "      <td>0.090421</td>\n",
       "      <td>-0.078086</td>\n",
       "      <td>-0.241139</td>\n",
       "      <td>0.398360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOW_1</th>\n",
       "      <td>0.239451</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.248535</td>\n",
       "      <td>0.235444</td>\n",
       "      <td>0.185994</td>\n",
       "      <td>0.122919</td>\n",
       "      <td>0.123414</td>\n",
       "      <td>0.052926</td>\n",
       "      <td>0.025526</td>\n",
       "      <td>0.320087</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.017901</td>\n",
       "      <td>-0.018165</td>\n",
       "      <td>-0.005425</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>-0.014745</td>\n",
       "      <td>-0.013502</td>\n",
       "      <td>-0.016770</td>\n",
       "      <td>-0.022564</td>\n",
       "      <td>0.007983</td>\n",
       "      <td>-0.029118</td>\n",
       "      <td>-0.014173</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>-0.017793</td>\n",
       "      <td>-0.010852</td>\n",
       "      <td>0.381061</td>\n",
       "      <td>0.092907</td>\n",
       "      <td>0.101717</td>\n",
       "      <td>0.271741</td>\n",
       "      <td>0.274315</td>\n",
       "      <td>0.222328</td>\n",
       "      <td>0.172557</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.441420</td>\n",
       "      <td>0.467889</td>\n",
       "      <td>0.488111</td>\n",
       "      <td>0.361109</td>\n",
       "      <td>-0.106459</td>\n",
       "      <td>0.090598</td>\n",
       "      <td>-0.072986</td>\n",
       "      <td>-0.251493</td>\n",
       "      <td>0.397150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOW_2</th>\n",
       "      <td>0.234692</td>\n",
       "      <td>0.248535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.242585</td>\n",
       "      <td>0.197536</td>\n",
       "      <td>0.120607</td>\n",
       "      <td>0.135044</td>\n",
       "      <td>0.063361</td>\n",
       "      <td>0.036966</td>\n",
       "      <td>0.336971</td>\n",
       "      <td>-0.002909</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>-0.019424</td>\n",
       "      <td>-0.005220</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>-0.007617</td>\n",
       "      <td>-0.030035</td>\n",
       "      <td>-0.022447</td>\n",
       "      <td>-0.012624</td>\n",
       "      <td>0.014201</td>\n",
       "      <td>-0.019137</td>\n",
       "      <td>-0.030887</td>\n",
       "      <td>0.029338</td>\n",
       "      <td>-0.013291</td>\n",
       "      <td>-0.016498</td>\n",
       "      <td>0.384234</td>\n",
       "      <td>0.089564</td>\n",
       "      <td>0.097099</td>\n",
       "      <td>0.286622</td>\n",
       "      <td>0.278179</td>\n",
       "      <td>0.238674</td>\n",
       "      <td>0.163488</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.449241</td>\n",
       "      <td>0.486109</td>\n",
       "      <td>0.505805</td>\n",
       "      <td>0.377148</td>\n",
       "      <td>-0.101740</td>\n",
       "      <td>0.099674</td>\n",
       "      <td>-0.087117</td>\n",
       "      <td>-0.273226</td>\n",
       "      <td>0.417149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOW_3</th>\n",
       "      <td>0.225882</td>\n",
       "      <td>0.235444</td>\n",
       "      <td>0.242585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204534</td>\n",
       "      <td>0.131171</td>\n",
       "      <td>0.128172</td>\n",
       "      <td>0.094587</td>\n",
       "      <td>0.066596</td>\n",
       "      <td>0.339671</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>0.021530</td>\n",
       "      <td>-0.016312</td>\n",
       "      <td>-0.010115</td>\n",
       "      <td>-0.000787</td>\n",
       "      <td>-0.000660</td>\n",
       "      <td>-0.025462</td>\n",
       "      <td>-0.029278</td>\n",
       "      <td>-0.014567</td>\n",
       "      <td>-0.000934</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.014357</td>\n",
       "      <td>0.008195</td>\n",
       "      <td>-0.008626</td>\n",
       "      <td>-0.015371</td>\n",
       "      <td>0.382928</td>\n",
       "      <td>0.078979</td>\n",
       "      <td>0.087339</td>\n",
       "      <td>0.273337</td>\n",
       "      <td>0.290232</td>\n",
       "      <td>0.260003</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.450414</td>\n",
       "      <td>0.485919</td>\n",
       "      <td>0.506955</td>\n",
       "      <td>0.372713</td>\n",
       "      <td>-0.113150</td>\n",
       "      <td>0.095464</td>\n",
       "      <td>-0.088083</td>\n",
       "      <td>-0.299676</td>\n",
       "      <td>0.421766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOW_4</th>\n",
       "      <td>0.176707</td>\n",
       "      <td>0.185994</td>\n",
       "      <td>0.197536</td>\n",
       "      <td>0.204534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139216</td>\n",
       "      <td>0.126009</td>\n",
       "      <td>0.100607</td>\n",
       "      <td>0.055575</td>\n",
       "      <td>0.334988</td>\n",
       "      <td>-0.005596</td>\n",
       "      <td>0.018773</td>\n",
       "      <td>-0.014867</td>\n",
       "      <td>-0.003749</td>\n",
       "      <td>-0.004226</td>\n",
       "      <td>-0.006766</td>\n",
       "      <td>-0.014771</td>\n",
       "      <td>-0.009122</td>\n",
       "      <td>-0.013820</td>\n",
       "      <td>-0.003563</td>\n",
       "      <td>-0.015573</td>\n",
       "      <td>-0.009558</td>\n",
       "      <td>0.020205</td>\n",
       "      <td>-0.007211</td>\n",
       "      <td>-0.007092</td>\n",
       "      <td>0.364731</td>\n",
       "      <td>0.051334</td>\n",
       "      <td>0.053484</td>\n",
       "      <td>0.248499</td>\n",
       "      <td>0.274976</td>\n",
       "      <td>0.296619</td>\n",
       "      <td>0.166407</td>\n",
       "      <td>-0.002040</td>\n",
       "      <td>0.429584</td>\n",
       "      <td>0.470255</td>\n",
       "      <td>0.489599</td>\n",
       "      <td>0.359802</td>\n",
       "      <td>-0.107482</td>\n",
       "      <td>0.095874</td>\n",
       "      <td>-0.091739</td>\n",
       "      <td>-0.302244</td>\n",
       "      <td>0.404916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOW_5</th>\n",
       "      <td>0.130844</td>\n",
       "      <td>0.122919</td>\n",
       "      <td>0.120607</td>\n",
       "      <td>0.131171</td>\n",
       "      <td>0.139216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.190266</td>\n",
       "      <td>0.016857</td>\n",
       "      <td>-0.007242</td>\n",
       "      <td>0.271503</td>\n",
       "      <td>-0.008867</td>\n",
       "      <td>0.019214</td>\n",
       "      <td>-0.036072</td>\n",
       "      <td>-0.011699</td>\n",
       "      <td>-0.001307</td>\n",
       "      <td>-0.020450</td>\n",
       "      <td>-0.026734</td>\n",
       "      <td>-0.011719</td>\n",
       "      <td>-0.006826</td>\n",
       "      <td>-0.002263</td>\n",
       "      <td>-0.011149</td>\n",
       "      <td>-0.019269</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>-0.022651</td>\n",
       "      <td>-0.011761</td>\n",
       "      <td>0.335270</td>\n",
       "      <td>0.072311</td>\n",
       "      <td>0.079144</td>\n",
       "      <td>0.224566</td>\n",
       "      <td>0.261760</td>\n",
       "      <td>0.264613</td>\n",
       "      <td>0.169846</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>0.409335</td>\n",
       "      <td>0.427865</td>\n",
       "      <td>0.441399</td>\n",
       "      <td>0.328541</td>\n",
       "      <td>-0.091810</td>\n",
       "      <td>0.099923</td>\n",
       "      <td>-0.076712</td>\n",
       "      <td>-0.206894</td>\n",
       "      <td>0.356761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOW_6</th>\n",
       "      <td>0.144047</td>\n",
       "      <td>0.123414</td>\n",
       "      <td>0.135044</td>\n",
       "      <td>0.128172</td>\n",
       "      <td>0.126009</td>\n",
       "      <td>0.190266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004533</td>\n",
       "      <td>-0.029825</td>\n",
       "      <td>0.284548</td>\n",
       "      <td>-0.016091</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>-0.028407</td>\n",
       "      <td>-0.015837</td>\n",
       "      <td>-0.004361</td>\n",
       "      <td>-0.014939</td>\n",
       "      <td>-0.022278</td>\n",
       "      <td>-0.023250</td>\n",
       "      <td>-0.016172</td>\n",
       "      <td>-0.005741</td>\n",
       "      <td>-0.019905</td>\n",
       "      <td>-0.018516</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>-0.013440</td>\n",
       "      <td>-0.012242</td>\n",
       "      <td>0.347896</td>\n",
       "      <td>0.057427</td>\n",
       "      <td>0.069818</td>\n",
       "      <td>0.238807</td>\n",
       "      <td>0.300375</td>\n",
       "      <td>0.267745</td>\n",
       "      <td>0.163296</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.417778</td>\n",
       "      <td>0.445346</td>\n",
       "      <td>0.457593</td>\n",
       "      <td>0.332949</td>\n",
       "      <td>-0.102645</td>\n",
       "      <td>0.110594</td>\n",
       "      <td>-0.103022</td>\n",
       "      <td>-0.220279</td>\n",
       "      <td>0.386163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLV_Score</th>\n",
       "      <td>0.041674</td>\n",
       "      <td>0.052926</td>\n",
       "      <td>0.063361</td>\n",
       "      <td>0.094587</td>\n",
       "      <td>0.100607</td>\n",
       "      <td>0.016857</td>\n",
       "      <td>-0.004533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989184</td>\n",
       "      <td>-0.019965</td>\n",
       "      <td>0.109149</td>\n",
       "      <td>0.082841</td>\n",
       "      <td>0.078898</td>\n",
       "      <td>0.024169</td>\n",
       "      <td>0.068521</td>\n",
       "      <td>0.073655</td>\n",
       "      <td>0.042226</td>\n",
       "      <td>0.039968</td>\n",
       "      <td>0.053582</td>\n",
       "      <td>0.051527</td>\n",
       "      <td>0.067097</td>\n",
       "      <td>0.045779</td>\n",
       "      <td>0.111758</td>\n",
       "      <td>0.055347</td>\n",
       "      <td>0.028161</td>\n",
       "      <td>-0.086399</td>\n",
       "      <td>0.036009</td>\n",
       "      <td>0.043632</td>\n",
       "      <td>0.050462</td>\n",
       "      <td>0.008237</td>\n",
       "      <td>-0.041549</td>\n",
       "      <td>-0.022636</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>-0.107176</td>\n",
       "      <td>-0.043445</td>\n",
       "      <td>-0.106557</td>\n",
       "      <td>0.119324</td>\n",
       "      <td>0.223013</td>\n",
       "      <td>0.125428</td>\n",
       "      <td>0.574515</td>\n",
       "      <td>-0.406842</td>\n",
       "      <td>-0.183989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFM_Score</th>\n",
       "      <td>0.015459</td>\n",
       "      <td>0.025526</td>\n",
       "      <td>0.036966</td>\n",
       "      <td>0.066596</td>\n",
       "      <td>0.055575</td>\n",
       "      <td>-0.007242</td>\n",
       "      <td>-0.029825</td>\n",
       "      <td>0.989184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.053272</td>\n",
       "      <td>0.105438</td>\n",
       "      <td>0.067619</td>\n",
       "      <td>0.077297</td>\n",
       "      <td>0.021920</td>\n",
       "      <td>0.067819</td>\n",
       "      <td>0.071444</td>\n",
       "      <td>0.038678</td>\n",
       "      <td>0.038424</td>\n",
       "      <td>0.052040</td>\n",
       "      <td>0.046542</td>\n",
       "      <td>0.063801</td>\n",
       "      <td>0.045087</td>\n",
       "      <td>0.107312</td>\n",
       "      <td>0.045112</td>\n",
       "      <td>0.027352</td>\n",
       "      <td>-0.124700</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.020149</td>\n",
       "      <td>0.021244</td>\n",
       "      <td>-0.016595</td>\n",
       "      <td>-0.058072</td>\n",
       "      <td>-0.031836</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>-0.162246</td>\n",
       "      <td>-0.101710</td>\n",
       "      <td>-0.165023</td>\n",
       "      <td>0.069739</td>\n",
       "      <td>0.214071</td>\n",
       "      <td>0.102301</td>\n",
       "      <td>0.620062</td>\n",
       "      <td>-0.364907</td>\n",
       "      <td>-0.247917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loyalty</th>\n",
       "      <td>0.309971</td>\n",
       "      <td>0.320087</td>\n",
       "      <td>0.336971</td>\n",
       "      <td>0.339671</td>\n",
       "      <td>0.334988</td>\n",
       "      <td>0.271503</td>\n",
       "      <td>0.284548</td>\n",
       "      <td>-0.019965</td>\n",
       "      <td>-0.053272</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100623</td>\n",
       "      <td>0.088698</td>\n",
       "      <td>0.068987</td>\n",
       "      <td>0.022231</td>\n",
       "      <td>0.045568</td>\n",
       "      <td>0.043057</td>\n",
       "      <td>0.025652</td>\n",
       "      <td>0.037066</td>\n",
       "      <td>0.046649</td>\n",
       "      <td>0.100662</td>\n",
       "      <td>0.057946</td>\n",
       "      <td>0.025661</td>\n",
       "      <td>0.078312</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.048223</td>\n",
       "      <td>0.384554</td>\n",
       "      <td>0.013339</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.254087</td>\n",
       "      <td>0.335086</td>\n",
       "      <td>0.360599</td>\n",
       "      <td>0.128033</td>\n",
       "      <td>0.009922</td>\n",
       "      <td>0.238197</td>\n",
       "      <td>0.581853</td>\n",
       "      <td>0.616230</td>\n",
       "      <td>0.418518</td>\n",
       "      <td>-0.158522</td>\n",
       "      <td>0.084251</td>\n",
       "      <td>-0.171074</td>\n",
       "      <td>-0.305183</td>\n",
       "      <td>0.495155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_American</th>\n",
       "      <td>-0.007948</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>-0.002909</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>-0.005596</td>\n",
       "      <td>-0.008867</td>\n",
       "      <td>-0.016091</td>\n",
       "      <td>0.109149</td>\n",
       "      <td>0.105438</td>\n",
       "      <td>0.100623</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044933</td>\n",
       "      <td>0.067861</td>\n",
       "      <td>0.019177</td>\n",
       "      <td>0.042542</td>\n",
       "      <td>0.045970</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>0.023113</td>\n",
       "      <td>0.049893</td>\n",
       "      <td>0.049653</td>\n",
       "      <td>0.044878</td>\n",
       "      <td>0.039889</td>\n",
       "      <td>0.067226</td>\n",
       "      <td>0.011723</td>\n",
       "      <td>0.030616</td>\n",
       "      <td>-0.095541</td>\n",
       "      <td>0.062320</td>\n",
       "      <td>0.059124</td>\n",
       "      <td>-0.004908</td>\n",
       "      <td>-0.037705</td>\n",
       "      <td>-0.063659</td>\n",
       "      <td>-0.098646</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>-0.111250</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>-0.044018</td>\n",
       "      <td>0.151092</td>\n",
       "      <td>0.259137</td>\n",
       "      <td>0.139976</td>\n",
       "      <td>0.074937</td>\n",
       "      <td>-0.001536</td>\n",
       "      <td>-0.047178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Asian</th>\n",
       "      <td>0.031864</td>\n",
       "      <td>0.017901</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.021530</td>\n",
       "      <td>0.018773</td>\n",
       "      <td>0.019214</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>0.082841</td>\n",
       "      <td>0.067619</td>\n",
       "      <td>0.088698</td>\n",
       "      <td>0.044933</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027021</td>\n",
       "      <td>0.018364</td>\n",
       "      <td>0.012362</td>\n",
       "      <td>0.021255</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.016458</td>\n",
       "      <td>0.032392</td>\n",
       "      <td>0.020247</td>\n",
       "      <td>0.036830</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.017991</td>\n",
       "      <td>0.030551</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>-0.013505</td>\n",
       "      <td>0.124434</td>\n",
       "      <td>0.084652</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>-0.024523</td>\n",
       "      <td>-0.078138</td>\n",
       "      <td>-0.024145</td>\n",
       "      <td>0.011969</td>\n",
       "      <td>-0.025968</td>\n",
       "      <td>0.087398</td>\n",
       "      <td>0.026927</td>\n",
       "      <td>0.134996</td>\n",
       "      <td>0.173878</td>\n",
       "      <td>0.198714</td>\n",
       "      <td>0.027655</td>\n",
       "      <td>-0.014886</td>\n",
       "      <td>0.009899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Beverages</th>\n",
       "      <td>-0.023641</td>\n",
       "      <td>-0.018165</td>\n",
       "      <td>-0.019424</td>\n",
       "      <td>-0.016312</td>\n",
       "      <td>-0.014867</td>\n",
       "      <td>-0.036072</td>\n",
       "      <td>-0.028407</td>\n",
       "      <td>0.078898</td>\n",
       "      <td>0.077297</td>\n",
       "      <td>0.068987</td>\n",
       "      <td>0.067861</td>\n",
       "      <td>0.027021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>0.037096</td>\n",
       "      <td>0.040950</td>\n",
       "      <td>0.037508</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>0.027950</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>0.044909</td>\n",
       "      <td>0.038532</td>\n",
       "      <td>0.060692</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.023358</td>\n",
       "      <td>-0.122599</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.050755</td>\n",
       "      <td>-0.016675</td>\n",
       "      <td>-0.063277</td>\n",
       "      <td>-0.055467</td>\n",
       "      <td>-0.104413</td>\n",
       "      <td>-0.005341</td>\n",
       "      <td>-0.115958</td>\n",
       "      <td>-0.034609</td>\n",
       "      <td>-0.066371</td>\n",
       "      <td>0.103059</td>\n",
       "      <td>0.217276</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.056381</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>-0.061953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Cafe</th>\n",
       "      <td>-0.009666</td>\n",
       "      <td>-0.005425</td>\n",
       "      <td>-0.005220</td>\n",
       "      <td>-0.010115</td>\n",
       "      <td>-0.003749</td>\n",
       "      <td>-0.011699</td>\n",
       "      <td>-0.015837</td>\n",
       "      <td>0.024169</td>\n",
       "      <td>0.021920</td>\n",
       "      <td>0.022231</td>\n",
       "      <td>0.019177</td>\n",
       "      <td>0.018364</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.011776</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.047521</td>\n",
       "      <td>0.013547</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.023686</td>\n",
       "      <td>-0.001544</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>-0.033732</td>\n",
       "      <td>0.035755</td>\n",
       "      <td>0.026209</td>\n",
       "      <td>-0.023953</td>\n",
       "      <td>-0.019539</td>\n",
       "      <td>-0.047638</td>\n",
       "      <td>-0.006549</td>\n",
       "      <td>-0.003634</td>\n",
       "      <td>-0.050680</td>\n",
       "      <td>-0.019424</td>\n",
       "      <td>-0.030914</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.022728</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>-0.025276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Chicken_Dishes</th>\n",
       "      <td>0.003849</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>-0.000787</td>\n",
       "      <td>-0.004226</td>\n",
       "      <td>-0.001307</td>\n",
       "      <td>-0.004361</td>\n",
       "      <td>0.068521</td>\n",
       "      <td>0.067819</td>\n",
       "      <td>0.045568</td>\n",
       "      <td>0.042542</td>\n",
       "      <td>0.012362</td>\n",
       "      <td>0.037096</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038158</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>0.017962</td>\n",
       "      <td>0.007127</td>\n",
       "      <td>0.028855</td>\n",
       "      <td>0.025071</td>\n",
       "      <td>0.059971</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>-0.074565</td>\n",
       "      <td>0.027999</td>\n",
       "      <td>0.047501</td>\n",
       "      <td>0.014607</td>\n",
       "      <td>-0.028598</td>\n",
       "      <td>-0.034149</td>\n",
       "      <td>-0.064339</td>\n",
       "      <td>-0.002565</td>\n",
       "      <td>-0.056442</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>-0.026319</td>\n",
       "      <td>0.116664</td>\n",
       "      <td>0.173162</td>\n",
       "      <td>0.078059</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>-0.010467</td>\n",
       "      <td>-0.025297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Chinese</th>\n",
       "      <td>-0.012840</td>\n",
       "      <td>-0.014745</td>\n",
       "      <td>-0.007617</td>\n",
       "      <td>-0.000660</td>\n",
       "      <td>-0.006766</td>\n",
       "      <td>-0.020450</td>\n",
       "      <td>-0.014939</td>\n",
       "      <td>0.073655</td>\n",
       "      <td>0.071444</td>\n",
       "      <td>0.043057</td>\n",
       "      <td>0.045970</td>\n",
       "      <td>0.021255</td>\n",
       "      <td>0.040950</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.038158</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>0.010143</td>\n",
       "      <td>0.028586</td>\n",
       "      <td>0.021004</td>\n",
       "      <td>0.032553</td>\n",
       "      <td>0.045482</td>\n",
       "      <td>0.074387</td>\n",
       "      <td>0.018343</td>\n",
       "      <td>0.011702</td>\n",
       "      <td>-0.092873</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.029808</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>-0.037329</td>\n",
       "      <td>-0.049854</td>\n",
       "      <td>-0.078869</td>\n",
       "      <td>-0.004241</td>\n",
       "      <td>-0.080605</td>\n",
       "      <td>-0.021622</td>\n",
       "      <td>-0.048720</td>\n",
       "      <td>0.106115</td>\n",
       "      <td>0.189395</td>\n",
       "      <td>0.071722</td>\n",
       "      <td>0.033248</td>\n",
       "      <td>-0.005776</td>\n",
       "      <td>-0.028910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Desserts</th>\n",
       "      <td>-0.020678</td>\n",
       "      <td>-0.013502</td>\n",
       "      <td>-0.030035</td>\n",
       "      <td>-0.025462</td>\n",
       "      <td>-0.014771</td>\n",
       "      <td>-0.026734</td>\n",
       "      <td>-0.022278</td>\n",
       "      <td>0.042226</td>\n",
       "      <td>0.038678</td>\n",
       "      <td>0.025652</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.037508</td>\n",
       "      <td>0.011776</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.014390</td>\n",
       "      <td>0.011883</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>0.028446</td>\n",
       "      <td>0.031545</td>\n",
       "      <td>0.006628</td>\n",
       "      <td>-0.070229</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.014950</td>\n",
       "      <td>-0.013392</td>\n",
       "      <td>-0.038499</td>\n",
       "      <td>-0.039750</td>\n",
       "      <td>-0.047355</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>-0.075052</td>\n",
       "      <td>-0.028531</td>\n",
       "      <td>-0.049974</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>0.113834</td>\n",
       "      <td>0.051868</td>\n",
       "      <td>0.034559</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>-0.045942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Healthy</th>\n",
       "      <td>-0.009510</td>\n",
       "      <td>-0.016770</td>\n",
       "      <td>-0.022447</td>\n",
       "      <td>-0.029278</td>\n",
       "      <td>-0.009122</td>\n",
       "      <td>-0.011719</td>\n",
       "      <td>-0.023250</td>\n",
       "      <td>0.039968</td>\n",
       "      <td>0.038424</td>\n",
       "      <td>0.037066</td>\n",
       "      <td>0.023113</td>\n",
       "      <td>0.016458</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>0.010143</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010665</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>0.018552</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>0.024644</td>\n",
       "      <td>-0.000496</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>-0.063150</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>-0.022495</td>\n",
       "      <td>-0.036976</td>\n",
       "      <td>-0.041040</td>\n",
       "      <td>-0.026871</td>\n",
       "      <td>-0.010762</td>\n",
       "      <td>-0.076371</td>\n",
       "      <td>-0.023637</td>\n",
       "      <td>-0.043395</td>\n",
       "      <td>0.033129</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>0.043988</td>\n",
       "      <td>0.037996</td>\n",
       "      <td>0.015912</td>\n",
       "      <td>-0.042946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Indian</th>\n",
       "      <td>-0.021093</td>\n",
       "      <td>-0.022564</td>\n",
       "      <td>-0.012624</td>\n",
       "      <td>-0.014567</td>\n",
       "      <td>-0.013820</td>\n",
       "      <td>-0.006826</td>\n",
       "      <td>-0.016172</td>\n",
       "      <td>0.053582</td>\n",
       "      <td>0.052040</td>\n",
       "      <td>0.046649</td>\n",
       "      <td>0.049893</td>\n",
       "      <td>0.032392</td>\n",
       "      <td>0.027950</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.017962</td>\n",
       "      <td>0.028586</td>\n",
       "      <td>0.014390</td>\n",
       "      <td>0.010665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040446</td>\n",
       "      <td>0.016559</td>\n",
       "      <td>0.023799</td>\n",
       "      <td>0.057851</td>\n",
       "      <td>0.019705</td>\n",
       "      <td>0.025443</td>\n",
       "      <td>-0.091053</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.033242</td>\n",
       "      <td>0.006460</td>\n",
       "      <td>-0.030659</td>\n",
       "      <td>-0.037663</td>\n",
       "      <td>-0.106213</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>-0.075448</td>\n",
       "      <td>-0.011355</td>\n",
       "      <td>-0.040390</td>\n",
       "      <td>0.102434</td>\n",
       "      <td>0.185433</td>\n",
       "      <td>0.080411</td>\n",
       "      <td>0.029767</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>-0.034492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Italian</th>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.007983</td>\n",
       "      <td>0.014201</td>\n",
       "      <td>-0.000934</td>\n",
       "      <td>-0.003563</td>\n",
       "      <td>-0.002263</td>\n",
       "      <td>-0.005741</td>\n",
       "      <td>0.051527</td>\n",
       "      <td>0.046542</td>\n",
       "      <td>0.100662</td>\n",
       "      <td>0.049653</td>\n",
       "      <td>0.020247</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>0.047521</td>\n",
       "      <td>0.007127</td>\n",
       "      <td>0.021004</td>\n",
       "      <td>0.011883</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>0.040446</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026348</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.028960</td>\n",
       "      <td>0.008926</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>-0.074735</td>\n",
       "      <td>0.051014</td>\n",
       "      <td>0.064033</td>\n",
       "      <td>0.023730</td>\n",
       "      <td>-0.010916</td>\n",
       "      <td>-0.073522</td>\n",
       "      <td>-0.069623</td>\n",
       "      <td>-0.009614</td>\n",
       "      <td>-0.069048</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.133291</td>\n",
       "      <td>0.188213</td>\n",
       "      <td>0.121660</td>\n",
       "      <td>0.019594</td>\n",
       "      <td>-0.004616</td>\n",
       "      <td>-0.003151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Japanese</th>\n",
       "      <td>-0.032283</td>\n",
       "      <td>-0.029118</td>\n",
       "      <td>-0.019137</td>\n",
       "      <td>-0.026389</td>\n",
       "      <td>-0.015573</td>\n",
       "      <td>-0.011149</td>\n",
       "      <td>-0.019905</td>\n",
       "      <td>0.067097</td>\n",
       "      <td>0.063801</td>\n",
       "      <td>0.057946</td>\n",
       "      <td>0.044878</td>\n",
       "      <td>0.036830</td>\n",
       "      <td>0.044909</td>\n",
       "      <td>0.013547</td>\n",
       "      <td>0.028855</td>\n",
       "      <td>0.032553</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>0.018552</td>\n",
       "      <td>0.016559</td>\n",
       "      <td>0.026348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024674</td>\n",
       "      <td>0.044269</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>0.028720</td>\n",
       "      <td>-0.088860</td>\n",
       "      <td>0.012773</td>\n",
       "      <td>0.035261</td>\n",
       "      <td>-0.018262</td>\n",
       "      <td>-0.052705</td>\n",
       "      <td>-0.061768</td>\n",
       "      <td>-0.058590</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>-0.108646</td>\n",
       "      <td>-0.024791</td>\n",
       "      <td>-0.062809</td>\n",
       "      <td>0.081741</td>\n",
       "      <td>0.187344</td>\n",
       "      <td>0.099722</td>\n",
       "      <td>0.055034</td>\n",
       "      <td>0.027812</td>\n",
       "      <td>-0.059256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Noodle_Dishes</th>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.014173</td>\n",
       "      <td>-0.030887</td>\n",
       "      <td>-0.014357</td>\n",
       "      <td>-0.009558</td>\n",
       "      <td>-0.019269</td>\n",
       "      <td>-0.018516</td>\n",
       "      <td>0.045779</td>\n",
       "      <td>0.045087</td>\n",
       "      <td>0.025661</td>\n",
       "      <td>0.039889</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.038532</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.025071</td>\n",
       "      <td>0.045482</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>0.023799</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.024674</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049150</td>\n",
       "      <td>0.014787</td>\n",
       "      <td>0.025630</td>\n",
       "      <td>-0.083204</td>\n",
       "      <td>0.013677</td>\n",
       "      <td>0.039217</td>\n",
       "      <td>-0.011507</td>\n",
       "      <td>-0.023583</td>\n",
       "      <td>-0.054367</td>\n",
       "      <td>-0.071455</td>\n",
       "      <td>-0.006460</td>\n",
       "      <td>-0.075103</td>\n",
       "      <td>-0.017732</td>\n",
       "      <td>-0.049222</td>\n",
       "      <td>0.091998</td>\n",
       "      <td>0.175486</td>\n",
       "      <td>0.072472</td>\n",
       "      <td>0.031145</td>\n",
       "      <td>0.014812</td>\n",
       "      <td>-0.043722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_OTHER</th>\n",
       "      <td>0.014130</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.029338</td>\n",
       "      <td>0.008195</td>\n",
       "      <td>0.020205</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>0.111758</td>\n",
       "      <td>0.107312</td>\n",
       "      <td>0.078312</td>\n",
       "      <td>0.067226</td>\n",
       "      <td>0.017991</td>\n",
       "      <td>0.060692</td>\n",
       "      <td>0.023686</td>\n",
       "      <td>0.059971</td>\n",
       "      <td>0.074387</td>\n",
       "      <td>0.028446</td>\n",
       "      <td>0.024644</td>\n",
       "      <td>0.057851</td>\n",
       "      <td>0.028960</td>\n",
       "      <td>0.044269</td>\n",
       "      <td>0.049150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.034475</td>\n",
       "      <td>-0.100366</td>\n",
       "      <td>0.039770</td>\n",
       "      <td>0.076726</td>\n",
       "      <td>0.034570</td>\n",
       "      <td>-0.021786</td>\n",
       "      <td>-0.040180</td>\n",
       "      <td>-0.090372</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>-0.066252</td>\n",
       "      <td>0.019656</td>\n",
       "      <td>-0.022687</td>\n",
       "      <td>0.199450</td>\n",
       "      <td>0.271743</td>\n",
       "      <td>0.126142</td>\n",
       "      <td>0.060305</td>\n",
       "      <td>-0.019620</td>\n",
       "      <td>-0.014624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Street_Food/Snacks</th>\n",
       "      <td>-0.016510</td>\n",
       "      <td>-0.017793</td>\n",
       "      <td>-0.013291</td>\n",
       "      <td>-0.008626</td>\n",
       "      <td>-0.007211</td>\n",
       "      <td>-0.022651</td>\n",
       "      <td>-0.013440</td>\n",
       "      <td>0.055347</td>\n",
       "      <td>0.045112</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.011723</td>\n",
       "      <td>0.030551</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>-0.001544</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>0.018343</td>\n",
       "      <td>0.031545</td>\n",
       "      <td>-0.000496</td>\n",
       "      <td>0.019705</td>\n",
       "      <td>0.008926</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>0.014787</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>-0.032380</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>-0.009778</td>\n",
       "      <td>-0.009926</td>\n",
       "      <td>-0.034443</td>\n",
       "      <td>-0.046950</td>\n",
       "      <td>-0.000392</td>\n",
       "      <td>-0.046854</td>\n",
       "      <td>0.032351</td>\n",
       "      <td>-0.026559</td>\n",
       "      <td>0.075274</td>\n",
       "      <td>0.133339</td>\n",
       "      <td>0.147282</td>\n",
       "      <td>0.018881</td>\n",
       "      <td>0.007585</td>\n",
       "      <td>-0.026195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUI_Thai</th>\n",
       "      <td>-0.011773</td>\n",
       "      <td>-0.010852</td>\n",
       "      <td>-0.016498</td>\n",
       "      <td>-0.015371</td>\n",
       "      <td>-0.007092</td>\n",
       "      <td>-0.011761</td>\n",
       "      <td>-0.012242</td>\n",
       "      <td>0.028161</td>\n",
       "      <td>0.027352</td>\n",
       "      <td>0.048223</td>\n",
       "      <td>0.030616</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>0.023358</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>0.011702</td>\n",
       "      <td>0.006628</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.025443</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>0.028720</td>\n",
       "      <td>0.025630</td>\n",
       "      <td>0.034475</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.048794</td>\n",
       "      <td>0.022438</td>\n",
       "      <td>0.034811</td>\n",
       "      <td>-0.011085</td>\n",
       "      <td>-0.030789</td>\n",
       "      <td>-0.041215</td>\n",
       "      <td>-0.058503</td>\n",
       "      <td>-0.009655</td>\n",
       "      <td>-0.063870</td>\n",
       "      <td>-0.011918</td>\n",
       "      <td>-0.030759</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.113304</td>\n",
       "      <td>0.051293</td>\n",
       "      <td>0.015079</td>\n",
       "      <td>0.011523</td>\n",
       "      <td>-0.020498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_chain</th>\n",
       "      <td>0.380964</td>\n",
       "      <td>0.381061</td>\n",
       "      <td>0.384234</td>\n",
       "      <td>0.382928</td>\n",
       "      <td>0.364731</td>\n",
       "      <td>0.335270</td>\n",
       "      <td>0.347896</td>\n",
       "      <td>-0.086399</td>\n",
       "      <td>-0.124700</td>\n",
       "      <td>0.384554</td>\n",
       "      <td>-0.095541</td>\n",
       "      <td>-0.013505</td>\n",
       "      <td>-0.122599</td>\n",
       "      <td>-0.033732</td>\n",
       "      <td>-0.074565</td>\n",
       "      <td>-0.092873</td>\n",
       "      <td>-0.070229</td>\n",
       "      <td>-0.063150</td>\n",
       "      <td>-0.091053</td>\n",
       "      <td>-0.074735</td>\n",
       "      <td>-0.088860</td>\n",
       "      <td>-0.083204</td>\n",
       "      <td>-0.100366</td>\n",
       "      <td>-0.032380</td>\n",
       "      <td>-0.048794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068228</td>\n",
       "      <td>0.083559</td>\n",
       "      <td>0.373017</td>\n",
       "      <td>0.391147</td>\n",
       "      <td>0.367363</td>\n",
       "      <td>0.288315</td>\n",
       "      <td>0.004309</td>\n",
       "      <td>0.704662</td>\n",
       "      <td>0.675685</td>\n",
       "      <td>0.727238</td>\n",
       "      <td>0.359597</td>\n",
       "      <td>-0.355419</td>\n",
       "      <td>0.084938</td>\n",
       "      <td>-0.239551</td>\n",
       "      <td>-0.333840</td>\n",
       "      <td>0.601557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orders_Night</th>\n",
       "      <td>0.117614</td>\n",
       "      <td>0.092907</td>\n",
       "      <td>0.089564</td>\n",
       "      <td>0.078979</td>\n",
       "      <td>0.051334</td>\n",
       "      <td>0.072311</td>\n",
       "      <td>0.057427</td>\n",
       "      <td>0.036009</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.013339</td>\n",
       "      <td>0.062320</td>\n",
       "      <td>0.124434</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.035755</td>\n",
       "      <td>0.027999</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.051014</td>\n",
       "      <td>0.012773</td>\n",
       "      <td>0.013677</td>\n",
       "      <td>0.039770</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.022438</td>\n",
       "      <td>0.068228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.261559</td>\n",
       "      <td>0.019263</td>\n",
       "      <td>-0.114402</td>\n",
       "      <td>-0.261940</td>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.186750</td>\n",
       "      <td>0.169706</td>\n",
       "      <td>0.156379</td>\n",
       "      <td>0.266130</td>\n",
       "      <td>0.206395</td>\n",
       "      <td>0.102970</td>\n",
       "      <td>-0.059586</td>\n",
       "      <td>-0.086558</td>\n",
       "      <td>0.127563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orders_Dawn</th>\n",
       "      <td>0.112148</td>\n",
       "      <td>0.101717</td>\n",
       "      <td>0.097099</td>\n",
       "      <td>0.087339</td>\n",
       "      <td>0.053484</td>\n",
       "      <td>0.079144</td>\n",
       "      <td>0.069818</td>\n",
       "      <td>0.043632</td>\n",
       "      <td>0.020149</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.059124</td>\n",
       "      <td>0.084652</td>\n",
       "      <td>0.050755</td>\n",
       "      <td>0.026209</td>\n",
       "      <td>0.047501</td>\n",
       "      <td>0.029808</td>\n",
       "      <td>0.014950</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>0.033242</td>\n",
       "      <td>0.064033</td>\n",
       "      <td>0.035261</td>\n",
       "      <td>0.039217</td>\n",
       "      <td>0.076726</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.034811</td>\n",
       "      <td>0.083559</td>\n",
       "      <td>0.261559</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104039</td>\n",
       "      <td>-0.097039</td>\n",
       "      <td>-0.289791</td>\n",
       "      <td>-0.086220</td>\n",
       "      <td>-0.002274</td>\n",
       "      <td>0.205403</td>\n",
       "      <td>0.168363</td>\n",
       "      <td>0.166701</td>\n",
       "      <td>0.310477</td>\n",
       "      <td>0.260687</td>\n",
       "      <td>0.074402</td>\n",
       "      <td>-0.073573</td>\n",
       "      <td>-0.104074</td>\n",
       "      <td>0.146064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orders_Morning</th>\n",
       "      <td>0.289692</td>\n",
       "      <td>0.271741</td>\n",
       "      <td>0.286622</td>\n",
       "      <td>0.273337</td>\n",
       "      <td>0.248499</td>\n",
       "      <td>0.224566</td>\n",
       "      <td>0.238807</td>\n",
       "      <td>0.050462</td>\n",
       "      <td>0.021244</td>\n",
       "      <td>0.254087</td>\n",
       "      <td>-0.004908</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>-0.016675</td>\n",
       "      <td>-0.023953</td>\n",
       "      <td>0.014607</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>-0.013392</td>\n",
       "      <td>-0.022495</td>\n",
       "      <td>0.006460</td>\n",
       "      <td>0.023730</td>\n",
       "      <td>-0.018262</td>\n",
       "      <td>-0.011507</td>\n",
       "      <td>0.034570</td>\n",
       "      <td>-0.009778</td>\n",
       "      <td>-0.011085</td>\n",
       "      <td>0.373017</td>\n",
       "      <td>0.019263</td>\n",
       "      <td>0.104039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111179</td>\n",
       "      <td>-0.042219</td>\n",
       "      <td>-0.002455</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>0.453062</td>\n",
       "      <td>0.455766</td>\n",
       "      <td>0.471672</td>\n",
       "      <td>0.409677</td>\n",
       "      <td>-0.010273</td>\n",
       "      <td>0.104077</td>\n",
       "      <td>-0.104287</td>\n",
       "      <td>-0.260038</td>\n",
       "      <td>0.388502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orders_Afternoon</th>\n",
       "      <td>0.270555</td>\n",
       "      <td>0.274315</td>\n",
       "      <td>0.278179</td>\n",
       "      <td>0.290232</td>\n",
       "      <td>0.274976</td>\n",
       "      <td>0.261760</td>\n",
       "      <td>0.300375</td>\n",
       "      <td>0.008237</td>\n",
       "      <td>-0.016595</td>\n",
       "      <td>0.335086</td>\n",
       "      <td>-0.037705</td>\n",
       "      <td>-0.024523</td>\n",
       "      <td>-0.063277</td>\n",
       "      <td>-0.019539</td>\n",
       "      <td>-0.028598</td>\n",
       "      <td>-0.037329</td>\n",
       "      <td>-0.038499</td>\n",
       "      <td>-0.036976</td>\n",
       "      <td>-0.030659</td>\n",
       "      <td>-0.010916</td>\n",
       "      <td>-0.052705</td>\n",
       "      <td>-0.023583</td>\n",
       "      <td>-0.021786</td>\n",
       "      <td>-0.009926</td>\n",
       "      <td>-0.030789</td>\n",
       "      <td>0.391147</td>\n",
       "      <td>-0.114402</td>\n",
       "      <td>-0.097039</td>\n",
       "      <td>0.111179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194082</td>\n",
       "      <td>0.100161</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>0.441676</td>\n",
       "      <td>0.481191</td>\n",
       "      <td>0.502094</td>\n",
       "      <td>0.307815</td>\n",
       "      <td>-0.196626</td>\n",
       "      <td>0.092326</td>\n",
       "      <td>-0.124388</td>\n",
       "      <td>-0.269009</td>\n",
       "      <td>0.422268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orders_Evening</th>\n",
       "      <td>0.209825</td>\n",
       "      <td>0.222328</td>\n",
       "      <td>0.238674</td>\n",
       "      <td>0.260003</td>\n",
       "      <td>0.296619</td>\n",
       "      <td>0.264613</td>\n",
       "      <td>0.267745</td>\n",
       "      <td>-0.041549</td>\n",
       "      <td>-0.058072</td>\n",
       "      <td>0.360599</td>\n",
       "      <td>-0.063659</td>\n",
       "      <td>-0.078138</td>\n",
       "      <td>-0.055467</td>\n",
       "      <td>-0.047638</td>\n",
       "      <td>-0.034149</td>\n",
       "      <td>-0.049854</td>\n",
       "      <td>-0.039750</td>\n",
       "      <td>-0.041040</td>\n",
       "      <td>-0.037663</td>\n",
       "      <td>-0.073522</td>\n",
       "      <td>-0.061768</td>\n",
       "      <td>-0.054367</td>\n",
       "      <td>-0.040180</td>\n",
       "      <td>-0.034443</td>\n",
       "      <td>-0.041215</td>\n",
       "      <td>0.367363</td>\n",
       "      <td>-0.261940</td>\n",
       "      <td>-0.289791</td>\n",
       "      <td>-0.042219</td>\n",
       "      <td>0.194082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150858</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.378383</td>\n",
       "      <td>0.422329</td>\n",
       "      <td>0.457202</td>\n",
       "      <td>0.174555</td>\n",
       "      <td>-0.334636</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>-0.118235</td>\n",
       "      <td>-0.216990</td>\n",
       "      <td>0.385903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orders_Dusk</th>\n",
       "      <td>0.166647</td>\n",
       "      <td>0.172557</td>\n",
       "      <td>0.163488</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.166407</td>\n",
       "      <td>0.169846</td>\n",
       "      <td>0.163296</td>\n",
       "      <td>-0.022636</td>\n",
       "      <td>-0.031836</td>\n",
       "      <td>0.128033</td>\n",
       "      <td>-0.098646</td>\n",
       "      <td>-0.024145</td>\n",
       "      <td>-0.104413</td>\n",
       "      <td>-0.006549</td>\n",
       "      <td>-0.064339</td>\n",
       "      <td>-0.078869</td>\n",
       "      <td>-0.047355</td>\n",
       "      <td>-0.026871</td>\n",
       "      <td>-0.106213</td>\n",
       "      <td>-0.069623</td>\n",
       "      <td>-0.058590</td>\n",
       "      <td>-0.071455</td>\n",
       "      <td>-0.090372</td>\n",
       "      <td>-0.046950</td>\n",
       "      <td>-0.058503</td>\n",
       "      <td>0.288315</td>\n",
       "      <td>0.026949</td>\n",
       "      <td>-0.086220</td>\n",
       "      <td>-0.002455</td>\n",
       "      <td>0.100161</td>\n",
       "      <td>0.150858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.285205</td>\n",
       "      <td>0.257246</td>\n",
       "      <td>0.275351</td>\n",
       "      <td>0.045480</td>\n",
       "      <td>-0.287962</td>\n",
       "      <td>0.029254</td>\n",
       "      <td>-0.037365</td>\n",
       "      <td>-0.114656</td>\n",
       "      <td>0.218687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age_Group</th>\n",
       "      <td>0.014155</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>-0.002040</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>0.009922</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>0.011969</td>\n",
       "      <td>-0.005341</td>\n",
       "      <td>-0.003634</td>\n",
       "      <td>-0.002565</td>\n",
       "      <td>-0.004241</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>-0.010762</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>-0.009614</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>-0.006460</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>-0.000392</td>\n",
       "      <td>-0.009655</td>\n",
       "      <td>0.004309</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>-0.002274</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>0.008210</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.003364</td>\n",
       "      <td>-0.003050</td>\n",
       "      <td>-0.012532</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vendor_count</th>\n",
       "      <td>0.444394</td>\n",
       "      <td>0.441420</td>\n",
       "      <td>0.449241</td>\n",
       "      <td>0.450414</td>\n",
       "      <td>0.429584</td>\n",
       "      <td>0.409335</td>\n",
       "      <td>0.417778</td>\n",
       "      <td>-0.107176</td>\n",
       "      <td>-0.162246</td>\n",
       "      <td>0.238197</td>\n",
       "      <td>-0.111250</td>\n",
       "      <td>-0.025968</td>\n",
       "      <td>-0.115958</td>\n",
       "      <td>-0.050680</td>\n",
       "      <td>-0.056442</td>\n",
       "      <td>-0.080605</td>\n",
       "      <td>-0.075052</td>\n",
       "      <td>-0.076371</td>\n",
       "      <td>-0.075448</td>\n",
       "      <td>-0.069048</td>\n",
       "      <td>-0.108646</td>\n",
       "      <td>-0.075103</td>\n",
       "      <td>-0.066252</td>\n",
       "      <td>-0.046854</td>\n",
       "      <td>-0.063870</td>\n",
       "      <td>0.704662</td>\n",
       "      <td>0.186750</td>\n",
       "      <td>0.205403</td>\n",
       "      <td>0.453062</td>\n",
       "      <td>0.441676</td>\n",
       "      <td>0.378383</td>\n",
       "      <td>0.285205</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.842378</td>\n",
       "      <td>0.886126</td>\n",
       "      <td>0.613952</td>\n",
       "      <td>-0.184805</td>\n",
       "      <td>0.178186</td>\n",
       "      <td>-0.367862</td>\n",
       "      <td>-0.442191</td>\n",
       "      <td>0.755993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_count</th>\n",
       "      <td>0.467621</td>\n",
       "      <td>0.467889</td>\n",
       "      <td>0.486109</td>\n",
       "      <td>0.485919</td>\n",
       "      <td>0.470255</td>\n",
       "      <td>0.427865</td>\n",
       "      <td>0.445346</td>\n",
       "      <td>-0.043445</td>\n",
       "      <td>-0.101710</td>\n",
       "      <td>0.581853</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.087398</td>\n",
       "      <td>-0.034609</td>\n",
       "      <td>-0.019424</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>-0.021622</td>\n",
       "      <td>-0.028531</td>\n",
       "      <td>-0.023637</td>\n",
       "      <td>-0.011355</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>-0.024791</td>\n",
       "      <td>-0.017732</td>\n",
       "      <td>0.019656</td>\n",
       "      <td>0.032351</td>\n",
       "      <td>-0.011918</td>\n",
       "      <td>0.675685</td>\n",
       "      <td>0.169706</td>\n",
       "      <td>0.168363</td>\n",
       "      <td>0.455766</td>\n",
       "      <td>0.481191</td>\n",
       "      <td>0.422329</td>\n",
       "      <td>0.257246</td>\n",
       "      <td>0.008210</td>\n",
       "      <td>0.842378</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946879</td>\n",
       "      <td>0.795042</td>\n",
       "      <td>-0.033330</td>\n",
       "      <td>0.465396</td>\n",
       "      <td>-0.339693</td>\n",
       "      <td>-0.468386</td>\n",
       "      <td>0.784038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Orders_Per_Client</th>\n",
       "      <td>0.487078</td>\n",
       "      <td>0.488111</td>\n",
       "      <td>0.505805</td>\n",
       "      <td>0.506955</td>\n",
       "      <td>0.489599</td>\n",
       "      <td>0.441399</td>\n",
       "      <td>0.457593</td>\n",
       "      <td>-0.106557</td>\n",
       "      <td>-0.165023</td>\n",
       "      <td>0.616230</td>\n",
       "      <td>-0.044018</td>\n",
       "      <td>0.026927</td>\n",
       "      <td>-0.066371</td>\n",
       "      <td>-0.030914</td>\n",
       "      <td>-0.026319</td>\n",
       "      <td>-0.048720</td>\n",
       "      <td>-0.049974</td>\n",
       "      <td>-0.043395</td>\n",
       "      <td>-0.040390</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>-0.062809</td>\n",
       "      <td>-0.049222</td>\n",
       "      <td>-0.022687</td>\n",
       "      <td>-0.026559</td>\n",
       "      <td>-0.030759</td>\n",
       "      <td>0.727238</td>\n",
       "      <td>0.156379</td>\n",
       "      <td>0.166701</td>\n",
       "      <td>0.471672</td>\n",
       "      <td>0.502094</td>\n",
       "      <td>0.457202</td>\n",
       "      <td>0.275351</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.886126</td>\n",
       "      <td>0.946879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.682539</td>\n",
       "      <td>-0.218448</td>\n",
       "      <td>0.181313</td>\n",
       "      <td>-0.386786</td>\n",
       "      <td>-0.491767</td>\n",
       "      <td>0.832197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnt</th>\n",
       "      <td>0.367186</td>\n",
       "      <td>0.361109</td>\n",
       "      <td>0.377148</td>\n",
       "      <td>0.372713</td>\n",
       "      <td>0.359802</td>\n",
       "      <td>0.328541</td>\n",
       "      <td>0.332949</td>\n",
       "      <td>0.119324</td>\n",
       "      <td>0.069739</td>\n",
       "      <td>0.418518</td>\n",
       "      <td>0.151092</td>\n",
       "      <td>0.134996</td>\n",
       "      <td>0.103059</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>0.116664</td>\n",
       "      <td>0.106115</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>0.033129</td>\n",
       "      <td>0.102434</td>\n",
       "      <td>0.133291</td>\n",
       "      <td>0.081741</td>\n",
       "      <td>0.091998</td>\n",
       "      <td>0.199450</td>\n",
       "      <td>0.075274</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.359597</td>\n",
       "      <td>0.266130</td>\n",
       "      <td>0.310477</td>\n",
       "      <td>0.409677</td>\n",
       "      <td>0.307815</td>\n",
       "      <td>0.174555</td>\n",
       "      <td>0.045480</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>0.613952</td>\n",
       "      <td>0.795042</td>\n",
       "      <td>0.682539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.512704</td>\n",
       "      <td>0.565664</td>\n",
       "      <td>-0.197222</td>\n",
       "      <td>-0.378727</td>\n",
       "      <td>0.566966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnt_Per_Order</th>\n",
       "      <td>-0.095872</td>\n",
       "      <td>-0.106459</td>\n",
       "      <td>-0.101740</td>\n",
       "      <td>-0.113150</td>\n",
       "      <td>-0.107482</td>\n",
       "      <td>-0.091810</td>\n",
       "      <td>-0.102645</td>\n",
       "      <td>0.223013</td>\n",
       "      <td>0.214071</td>\n",
       "      <td>-0.158522</td>\n",
       "      <td>0.259137</td>\n",
       "      <td>0.173878</td>\n",
       "      <td>0.217276</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>0.173162</td>\n",
       "      <td>0.189395</td>\n",
       "      <td>0.113834</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>0.185433</td>\n",
       "      <td>0.188213</td>\n",
       "      <td>0.187344</td>\n",
       "      <td>0.175486</td>\n",
       "      <td>0.271743</td>\n",
       "      <td>0.133339</td>\n",
       "      <td>0.113304</td>\n",
       "      <td>-0.355419</td>\n",
       "      <td>0.206395</td>\n",
       "      <td>0.260687</td>\n",
       "      <td>-0.010273</td>\n",
       "      <td>-0.196626</td>\n",
       "      <td>-0.334636</td>\n",
       "      <td>-0.287962</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>-0.184805</td>\n",
       "      <td>-0.033330</td>\n",
       "      <td>-0.218448</td>\n",
       "      <td>0.512704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521619</td>\n",
       "      <td>0.110296</td>\n",
       "      <td>0.063692</td>\n",
       "      <td>-0.186461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Items_Per_Order</th>\n",
       "      <td>0.090421</td>\n",
       "      <td>0.090598</td>\n",
       "      <td>0.099674</td>\n",
       "      <td>0.095464</td>\n",
       "      <td>0.095874</td>\n",
       "      <td>0.099923</td>\n",
       "      <td>0.110594</td>\n",
       "      <td>0.125428</td>\n",
       "      <td>0.102301</td>\n",
       "      <td>0.084251</td>\n",
       "      <td>0.139976</td>\n",
       "      <td>0.198714</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.078059</td>\n",
       "      <td>0.071722</td>\n",
       "      <td>0.051868</td>\n",
       "      <td>0.043988</td>\n",
       "      <td>0.080411</td>\n",
       "      <td>0.121660</td>\n",
       "      <td>0.099722</td>\n",
       "      <td>0.072472</td>\n",
       "      <td>0.126142</td>\n",
       "      <td>0.147282</td>\n",
       "      <td>0.051293</td>\n",
       "      <td>0.084938</td>\n",
       "      <td>0.102970</td>\n",
       "      <td>0.074402</td>\n",
       "      <td>0.104077</td>\n",
       "      <td>0.092326</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.029254</td>\n",
       "      <td>0.003364</td>\n",
       "      <td>0.178186</td>\n",
       "      <td>0.465396</td>\n",
       "      <td>0.181313</td>\n",
       "      <td>0.565664</td>\n",
       "      <td>0.521619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019855</td>\n",
       "      <td>-0.092306</td>\n",
       "      <td>0.140074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frq</th>\n",
       "      <td>-0.078086</td>\n",
       "      <td>-0.072986</td>\n",
       "      <td>-0.087117</td>\n",
       "      <td>-0.088083</td>\n",
       "      <td>-0.091739</td>\n",
       "      <td>-0.076712</td>\n",
       "      <td>-0.103022</td>\n",
       "      <td>0.574515</td>\n",
       "      <td>0.620062</td>\n",
       "      <td>-0.171074</td>\n",
       "      <td>0.074937</td>\n",
       "      <td>0.027655</td>\n",
       "      <td>0.056381</td>\n",
       "      <td>0.022728</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>0.033248</td>\n",
       "      <td>0.034559</td>\n",
       "      <td>0.037996</td>\n",
       "      <td>0.029767</td>\n",
       "      <td>0.019594</td>\n",
       "      <td>0.055034</td>\n",
       "      <td>0.031145</td>\n",
       "      <td>0.060305</td>\n",
       "      <td>0.018881</td>\n",
       "      <td>0.015079</td>\n",
       "      <td>-0.239551</td>\n",
       "      <td>-0.059586</td>\n",
       "      <td>-0.073573</td>\n",
       "      <td>-0.104287</td>\n",
       "      <td>-0.124388</td>\n",
       "      <td>-0.118235</td>\n",
       "      <td>-0.037365</td>\n",
       "      <td>-0.003050</td>\n",
       "      <td>-0.367862</td>\n",
       "      <td>-0.339693</td>\n",
       "      <td>-0.386786</td>\n",
       "      <td>-0.197222</td>\n",
       "      <td>0.110296</td>\n",
       "      <td>-0.019855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337825</td>\n",
       "      <td>-0.718351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rcn</th>\n",
       "      <td>-0.241139</td>\n",
       "      <td>-0.251493</td>\n",
       "      <td>-0.273226</td>\n",
       "      <td>-0.299676</td>\n",
       "      <td>-0.302244</td>\n",
       "      <td>-0.206894</td>\n",
       "      <td>-0.220279</td>\n",
       "      <td>-0.406842</td>\n",
       "      <td>-0.364907</td>\n",
       "      <td>-0.305183</td>\n",
       "      <td>-0.001536</td>\n",
       "      <td>-0.014886</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>-0.010467</td>\n",
       "      <td>-0.005776</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.015912</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>-0.004616</td>\n",
       "      <td>0.027812</td>\n",
       "      <td>0.014812</td>\n",
       "      <td>-0.019620</td>\n",
       "      <td>0.007585</td>\n",
       "      <td>0.011523</td>\n",
       "      <td>-0.333840</td>\n",
       "      <td>-0.086558</td>\n",
       "      <td>-0.104074</td>\n",
       "      <td>-0.260038</td>\n",
       "      <td>-0.269009</td>\n",
       "      <td>-0.216990</td>\n",
       "      <td>-0.114656</td>\n",
       "      <td>-0.012532</td>\n",
       "      <td>-0.442191</td>\n",
       "      <td>-0.468386</td>\n",
       "      <td>-0.491767</td>\n",
       "      <td>-0.378727</td>\n",
       "      <td>0.063692</td>\n",
       "      <td>-0.092306</td>\n",
       "      <td>0.337825</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.590171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity</th>\n",
       "      <td>0.398360</td>\n",
       "      <td>0.397150</td>\n",
       "      <td>0.417149</td>\n",
       "      <td>0.421766</td>\n",
       "      <td>0.404916</td>\n",
       "      <td>0.356761</td>\n",
       "      <td>0.386163</td>\n",
       "      <td>-0.183989</td>\n",
       "      <td>-0.247917</td>\n",
       "      <td>0.495155</td>\n",
       "      <td>-0.047178</td>\n",
       "      <td>0.009899</td>\n",
       "      <td>-0.061953</td>\n",
       "      <td>-0.025276</td>\n",
       "      <td>-0.025297</td>\n",
       "      <td>-0.028910</td>\n",
       "      <td>-0.045942</td>\n",
       "      <td>-0.042946</td>\n",
       "      <td>-0.034492</td>\n",
       "      <td>-0.003151</td>\n",
       "      <td>-0.059256</td>\n",
       "      <td>-0.043722</td>\n",
       "      <td>-0.014624</td>\n",
       "      <td>-0.026195</td>\n",
       "      <td>-0.020498</td>\n",
       "      <td>0.601557</td>\n",
       "      <td>0.127563</td>\n",
       "      <td>0.146064</td>\n",
       "      <td>0.388502</td>\n",
       "      <td>0.422268</td>\n",
       "      <td>0.385903</td>\n",
       "      <td>0.218687</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.755993</td>\n",
       "      <td>0.784038</td>\n",
       "      <td>0.832197</td>\n",
       "      <td>0.566966</td>\n",
       "      <td>-0.186461</td>\n",
       "      <td>0.140074</td>\n",
       "      <td>-0.718351</td>\n",
       "      <td>-0.590171</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            DOW_0     DOW_1     DOW_2     DOW_3     DOW_4  \\\n",
       "DOW_0                    1.000000  0.239451  0.234692  0.225882  0.176707   \n",
       "DOW_1                    0.239451  1.000000  0.248535  0.235444  0.185994   \n",
       "DOW_2                    0.234692  0.248535  1.000000  0.242585  0.197536   \n",
       "DOW_3                    0.225882  0.235444  0.242585  1.000000  0.204534   \n",
       "DOW_4                    0.176707  0.185994  0.197536  0.204534  1.000000   \n",
       "DOW_5                    0.130844  0.122919  0.120607  0.131171  0.139216   \n",
       "DOW_6                    0.144047  0.123414  0.135044  0.128172  0.126009   \n",
       "CLV_Score                0.041674  0.052926  0.063361  0.094587  0.100607   \n",
       "RFM_Score                0.015459  0.025526  0.036966  0.066596  0.055575   \n",
       "Loyalty                  0.309971  0.320087  0.336971  0.339671  0.334988   \n",
       "CUI_American            -0.007948  0.004187 -0.002909 -0.000416 -0.005596   \n",
       "CUI_Asian                0.031864  0.017901  0.008809  0.021530  0.018773   \n",
       "CUI_Beverages           -0.023641 -0.018165 -0.019424 -0.016312 -0.014867   \n",
       "CUI_Cafe                -0.009666 -0.005425 -0.005220 -0.010115 -0.003749   \n",
       "CUI_Chicken_Dishes       0.003849  0.006780  0.007988 -0.000787 -0.004226   \n",
       "CUI_Chinese             -0.012840 -0.014745 -0.007617 -0.000660 -0.006766   \n",
       "CUI_Desserts            -0.020678 -0.013502 -0.030035 -0.025462 -0.014771   \n",
       "CUI_Healthy             -0.009510 -0.016770 -0.022447 -0.029278 -0.009122   \n",
       "CUI_Indian              -0.021093 -0.022564 -0.012624 -0.014567 -0.013820   \n",
       "CUI_Italian              0.006957  0.007983  0.014201 -0.000934 -0.003563   \n",
       "CUI_Japanese            -0.032283 -0.029118 -0.019137 -0.026389 -0.015573   \n",
       "CUI_Noodle_Dishes       -0.020294 -0.014173 -0.030887 -0.014357 -0.009558   \n",
       "CUI_OTHER                0.014130  0.022086  0.029338  0.008195  0.020205   \n",
       "CUI_Street_Food/Snacks  -0.016510 -0.017793 -0.013291 -0.008626 -0.007211   \n",
       "CUI_Thai                -0.011773 -0.010852 -0.016498 -0.015371 -0.007092   \n",
       "is_chain                 0.380964  0.381061  0.384234  0.382928  0.364731   \n",
       "Orders_Night             0.117614  0.092907  0.089564  0.078979  0.051334   \n",
       "Orders_Dawn              0.112148  0.101717  0.097099  0.087339  0.053484   \n",
       "Orders_Morning           0.289692  0.271741  0.286622  0.273337  0.248499   \n",
       "Orders_Afternoon         0.270555  0.274315  0.278179  0.290232  0.274976   \n",
       "Orders_Evening           0.209825  0.222328  0.238674  0.260003  0.296619   \n",
       "Orders_Dusk              0.166647  0.172557  0.163488  0.160200  0.166407   \n",
       "Age_Group                0.014155  0.002188  0.002543  0.008337 -0.002040   \n",
       "vendor_count             0.444394  0.441420  0.449241  0.450414  0.429584   \n",
       "product_count            0.467621  0.467889  0.486109  0.485919  0.470255   \n",
       "Total_Orders_Per_Client  0.487078  0.488111  0.505805  0.506955  0.489599   \n",
       "mnt                      0.367186  0.361109  0.377148  0.372713  0.359802   \n",
       "mnt_Per_Order           -0.095872 -0.106459 -0.101740 -0.113150 -0.107482   \n",
       "Items_Per_Order          0.090421  0.090598  0.099674  0.095464  0.095874   \n",
       "frq                     -0.078086 -0.072986 -0.087117 -0.088083 -0.091739   \n",
       "rcn                     -0.241139 -0.251493 -0.273226 -0.299676 -0.302244   \n",
       "activity                 0.398360  0.397150  0.417149  0.421766  0.404916   \n",
       "\n",
       "                            DOW_5     DOW_6  CLV_Score  RFM_Score   Loyalty  \\\n",
       "DOW_0                    0.130844  0.144047   0.041674   0.015459  0.309971   \n",
       "DOW_1                    0.122919  0.123414   0.052926   0.025526  0.320087   \n",
       "DOW_2                    0.120607  0.135044   0.063361   0.036966  0.336971   \n",
       "DOW_3                    0.131171  0.128172   0.094587   0.066596  0.339671   \n",
       "DOW_4                    0.139216  0.126009   0.100607   0.055575  0.334988   \n",
       "DOW_5                    1.000000  0.190266   0.016857  -0.007242  0.271503   \n",
       "DOW_6                    0.190266  1.000000  -0.004533  -0.029825  0.284548   \n",
       "CLV_Score                0.016857 -0.004533   1.000000   0.989184 -0.019965   \n",
       "RFM_Score               -0.007242 -0.029825   0.989184   1.000000 -0.053272   \n",
       "Loyalty                  0.271503  0.284548  -0.019965  -0.053272  1.000000   \n",
       "CUI_American            -0.008867 -0.016091   0.109149   0.105438  0.100623   \n",
       "CUI_Asian                0.019214  0.008614   0.082841   0.067619  0.088698   \n",
       "CUI_Beverages           -0.036072 -0.028407   0.078898   0.077297  0.068987   \n",
       "CUI_Cafe                -0.011699 -0.015837   0.024169   0.021920  0.022231   \n",
       "CUI_Chicken_Dishes      -0.001307 -0.004361   0.068521   0.067819  0.045568   \n",
       "CUI_Chinese             -0.020450 -0.014939   0.073655   0.071444  0.043057   \n",
       "CUI_Desserts            -0.026734 -0.022278   0.042226   0.038678  0.025652   \n",
       "CUI_Healthy             -0.011719 -0.023250   0.039968   0.038424  0.037066   \n",
       "CUI_Indian              -0.006826 -0.016172   0.053582   0.052040  0.046649   \n",
       "CUI_Italian             -0.002263 -0.005741   0.051527   0.046542  0.100662   \n",
       "CUI_Japanese            -0.011149 -0.019905   0.067097   0.063801  0.057946   \n",
       "CUI_Noodle_Dishes       -0.019269 -0.018516   0.045779   0.045087  0.025661   \n",
       "CUI_OTHER                0.018507  0.016055   0.111758   0.107312  0.078312   \n",
       "CUI_Street_Food/Snacks  -0.022651 -0.013440   0.055347   0.045112  0.021250   \n",
       "CUI_Thai                -0.011761 -0.012242   0.028161   0.027352  0.048223   \n",
       "is_chain                 0.335270  0.347896  -0.086399  -0.124700  0.384554   \n",
       "Orders_Night             0.072311  0.057427   0.036009   0.015206  0.013339   \n",
       "Orders_Dawn              0.079144  0.069818   0.043632   0.020149  0.001447   \n",
       "Orders_Morning           0.224566  0.238807   0.050462   0.021244  0.254087   \n",
       "Orders_Afternoon         0.261760  0.300375   0.008237  -0.016595  0.335086   \n",
       "Orders_Evening           0.264613  0.267745  -0.041549  -0.058072  0.360599   \n",
       "Orders_Dusk              0.169846  0.163296  -0.022636  -0.031836  0.128033   \n",
       "Age_Group               -0.002210  0.003904   0.008029   0.006676  0.009922   \n",
       "vendor_count             0.409335  0.417778  -0.107176  -0.162246  0.238197   \n",
       "product_count            0.427865  0.445346  -0.043445  -0.101710  0.581853   \n",
       "Total_Orders_Per_Client  0.441399  0.457593  -0.106557  -0.165023  0.616230   \n",
       "mnt                      0.328541  0.332949   0.119324   0.069739  0.418518   \n",
       "mnt_Per_Order           -0.091810 -0.102645   0.223013   0.214071 -0.158522   \n",
       "Items_Per_Order          0.099923  0.110594   0.125428   0.102301  0.084251   \n",
       "frq                     -0.076712 -0.103022   0.574515   0.620062 -0.171074   \n",
       "rcn                     -0.206894 -0.220279  -0.406842  -0.364907 -0.305183   \n",
       "activity                 0.356761  0.386163  -0.183989  -0.247917  0.495155   \n",
       "\n",
       "                         CUI_American  CUI_Asian  CUI_Beverages  CUI_Cafe  \\\n",
       "DOW_0                       -0.007948   0.031864      -0.023641 -0.009666   \n",
       "DOW_1                        0.004187   0.017901      -0.018165 -0.005425   \n",
       "DOW_2                       -0.002909   0.008809      -0.019424 -0.005220   \n",
       "DOW_3                       -0.000416   0.021530      -0.016312 -0.010115   \n",
       "DOW_4                       -0.005596   0.018773      -0.014867 -0.003749   \n",
       "DOW_5                       -0.008867   0.019214      -0.036072 -0.011699   \n",
       "DOW_6                       -0.016091   0.008614      -0.028407 -0.015837   \n",
       "CLV_Score                    0.109149   0.082841       0.078898  0.024169   \n",
       "RFM_Score                    0.105438   0.067619       0.077297  0.021920   \n",
       "Loyalty                      0.100623   0.088698       0.068987  0.022231   \n",
       "CUI_American                 1.000000   0.044933       0.067861  0.019177   \n",
       "CUI_Asian                    0.044933   1.000000       0.027021  0.018364   \n",
       "CUI_Beverages                0.067861   0.027021       1.000000 -0.003504   \n",
       "CUI_Cafe                     0.019177   0.018364      -0.003504  1.000000   \n",
       "CUI_Chicken_Dishes           0.042542   0.012362       0.037096  0.002464   \n",
       "CUI_Chinese                  0.045970   0.021255       0.040950  0.003713   \n",
       "CUI_Desserts                 0.033984   0.023807       0.037508  0.011776   \n",
       "CUI_Healthy                  0.023113   0.016458       0.013354  0.030000   \n",
       "CUI_Indian                   0.049893   0.032392       0.027950  0.004777   \n",
       "CUI_Italian                  0.049653   0.020247       0.022779  0.047521   \n",
       "CUI_Japanese                 0.044878   0.036830       0.044909  0.013547   \n",
       "CUI_Noodle_Dishes            0.039889   0.022214       0.038532  0.002304   \n",
       "CUI_OTHER                    0.067226   0.017991       0.060692  0.023686   \n",
       "CUI_Street_Food/Snacks       0.011723   0.030551       0.013640 -0.001544   \n",
       "CUI_Thai                     0.030616   0.018244       0.023358  0.015465   \n",
       "is_chain                    -0.095541  -0.013505      -0.122599 -0.033732   \n",
       "Orders_Night                 0.062320   0.124434       0.014400  0.035755   \n",
       "Orders_Dawn                  0.059124   0.084652       0.050755  0.026209   \n",
       "Orders_Morning              -0.004908   0.005080      -0.016675 -0.023953   \n",
       "Orders_Afternoon            -0.037705  -0.024523      -0.063277 -0.019539   \n",
       "Orders_Evening              -0.063659  -0.078138      -0.055467 -0.047638   \n",
       "Orders_Dusk                 -0.098646  -0.024145      -0.104413 -0.006549   \n",
       "Age_Group                    0.009693   0.011969      -0.005341 -0.003634   \n",
       "vendor_count                -0.111250  -0.025968      -0.115958 -0.050680   \n",
       "product_count                0.005921   0.087398      -0.034609 -0.019424   \n",
       "Total_Orders_Per_Client     -0.044018   0.026927      -0.066371 -0.030914   \n",
       "mnt                          0.151092   0.134996       0.103059  0.006832   \n",
       "mnt_Per_Order                0.259137   0.173878       0.217276  0.052662   \n",
       "Items_Per_Order              0.139976   0.198714       0.074321  0.025960   \n",
       "frq                          0.074937   0.027655       0.056381  0.022728   \n",
       "rcn                         -0.001536  -0.014886       0.010748  0.010790   \n",
       "activity                    -0.047178   0.009899      -0.061953 -0.025276   \n",
       "\n",
       "                         CUI_Chicken_Dishes  CUI_Chinese  CUI_Desserts  \\\n",
       "DOW_0                              0.003849    -0.012840     -0.020678   \n",
       "DOW_1                              0.006780    -0.014745     -0.013502   \n",
       "DOW_2                              0.007988    -0.007617     -0.030035   \n",
       "DOW_3                             -0.000787    -0.000660     -0.025462   \n",
       "DOW_4                             -0.004226    -0.006766     -0.014771   \n",
       "DOW_5                             -0.001307    -0.020450     -0.026734   \n",
       "DOW_6                             -0.004361    -0.014939     -0.022278   \n",
       "CLV_Score                          0.068521     0.073655      0.042226   \n",
       "RFM_Score                          0.067819     0.071444      0.038678   \n",
       "Loyalty                            0.045568     0.043057      0.025652   \n",
       "CUI_American                       0.042542     0.045970      0.033984   \n",
       "CUI_Asian                          0.012362     0.021255      0.023807   \n",
       "CUI_Beverages                      0.037096     0.040950      0.037508   \n",
       "CUI_Cafe                           0.002464     0.003713      0.011776   \n",
       "CUI_Chicken_Dishes                 1.000000     0.038158      0.015061   \n",
       "CUI_Chinese                        0.038158     1.000000      0.007524   \n",
       "CUI_Desserts                       0.015061     0.007524      1.000000   \n",
       "CUI_Healthy                        0.014262     0.010143      0.010363   \n",
       "CUI_Indian                         0.017962     0.028586      0.014390   \n",
       "CUI_Italian                        0.007127     0.021004      0.011883   \n",
       "CUI_Japanese                       0.028855     0.032553      0.023153   \n",
       "CUI_Noodle_Dishes                  0.025071     0.045482      0.009643   \n",
       "CUI_OTHER                          0.059971     0.074387      0.028446   \n",
       "CUI_Street_Food/Snacks             0.004111     0.018343      0.031545   \n",
       "CUI_Thai                           0.016097     0.011702      0.006628   \n",
       "is_chain                          -0.074565    -0.092873     -0.070229   \n",
       "Orders_Night                       0.027999     0.021352      0.002119   \n",
       "Orders_Dawn                        0.047501     0.029808      0.014950   \n",
       "Orders_Morning                     0.014607     0.009509     -0.013392   \n",
       "Orders_Afternoon                  -0.028598    -0.037329     -0.038499   \n",
       "Orders_Evening                    -0.034149    -0.049854     -0.039750   \n",
       "Orders_Dusk                       -0.064339    -0.078869     -0.047355   \n",
       "Age_Group                         -0.002565    -0.004241      0.002856   \n",
       "vendor_count                      -0.056442    -0.080605     -0.075052   \n",
       "product_count                     -0.000743    -0.021622     -0.028531   \n",
       "Total_Orders_Per_Client           -0.026319    -0.048720     -0.049974   \n",
       "mnt                                0.116664     0.106115      0.034422   \n",
       "mnt_Per_Order                      0.173162     0.189395      0.113834   \n",
       "Items_Per_Order                    0.078059     0.071722      0.051868   \n",
       "frq                                0.039513     0.033248      0.034559   \n",
       "rcn                               -0.010467    -0.005776      0.014500   \n",
       "activity                          -0.025297    -0.028910     -0.045942   \n",
       "\n",
       "                         CUI_Healthy  CUI_Indian  CUI_Italian  CUI_Japanese  \\\n",
       "DOW_0                      -0.009510   -0.021093     0.006957     -0.032283   \n",
       "DOW_1                      -0.016770   -0.022564     0.007983     -0.029118   \n",
       "DOW_2                      -0.022447   -0.012624     0.014201     -0.019137   \n",
       "DOW_3                      -0.029278   -0.014567    -0.000934     -0.026389   \n",
       "DOW_4                      -0.009122   -0.013820    -0.003563     -0.015573   \n",
       "DOW_5                      -0.011719   -0.006826    -0.002263     -0.011149   \n",
       "DOW_6                      -0.023250   -0.016172    -0.005741     -0.019905   \n",
       "CLV_Score                   0.039968    0.053582     0.051527      0.067097   \n",
       "RFM_Score                   0.038424    0.052040     0.046542      0.063801   \n",
       "Loyalty                     0.037066    0.046649     0.100662      0.057946   \n",
       "CUI_American                0.023113    0.049893     0.049653      0.044878   \n",
       "CUI_Asian                   0.016458    0.032392     0.020247      0.036830   \n",
       "CUI_Beverages               0.013354    0.027950     0.022779      0.044909   \n",
       "CUI_Cafe                    0.030000    0.004777     0.047521      0.013547   \n",
       "CUI_Chicken_Dishes          0.014262    0.017962     0.007127      0.028855   \n",
       "CUI_Chinese                 0.010143    0.028586     0.021004      0.032553   \n",
       "CUI_Desserts                0.010363    0.014390     0.011883      0.023153   \n",
       "CUI_Healthy                 1.000000    0.010665     0.017604      0.018552   \n",
       "CUI_Indian                  0.010665    1.000000     0.040446      0.016559   \n",
       "CUI_Italian                 0.017604    0.040446     1.000000      0.026348   \n",
       "CUI_Japanese                0.018552    0.016559     0.026348      1.000000   \n",
       "CUI_Noodle_Dishes           0.008778    0.023799     0.017226      0.024674   \n",
       "CUI_OTHER                   0.024644    0.057851     0.028960      0.044269   \n",
       "CUI_Street_Food/Snacks     -0.000496    0.019705     0.008926      0.014532   \n",
       "CUI_Thai                    0.003271    0.025443     0.011422      0.028720   \n",
       "is_chain                   -0.063150   -0.091053    -0.074735     -0.088860   \n",
       "Orders_Night                0.025375    0.002744     0.051014      0.012773   \n",
       "Orders_Dawn                 0.008207    0.033242     0.064033      0.035261   \n",
       "Orders_Morning             -0.022495    0.006460     0.023730     -0.018262   \n",
       "Orders_Afternoon           -0.036976   -0.030659    -0.010916     -0.052705   \n",
       "Orders_Evening             -0.041040   -0.037663    -0.073522     -0.061768   \n",
       "Orders_Dusk                -0.026871   -0.106213    -0.069623     -0.058590   \n",
       "Age_Group                  -0.010762    0.005715    -0.009614      0.005911   \n",
       "vendor_count               -0.076371   -0.075448    -0.069048     -0.108646   \n",
       "product_count              -0.023637   -0.011355     0.038050     -0.024791   \n",
       "Total_Orders_Per_Client    -0.043395   -0.040390     0.000771     -0.062809   \n",
       "mnt                         0.033129    0.102434     0.133291      0.081741   \n",
       "mnt_Per_Order               0.097381    0.185433     0.188213      0.187344   \n",
       "Items_Per_Order             0.043988    0.080411     0.121660      0.099722   \n",
       "frq                         0.037996    0.029767     0.019594      0.055034   \n",
       "rcn                         0.015912    0.010372    -0.004616      0.027812   \n",
       "activity                   -0.042946   -0.034492    -0.003151     -0.059256   \n",
       "\n",
       "                         CUI_Noodle_Dishes  CUI_OTHER  CUI_Street_Food/Snacks  \\\n",
       "DOW_0                            -0.020294   0.014130               -0.016510   \n",
       "DOW_1                            -0.014173   0.022086               -0.017793   \n",
       "DOW_2                            -0.030887   0.029338               -0.013291   \n",
       "DOW_3                            -0.014357   0.008195               -0.008626   \n",
       "DOW_4                            -0.009558   0.020205               -0.007211   \n",
       "DOW_5                            -0.019269   0.018507               -0.022651   \n",
       "DOW_6                            -0.018516   0.016055               -0.013440   \n",
       "CLV_Score                         0.045779   0.111758                0.055347   \n",
       "RFM_Score                         0.045087   0.107312                0.045112   \n",
       "Loyalty                           0.025661   0.078312                0.021250   \n",
       "CUI_American                      0.039889   0.067226                0.011723   \n",
       "CUI_Asian                         0.022214   0.017991                0.030551   \n",
       "CUI_Beverages                     0.038532   0.060692                0.013640   \n",
       "CUI_Cafe                          0.002304   0.023686               -0.001544   \n",
       "CUI_Chicken_Dishes                0.025071   0.059971                0.004111   \n",
       "CUI_Chinese                       0.045482   0.074387                0.018343   \n",
       "CUI_Desserts                      0.009643   0.028446                0.031545   \n",
       "CUI_Healthy                       0.008778   0.024644               -0.000496   \n",
       "CUI_Indian                        0.023799   0.057851                0.019705   \n",
       "CUI_Italian                       0.017226   0.028960                0.008926   \n",
       "CUI_Japanese                      0.024674   0.044269                0.014532   \n",
       "CUI_Noodle_Dishes                 1.000000   0.049150                0.014787   \n",
       "CUI_OTHER                         0.049150   1.000000                0.000692   \n",
       "CUI_Street_Food/Snacks            0.014787   0.000692                1.000000   \n",
       "CUI_Thai                          0.025630   0.034475                0.008138   \n",
       "is_chain                         -0.083204  -0.100366               -0.032380   \n",
       "Orders_Night                      0.013677   0.039770                0.001561   \n",
       "Orders_Dawn                       0.039217   0.076726                0.012194   \n",
       "Orders_Morning                   -0.011507   0.034570               -0.009778   \n",
       "Orders_Afternoon                 -0.023583  -0.021786               -0.009926   \n",
       "Orders_Evening                   -0.054367  -0.040180               -0.034443   \n",
       "Orders_Dusk                      -0.071455  -0.090372               -0.046950   \n",
       "Age_Group                        -0.006460   0.004447               -0.000392   \n",
       "vendor_count                     -0.075103  -0.066252               -0.046854   \n",
       "product_count                    -0.017732   0.019656                0.032351   \n",
       "Total_Orders_Per_Client          -0.049222  -0.022687               -0.026559   \n",
       "mnt                               0.091998   0.199450                0.075274   \n",
       "mnt_Per_Order                     0.175486   0.271743                0.133339   \n",
       "Items_Per_Order                   0.072472   0.126142                0.147282   \n",
       "frq                               0.031145   0.060305                0.018881   \n",
       "rcn                               0.014812  -0.019620                0.007585   \n",
       "activity                         -0.043722  -0.014624               -0.026195   \n",
       "\n",
       "                         CUI_Thai  is_chain  Orders_Night  Orders_Dawn  \\\n",
       "DOW_0                   -0.011773  0.380964      0.117614     0.112148   \n",
       "DOW_1                   -0.010852  0.381061      0.092907     0.101717   \n",
       "DOW_2                   -0.016498  0.384234      0.089564     0.097099   \n",
       "DOW_3                   -0.015371  0.382928      0.078979     0.087339   \n",
       "DOW_4                   -0.007092  0.364731      0.051334     0.053484   \n",
       "DOW_5                   -0.011761  0.335270      0.072311     0.079144   \n",
       "DOW_6                   -0.012242  0.347896      0.057427     0.069818   \n",
       "CLV_Score                0.028161 -0.086399      0.036009     0.043632   \n",
       "RFM_Score                0.027352 -0.124700      0.015206     0.020149   \n",
       "Loyalty                  0.048223  0.384554      0.013339     0.001447   \n",
       "CUI_American             0.030616 -0.095541      0.062320     0.059124   \n",
       "CUI_Asian                0.018244 -0.013505      0.124434     0.084652   \n",
       "CUI_Beverages            0.023358 -0.122599      0.014400     0.050755   \n",
       "CUI_Cafe                 0.015465 -0.033732      0.035755     0.026209   \n",
       "CUI_Chicken_Dishes       0.016097 -0.074565      0.027999     0.047501   \n",
       "CUI_Chinese              0.011702 -0.092873      0.021352     0.029808   \n",
       "CUI_Desserts             0.006628 -0.070229      0.002119     0.014950   \n",
       "CUI_Healthy              0.003271 -0.063150      0.025375     0.008207   \n",
       "CUI_Indian               0.025443 -0.091053      0.002744     0.033242   \n",
       "CUI_Italian              0.011422 -0.074735      0.051014     0.064033   \n",
       "CUI_Japanese             0.028720 -0.088860      0.012773     0.035261   \n",
       "CUI_Noodle_Dishes        0.025630 -0.083204      0.013677     0.039217   \n",
       "CUI_OTHER                0.034475 -0.100366      0.039770     0.076726   \n",
       "CUI_Street_Food/Snacks   0.008138 -0.032380      0.001561     0.012194   \n",
       "CUI_Thai                 1.000000 -0.048794      0.022438     0.034811   \n",
       "is_chain                -0.048794  1.000000      0.068228     0.083559   \n",
       "Orders_Night             0.022438  0.068228      1.000000     0.261559   \n",
       "Orders_Dawn              0.034811  0.083559      0.261559     1.000000   \n",
       "Orders_Morning          -0.011085  0.373017      0.019263     0.104039   \n",
       "Orders_Afternoon        -0.030789  0.391147     -0.114402    -0.097039   \n",
       "Orders_Evening          -0.041215  0.367363     -0.261940    -0.289791   \n",
       "Orders_Dusk             -0.058503  0.288315      0.026949    -0.086220   \n",
       "Age_Group               -0.009655  0.004309      0.000482    -0.002274   \n",
       "vendor_count            -0.063870  0.704662      0.186750     0.205403   \n",
       "product_count           -0.011918  0.675685      0.169706     0.168363   \n",
       "Total_Orders_Per_Client -0.030759  0.727238      0.156379     0.166701   \n",
       "mnt                      0.051600  0.359597      0.266130     0.310477   \n",
       "mnt_Per_Order            0.113304 -0.355419      0.206395     0.260687   \n",
       "Items_Per_Order          0.051293  0.084938      0.102970     0.074402   \n",
       "frq                      0.015079 -0.239551     -0.059586    -0.073573   \n",
       "rcn                      0.011523 -0.333840     -0.086558    -0.104074   \n",
       "activity                -0.020498  0.601557      0.127563     0.146064   \n",
       "\n",
       "                         Orders_Morning  Orders_Afternoon  Orders_Evening  \\\n",
       "DOW_0                          0.289692          0.270555        0.209825   \n",
       "DOW_1                          0.271741          0.274315        0.222328   \n",
       "DOW_2                          0.286622          0.278179        0.238674   \n",
       "DOW_3                          0.273337          0.290232        0.260003   \n",
       "DOW_4                          0.248499          0.274976        0.296619   \n",
       "DOW_5                          0.224566          0.261760        0.264613   \n",
       "DOW_6                          0.238807          0.300375        0.267745   \n",
       "CLV_Score                      0.050462          0.008237       -0.041549   \n",
       "RFM_Score                      0.021244         -0.016595       -0.058072   \n",
       "Loyalty                        0.254087          0.335086        0.360599   \n",
       "CUI_American                  -0.004908         -0.037705       -0.063659   \n",
       "CUI_Asian                      0.005080         -0.024523       -0.078138   \n",
       "CUI_Beverages                 -0.016675         -0.063277       -0.055467   \n",
       "CUI_Cafe                      -0.023953         -0.019539       -0.047638   \n",
       "CUI_Chicken_Dishes             0.014607         -0.028598       -0.034149   \n",
       "CUI_Chinese                    0.009509         -0.037329       -0.049854   \n",
       "CUI_Desserts                  -0.013392         -0.038499       -0.039750   \n",
       "CUI_Healthy                   -0.022495         -0.036976       -0.041040   \n",
       "CUI_Indian                     0.006460         -0.030659       -0.037663   \n",
       "CUI_Italian                    0.023730         -0.010916       -0.073522   \n",
       "CUI_Japanese                  -0.018262         -0.052705       -0.061768   \n",
       "CUI_Noodle_Dishes             -0.011507         -0.023583       -0.054367   \n",
       "CUI_OTHER                      0.034570         -0.021786       -0.040180   \n",
       "CUI_Street_Food/Snacks        -0.009778         -0.009926       -0.034443   \n",
       "CUI_Thai                      -0.011085         -0.030789       -0.041215   \n",
       "is_chain                       0.373017          0.391147        0.367363   \n",
       "Orders_Night                   0.019263         -0.114402       -0.261940   \n",
       "Orders_Dawn                    0.104039         -0.097039       -0.289791   \n",
       "Orders_Morning                 1.000000          0.111179       -0.042219   \n",
       "Orders_Afternoon               0.111179          1.000000        0.194082   \n",
       "Orders_Evening                -0.042219          0.194082        1.000000   \n",
       "Orders_Dusk                   -0.002455          0.100161        0.150858   \n",
       "Age_Group                     -0.001494          0.006926        0.001311   \n",
       "vendor_count                   0.453062          0.441676        0.378383   \n",
       "product_count                  0.455766          0.481191        0.422329   \n",
       "Total_Orders_Per_Client        0.471672          0.502094        0.457202   \n",
       "mnt                            0.409677          0.307815        0.174555   \n",
       "mnt_Per_Order                 -0.010273         -0.196626       -0.334636   \n",
       "Items_Per_Order                0.104077          0.092326        0.040564   \n",
       "frq                           -0.104287         -0.124388       -0.118235   \n",
       "rcn                           -0.260038         -0.269009       -0.216990   \n",
       "activity                       0.388502          0.422268        0.385903   \n",
       "\n",
       "                         Orders_Dusk  Age_Group  vendor_count  product_count  \\\n",
       "DOW_0                       0.166647   0.014155      0.444394       0.467621   \n",
       "DOW_1                       0.172557   0.002188      0.441420       0.467889   \n",
       "DOW_2                       0.163488   0.002543      0.449241       0.486109   \n",
       "DOW_3                       0.160200   0.008337      0.450414       0.485919   \n",
       "DOW_4                       0.166407  -0.002040      0.429584       0.470255   \n",
       "DOW_5                       0.169846  -0.002210      0.409335       0.427865   \n",
       "DOW_6                       0.163296   0.003904      0.417778       0.445346   \n",
       "CLV_Score                  -0.022636   0.008029     -0.107176      -0.043445   \n",
       "RFM_Score                  -0.031836   0.006676     -0.162246      -0.101710   \n",
       "Loyalty                     0.128033   0.009922      0.238197       0.581853   \n",
       "CUI_American               -0.098646   0.009693     -0.111250       0.005921   \n",
       "CUI_Asian                  -0.024145   0.011969     -0.025968       0.087398   \n",
       "CUI_Beverages              -0.104413  -0.005341     -0.115958      -0.034609   \n",
       "CUI_Cafe                   -0.006549  -0.003634     -0.050680      -0.019424   \n",
       "CUI_Chicken_Dishes         -0.064339  -0.002565     -0.056442      -0.000743   \n",
       "CUI_Chinese                -0.078869  -0.004241     -0.080605      -0.021622   \n",
       "CUI_Desserts               -0.047355   0.002856     -0.075052      -0.028531   \n",
       "CUI_Healthy                -0.026871  -0.010762     -0.076371      -0.023637   \n",
       "CUI_Indian                 -0.106213   0.005715     -0.075448      -0.011355   \n",
       "CUI_Italian                -0.069623  -0.009614     -0.069048       0.038050   \n",
       "CUI_Japanese               -0.058590   0.005911     -0.108646      -0.024791   \n",
       "CUI_Noodle_Dishes          -0.071455  -0.006460     -0.075103      -0.017732   \n",
       "CUI_OTHER                  -0.090372   0.004447     -0.066252       0.019656   \n",
       "CUI_Street_Food/Snacks     -0.046950  -0.000392     -0.046854       0.032351   \n",
       "CUI_Thai                   -0.058503  -0.009655     -0.063870      -0.011918   \n",
       "is_chain                    0.288315   0.004309      0.704662       0.675685   \n",
       "Orders_Night                0.026949   0.000482      0.186750       0.169706   \n",
       "Orders_Dawn                -0.086220  -0.002274      0.205403       0.168363   \n",
       "Orders_Morning             -0.002455  -0.001494      0.453062       0.455766   \n",
       "Orders_Afternoon            0.100161   0.006926      0.441676       0.481191   \n",
       "Orders_Evening              0.150858   0.001311      0.378383       0.422329   \n",
       "Orders_Dusk                 1.000000   0.001361      0.285205       0.257246   \n",
       "Age_Group                   0.001361   1.000000      0.003960       0.008210   \n",
       "vendor_count                0.285205   0.003960      1.000000       0.842378   \n",
       "product_count               0.257246   0.008210      0.842378       1.000000   \n",
       "Total_Orders_Per_Client     0.275351   0.008039      0.886126       0.946879   \n",
       "mnt                         0.045480   0.007902      0.613952       0.795042   \n",
       "mnt_Per_Order              -0.287962   0.002042     -0.184805      -0.033330   \n",
       "Items_Per_Order             0.029254   0.003364      0.178186       0.465396   \n",
       "frq                        -0.037365  -0.003050     -0.367862      -0.339693   \n",
       "rcn                        -0.114656  -0.012532     -0.442191      -0.468386   \n",
       "activity                    0.218687   0.006400      0.755993       0.784038   \n",
       "\n",
       "                         Total_Orders_Per_Client       mnt  mnt_Per_Order  \\\n",
       "DOW_0                                   0.487078  0.367186      -0.095872   \n",
       "DOW_1                                   0.488111  0.361109      -0.106459   \n",
       "DOW_2                                   0.505805  0.377148      -0.101740   \n",
       "DOW_3                                   0.506955  0.372713      -0.113150   \n",
       "DOW_4                                   0.489599  0.359802      -0.107482   \n",
       "DOW_5                                   0.441399  0.328541      -0.091810   \n",
       "DOW_6                                   0.457593  0.332949      -0.102645   \n",
       "CLV_Score                              -0.106557  0.119324       0.223013   \n",
       "RFM_Score                              -0.165023  0.069739       0.214071   \n",
       "Loyalty                                 0.616230  0.418518      -0.158522   \n",
       "CUI_American                           -0.044018  0.151092       0.259137   \n",
       "CUI_Asian                               0.026927  0.134996       0.173878   \n",
       "CUI_Beverages                          -0.066371  0.103059       0.217276   \n",
       "CUI_Cafe                               -0.030914  0.006832       0.052662   \n",
       "CUI_Chicken_Dishes                     -0.026319  0.116664       0.173162   \n",
       "CUI_Chinese                            -0.048720  0.106115       0.189395   \n",
       "CUI_Desserts                           -0.049974  0.034422       0.113834   \n",
       "CUI_Healthy                            -0.043395  0.033129       0.097381   \n",
       "CUI_Indian                             -0.040390  0.102434       0.185433   \n",
       "CUI_Italian                             0.000771  0.133291       0.188213   \n",
       "CUI_Japanese                           -0.062809  0.081741       0.187344   \n",
       "CUI_Noodle_Dishes                      -0.049222  0.091998       0.175486   \n",
       "CUI_OTHER                              -0.022687  0.199450       0.271743   \n",
       "CUI_Street_Food/Snacks                 -0.026559  0.075274       0.133339   \n",
       "CUI_Thai                               -0.030759  0.051600       0.113304   \n",
       "is_chain                                0.727238  0.359597      -0.355419   \n",
       "Orders_Night                            0.156379  0.266130       0.206395   \n",
       "Orders_Dawn                             0.166701  0.310477       0.260687   \n",
       "Orders_Morning                          0.471672  0.409677      -0.010273   \n",
       "Orders_Afternoon                        0.502094  0.307815      -0.196626   \n",
       "Orders_Evening                          0.457202  0.174555      -0.334636   \n",
       "Orders_Dusk                             0.275351  0.045480      -0.287962   \n",
       "Age_Group                               0.008039  0.007902       0.002042   \n",
       "vendor_count                            0.886126  0.613952      -0.184805   \n",
       "product_count                           0.946879  0.795042      -0.033330   \n",
       "Total_Orders_Per_Client                 1.000000  0.682539      -0.218448   \n",
       "mnt                                     0.682539  1.000000       0.512704   \n",
       "mnt_Per_Order                          -0.218448  0.512704       1.000000   \n",
       "Items_Per_Order                         0.181313  0.565664       0.521619   \n",
       "frq                                    -0.386786 -0.197222       0.110296   \n",
       "rcn                                    -0.491767 -0.378727       0.063692   \n",
       "activity                                0.832197  0.566966      -0.186461   \n",
       "\n",
       "                         Items_Per_Order       frq       rcn  activity  \n",
       "DOW_0                           0.090421 -0.078086 -0.241139  0.398360  \n",
       "DOW_1                           0.090598 -0.072986 -0.251493  0.397150  \n",
       "DOW_2                           0.099674 -0.087117 -0.273226  0.417149  \n",
       "DOW_3                           0.095464 -0.088083 -0.299676  0.421766  \n",
       "DOW_4                           0.095874 -0.091739 -0.302244  0.404916  \n",
       "DOW_5                           0.099923 -0.076712 -0.206894  0.356761  \n",
       "DOW_6                           0.110594 -0.103022 -0.220279  0.386163  \n",
       "CLV_Score                       0.125428  0.574515 -0.406842 -0.183989  \n",
       "RFM_Score                       0.102301  0.620062 -0.364907 -0.247917  \n",
       "Loyalty                         0.084251 -0.171074 -0.305183  0.495155  \n",
       "CUI_American                    0.139976  0.074937 -0.001536 -0.047178  \n",
       "CUI_Asian                       0.198714  0.027655 -0.014886  0.009899  \n",
       "CUI_Beverages                   0.074321  0.056381  0.010748 -0.061953  \n",
       "CUI_Cafe                        0.025960  0.022728  0.010790 -0.025276  \n",
       "CUI_Chicken_Dishes              0.078059  0.039513 -0.010467 -0.025297  \n",
       "CUI_Chinese                     0.071722  0.033248 -0.005776 -0.028910  \n",
       "CUI_Desserts                    0.051868  0.034559  0.014500 -0.045942  \n",
       "CUI_Healthy                     0.043988  0.037996  0.015912 -0.042946  \n",
       "CUI_Indian                      0.080411  0.029767  0.010372 -0.034492  \n",
       "CUI_Italian                     0.121660  0.019594 -0.004616 -0.003151  \n",
       "CUI_Japanese                    0.099722  0.055034  0.027812 -0.059256  \n",
       "CUI_Noodle_Dishes               0.072472  0.031145  0.014812 -0.043722  \n",
       "CUI_OTHER                       0.126142  0.060305 -0.019620 -0.014624  \n",
       "CUI_Street_Food/Snacks          0.147282  0.018881  0.007585 -0.026195  \n",
       "CUI_Thai                        0.051293  0.015079  0.011523 -0.020498  \n",
       "is_chain                        0.084938 -0.239551 -0.333840  0.601557  \n",
       "Orders_Night                    0.102970 -0.059586 -0.086558  0.127563  \n",
       "Orders_Dawn                     0.074402 -0.073573 -0.104074  0.146064  \n",
       "Orders_Morning                  0.104077 -0.104287 -0.260038  0.388502  \n",
       "Orders_Afternoon                0.092326 -0.124388 -0.269009  0.422268  \n",
       "Orders_Evening                  0.040564 -0.118235 -0.216990  0.385903  \n",
       "Orders_Dusk                     0.029254 -0.037365 -0.114656  0.218687  \n",
       "Age_Group                       0.003364 -0.003050 -0.012532  0.006400  \n",
       "vendor_count                    0.178186 -0.367862 -0.442191  0.755993  \n",
       "product_count                   0.465396 -0.339693 -0.468386  0.784038  \n",
       "Total_Orders_Per_Client         0.181313 -0.386786 -0.491767  0.832197  \n",
       "mnt                             0.565664 -0.197222 -0.378727  0.566966  \n",
       "mnt_Per_Order                   0.521619  0.110296  0.063692 -0.186461  \n",
       "Items_Per_Order                 1.000000 -0.019855 -0.092306  0.140074  \n",
       "frq                            -0.019855  1.000000  0.337825 -0.718351  \n",
       "rcn                            -0.092306  0.337825  1.000000 -0.590171  \n",
       "activity                        0.140074 -0.718351 -0.590171  1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Spearman correlation matrix\n",
    "spearman_corr = num_df.corr(method='spearman')\n",
    "\n",
    "# Display the correlation matrix\n",
    "spearman_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZR2YIa8uY7nF",
    "outputId": "5422db7c-629d-4dde-ae7d-b475d284ba63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations above 0.5:\n",
      "CLV_Score - RFM_Score: 0.989\n",
      "Total_Orders_Per_Client - product_count: 0.947\n",
      "Total_Orders_Per_Client - vendor_count: 0.886\n",
      "product_count - vendor_count: 0.842\n",
      "Total_Orders_Per_Client - activity: 0.832\n",
      "mnt - product_count: 0.795\n",
      "activity - product_count: 0.784\n",
      "activity - vendor_count: 0.756\n",
      "Total_Orders_Per_Client - is_chain: 0.727\n",
      "activity - frq: -0.718\n",
      "is_chain - vendor_count: 0.705\n"
     ]
    }
   ],
   "source": [
    "# Get correlations above 0.5 (excluding self-correlations)\n",
    "strong_corrs = [(i, j, spearman_corr.loc[i, j])\n",
    "                for i in spearman_corr.index\n",
    "                for j in spearman_corr.columns\n",
    "                if abs(spearman_corr.loc[i, j]) > 0.7 and i < j]\n",
    "\n",
    "# Print sorted results\n",
    "print(\"Correlations above 0.5:\")\n",
    "for var1, var2, corr in sorted(strong_corrs, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f\"{var1} - {var2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJzWANaLZipw",
    "outputId": "76da4f85-6a7f-4928-dab6-bebd43511b81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6',\n",
       "       'CLV_Score', 'RFM_Score', 'Loyalty', 'CUI_American', 'CUI_Asian',\n",
       "       'CUI_Beverages', 'CUI_Cafe', 'CUI_Chicken_Dishes', 'CUI_Chinese',\n",
       "       'CUI_Desserts', 'CUI_Healthy', 'CUI_Indian', 'CUI_Italian',\n",
       "       'CUI_Japanese', 'CUI_Noodle_Dishes', 'CUI_OTHER',\n",
       "       'CUI_Street_Food/Snacks', 'CUI_Thai', 'is_chain', 'Orders_Night',\n",
       "       'Orders_Dawn', 'Orders_Morning', 'Orders_Afternoon', 'Orders_Evening',\n",
       "       'Orders_Dusk', 'Age_Group', 'vendor_count', 'product_count',\n",
       "       'Total_Orders_Per_Client', 'mnt', 'mnt_Per_Order', 'Items_Per_Order',\n",
       "       'frq', 'rcn', 'activity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(df_pca):\n",
    "    # Initialize PCA\n",
    "    pca = PCA()\n",
    "    pca_feat = pca.fit_transform(df_pca)\n",
    "\n",
    "    # Calculate cumulative explained variance\n",
    "    explained_variance = pca.explained_variance_\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "    # Find the number of components needed to reach 80% variance\n",
    "    num_components = np.argmax(cumulative_explained_variance_ratio >= 0.8) + 1\n",
    "\n",
    "    # Add PCA components explaining 80% variance back to DataFrame\n",
    "    selected_pcas = pca_feat[:, :num_components]\n",
    "    for i in range(num_components):\n",
    "        df_pca[f'PCA_{i+1}'] = selected_pcas[:, i]\n",
    "\n",
    "    # Plotting\n",
    "    # Create figure and subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6), facecolor='white')\n",
    "\n",
    "    # Plot 1: Scree Plot (Eigenvalues)\n",
    "    ax1.plot(range(1, len(explained_variance) + 1), explained_variance, 'ro-', linewidth=2, markersize=8, label='Eigenvalues')\n",
    "    ax1.set_title(\"Scree Plot (Eigenvalues)\", fontsize=18, pad=15, fontweight='bold')\n",
    "    ax1.set_xlabel(\"Principal Components\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Eigenvalue\", fontsize=14)\n",
    "    ax1.set_xticks(range(1, len(explained_variance) + 1, 2))\n",
    "    ax1.set_xticklabels(range(1, len(explained_variance) + 1, 2), fontsize=12)\n",
    "    ax1.set_facecolor('white')\n",
    "    ax1.grid(True, color='black', linestyle='-', linewidth=0.8, alpha=0.4)  # Lighter grid lines\n",
    "    ax1.tick_params(axis='both', which='both', colors='black', width=1)  # Black ticks with better visibility\n",
    "    ax1.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "    # Plot 2: Explained Variance as a Bar Chart\n",
    "    ax2.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, \n",
    "            color='#1f77b4', alpha=0.7, label='Explained Variance (%)', width=0.8)  # Customize bar color\n",
    "    ax2.plot(range(1, len(cumulative_explained_variance_ratio) + 1), cumulative_explained_variance_ratio, \n",
    "             'ro-', linewidth=2, markersize=8, label='Cumulative Variance (%)')\n",
    "\n",
    "    # Customize the plot\n",
    "    ax2.set_title(\"Variance Explained by Principal Components\", fontsize=18, pad=15, fontweight='bold')\n",
    "    ax2.set_xlabel(\"Principal Components\", fontsize=14)\n",
    "    ax2.set_ylabel(\"Variance (%)\", fontsize=14)\n",
    "    ax2.set_xticks(range(1, len(explained_variance_ratio) + 1, 2))  # Show every 2nd PC for clarity\n",
    "    ax2.set_xticklabels(range(1, len(explained_variance_ratio) + 1, 2), fontsize=12)\n",
    "    ax2.set_facecolor('white')\n",
    "    ax2.grid(True, color='black', linestyle='-', linewidth=0.8, alpha=0.4)  # Lighter grid lines\n",
    "    ax2.tick_params(axis='both', which='both', colors='black', width=1)  # Black ticks with better visibility\n",
    "\n",
    "    # Add legend\n",
    "    ax2.legend(fontsize=12, loc='upper left')\n",
    "\n",
    "    # Adjust layout to ensure everything fits and has proper padding\n",
    "    plt.tight_layout(pad=4.0)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    return df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(df):\n",
    "   # Get cluster labels\n",
    "   kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "   labels = kmeans.fit_predict(df)\n",
    "\n",
    "   # Train RF classifier on clusters\n",
    "   rf = RandomForestClassifier(random_state=42)\n",
    "   rf.fit(df, labels)\n",
    "\n",
    "   # Get feature importance\n",
    "   importance = pd.DataFrame({\n",
    "       'feature': df.columns,\n",
    "       'importance': rf.feature_importances_\n",
    "   }).sort_values('importance', ascending=False)\n",
    "\n",
    "   return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_analysis(df, k_range=(3, 11), random_state=42):\n",
    "\n",
    "    # Ensure input is a NumPy array\n",
    "    np_array = df.values if isinstance(df, pd.DataFrame) else df\n",
    "\n",
    "    # Clustering metrics\n",
    "    metrics = []\n",
    "    for k in range(k_range[0], k_range[1]):\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=random_state)\n",
    "        labels = kmeans.fit_predict(np_array)\n",
    "\n",
    "        # Calculate SSE\n",
    "        sse = np.sum((np_array - kmeans.cluster_centers_[labels]) ** 2)\n",
    "\n",
    "        # Calculate R2\n",
    "        r2 = r2_score(np_array, kmeans.cluster_centers_[labels])\n",
    "\n",
    "        metrics.append({\n",
    "            'k': k,\n",
    "            'silhouette': silhouette_score(np_array, labels),\n",
    "            'calinski': calinski_harabasz_score(np_array, labels),\n",
    "            'davies': davies_bouldin_score(np_array, labels),\n",
    "            'inertia': kmeans.inertia_,\n",
    "            'sse': sse,\n",
    "            'r2': r2\n",
    "        })\n",
    "\n",
    "    # Print metrics\n",
    "    for m in metrics:\n",
    "        print(f\"\\nk={m['k']}:\")\n",
    "        print(f\"Silhouette: {m['silhouette']:.3f}\")\n",
    "        print(f\"Calinski-Harabasz: {m['calinski']:.2f}\")\n",
    "        print(f\"Davies-Bouldin: {m['davies']:.2f}\")\n",
    "        print(f\"SSE: {m['sse']:.2f}\")\n",
    "        print(f\"R2: {m['r2']:.3f}\")\n",
    "        print(f\"Inertia: {m['inertia']:.2f}\")\n",
    "\n",
    "    # Select best k based on Silhouette score\n",
    "    best_k = max(metrics, key=lambda x: x['silhouette'])['k']\n",
    "    kmeans_final = KMeans(n_clusters=best_k, init='k-means++', random_state=random_state)\n",
    "    labels = kmeans_final.fit_predict(np_array)\n",
    "\n",
    "    # Update DataFrame with cluster labels\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df['cluster'] = labels\n",
    "\n",
    "    # T-SNE visualization\n",
    "    tsne = TSNE(random_state=random_state)\n",
    "    embedding = tsne.fit_transform(np_array)\n",
    "\n",
    "    # Plotting\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # T-SNE plot\n",
    "    scatter = ax1.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='viridis')\n",
    "    ax1.set_title(f'T-SNE visualization of {best_k} clusters')\n",
    "    plt.colorbar(scatter, ax=ax1)\n",
    "\n",
    "    # Elbow plot\n",
    "    ax2.plot(range(k_range[0], k_range[1]), [m['inertia'] for m in metrics], 'bo-')\n",
    "    ax2.set_xlabel('Number of clusters (k)')\n",
    "    ax2.set_ylabel('Inertia')\n",
    "    ax2.set_title('Elbow Method')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clustering_methods(data):\n",
    "    metrics = []\n",
    "\n",
    "    for n_clusters in range(2, 8):\n",
    "        # K-Means Clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(data)\n",
    "        kmeans_labels = kmeans.labels_\n",
    "\n",
    "        # Hierarchical Clustering\n",
    "        linkage_matrix = linkage(data, method='ward')  # Use Ward's method\n",
    "        hierarchical_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "\n",
    "        # Calculate metrics\n",
    "        kmeans_silhouette = silhouette_score(data, kmeans_labels)\n",
    "        kmeans_calinski = calinski_harabasz_score(data, kmeans_labels)\n",
    "        kmeans_davies = davies_bouldin_score(data, kmeans_labels)\n",
    "        kmeans_sse = kmeans.inertia_\n",
    "        kmeans_r2 = 1 - (kmeans_sse / np.var(data, axis=0).sum())\n",
    "\n",
    "        metrics.append({\n",
    "            'k': n_clusters,\n",
    "            'silhouette': kmeans_silhouette,\n",
    "            'calinski': kmeans_calinski,\n",
    "            'davies': kmeans_davies,\n",
    "            'sse': kmeans_sse,\n",
    "            'r2': kmeans_r2,\n",
    "            'inertia': kmeans.inertia_\n",
    "        })\n",
    "\n",
    "    # Print the metrics\n",
    "    for m in metrics:\n",
    "        print(f\"\\nk={m['k']}:\")\n",
    "        print(f\"Silhouette: {m['silhouette']:.3f}\")\n",
    "        print(f\"Calinski-Harabasz: {m['calinski']:.2f}\")\n",
    "        print(f\"Davies-Bouldin: {m['davies']:.2f}\")\n",
    "        print(f\"SSE: {m['sse']:.2f}\")\n",
    "        print(f\"R2: {m['r2']:.3f}\")\n",
    "        print(f\"Inertia: {m['inertia']:.2f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "emBgdw5VZ2VX"
   },
   "outputs": [],
   "source": [
    "value_engagement_metrics = [\n",
    "    # Value\n",
    "    'mnt', 'mnt_Per_Order', 'CLV_Score', 'RFM_Score',\n",
    "    # Engagement\n",
    "    'activity', 'Loyalty', 'frq', 'rcn' , 'Age_Group']\n",
    "\n",
    "preference_metrics = [\n",
    "    # Cuisine preferences\n",
    "    'CUI_American', 'CUI_Asian', 'CUI_Beverages', 'CUI_Cafe',\n",
    "    'CUI_Chicken_Dishes', 'CUI_Chinese', 'CUI_Desserts', 'CUI_Healthy',\n",
    "    'CUI_Indian', 'CUI_Italian', 'CUI_Japanese', 'CUI_Noodle_Dishes',\n",
    "    'CUI_OTHER', 'CUI_Street_Food/Snacks', 'CUI_Thai'\n",
    "]\n",
    "\n",
    "shopping_behavior_metrics = [\n",
    "    # Shopping patterns\n",
    "    'vendor_count', 'product_count', 'is_chain', 'Items_Per_Order',\n",
    "    # Timing preferences\n",
    "    'Orders_Night', 'Orders_Dawn', 'Orders_Morning', 'Orders_Afternoon',\n",
    "    'Orders_Evening', 'Orders_Dusk',\n",
    "    'DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6'\n",
    "]\n",
    "\n",
    "demographic_metrics = [\n",
    "    'customer_region_0', 'customer_region_1', 'customer_region_2',\n",
    "    'customer_region_3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "s_6IUQ51Z_KU"
   },
   "outputs": [],
   "source": [
    "df_val = num_df[value_engagement_metrics].copy()\n",
    "df_pref = num_df[preference_metrics].copy()\n",
    "df_shop = num_df[shopping_behavior_metrics].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4b54A52ab4f"
   },
   "source": [
    "time to reduce feature amount to label data better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztPKugQmaNWG",
    "outputId": "1bab52f4-50c2-48ff-c2aa-ea1b90372a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in df_pref: 15\n",
      "Number of columns in df_shop: 17\n",
      "Number of columns in df_val: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns in df_pref: {df_pref.shape[1]}\")\n",
    "print(f\"Number of columns in df_shop: {df_shop.shape[1]}\")\n",
    "print(f\"Number of columns in df_val: {df_val.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr9t8lc9-y4e"
   },
   "source": [
    "Balancing the Cuisine groupings to not have a majority customer group dominate the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference Based Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaebgvj7abNw",
    "outputId": "324cbb0e-b723-4851-d1d8-17615226c2c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Totals and Percentages:\n",
      "Other_Asian    :        0 ( 70.9%)\n",
      "General_Asian  :       -0 (-61.3%)\n",
      "Western        :        0 ( 35.8%)\n",
      "Beverages_Cafe :        0 ( 29.7%)\n",
      "Desserts_Snacks:       -0 (-24.9%)\n",
      "Main_Dishes    :        0 ( 49.8%)\n",
      "\n",
      "Total Sum:             0\n"
     ]
    }
   ],
   "source": [
    "cuisine_groups = {\n",
    "    'Other_Asian': ['CUI_Chinese', 'CUI_Japanese', 'CUI_Noodle_Dishes','CUI_Thai', 'CUI_Indian'],\n",
    "    'General_Asian' : ['CUI_Asian'],\n",
    "    'Western': ['CUI_American', 'CUI_Italian'],\n",
    "    'Beverages_Cafe': ['CUI_Beverages', 'CUI_Cafe'],\n",
    "    'Desserts_Snacks': ['CUI_Desserts', 'CUI_Street_Food/Snacks'],\n",
    "    'Main_Dishes': ['CUI_Chicken_Dishes', 'CUI_Healthy', 'CUI_OTHER']\n",
    "}\n",
    "\n",
    "# Calculate group totals and percentages\n",
    "group_totals = {}\n",
    "total_sum = 0\n",
    "\n",
    "for group, cuisines in cuisine_groups.items():\n",
    "    group_sum = num_df[cuisines].sum().sum()\n",
    "    group_totals[group] = group_sum\n",
    "    total_sum += group_sum\n",
    "\n",
    "# Print results\n",
    "print(\"Group Totals and Percentages:\")\n",
    "for group, total in group_totals.items():\n",
    "    percentage = (total / total_sum) * 100\n",
    "    print(f\"{group:15s}: {total:8.0f} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal Sum:      {total_sum:8.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kIlbcn6WcWFK"
   },
   "outputs": [],
   "source": [
    "# Create copy\n",
    "pref_clean = df_pref.copy()\n",
    "\n",
    "# Initialize new columns for each group\n",
    "group_columns = {\n",
    "    'Other_Asian': ['CUI_Chinese', 'CUI_Japanese', 'CUI_Noodle_Dishes', 'CUI_Thai', 'CUI_Indian'],\n",
    "    'General_Asian': ['CUI_Asian'],\n",
    "    'Western': ['CUI_American', 'CUI_Italian'],\n",
    "    'Beverages_Cafe': ['CUI_Beverages', 'CUI_Cafe'],\n",
    "    'Desserts_Snacks': ['CUI_Desserts', 'CUI_Street_Food/Snacks'],\n",
    "    'Main_Dishes': ['CUI_Chicken_Dishes', 'CUI_Healthy', 'CUI_OTHER']\n",
    "}\n",
    "\n",
    "# Create grouped columns\n",
    "for group, cuisines in group_columns.items():\n",
    "    pref_clean[group] = df_pref[cuisines].sum(axis=1)\n",
    "\n",
    "# Keep only the new grouped columns\n",
    "pref_clean = pref_clean[list(group_columns.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEW6Cf-PEerD"
   },
   "source": [
    "loop to decide the ideal elbow point for this segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pref_clean = pca(pref_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3DcFBrdM_8Ze",
    "outputId": "61b23d9c-da1e-4d93-ff31-35ae785e49b5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k=3:\n",
      "Silhouette: 0.679\n",
      "Calinski-Harabasz: 5914.59\n",
      "Davies-Bouldin: 0.97\n",
      "SSE: 105720.11\n",
      "R2: 0.264\n",
      "Inertia: 105720.11\n",
      "\n",
      "k=4:\n",
      "Silhouette: 0.675\n",
      "Calinski-Harabasz: 5614.25\n",
      "Davies-Bouldin: 0.88\n",
      "SSE: 94807.78\n",
      "R2: 0.340\n",
      "Inertia: 94807.78\n",
      "\n",
      "k=5:\n",
      "Silhouette: 0.632\n",
      "Calinski-Harabasz: 5875.61\n",
      "Davies-Bouldin: 0.88\n",
      "SSE: 83375.75\n",
      "R2: 0.426\n",
      "Inertia: 83375.75\n",
      "\n",
      "k=6:\n",
      "Silhouette: 0.622\n",
      "Calinski-Harabasz: 5600.61\n",
      "Davies-Bouldin: 0.85\n",
      "SSE: 77092.83\n",
      "R2: 0.443\n",
      "Inertia: 77092.83\n",
      "\n",
      "k=7:\n",
      "Silhouette: 0.506\n",
      "Calinski-Harabasz: 5410.81\n",
      "Davies-Bouldin: 0.86\n",
      "SSE: 71733.56\n",
      "R2: 0.450\n",
      "Inertia: 71733.56\n",
      "\n",
      "k=8:\n",
      "Silhouette: 0.499\n",
      "Calinski-Harabasz: 5517.82\n",
      "Davies-Bouldin: 0.80\n",
      "SSE: 65451.79\n",
      "R2: 0.498\n",
      "Inertia: 65451.79\n",
      "\n",
      "k=9:\n",
      "Silhouette: 0.499\n",
      "Calinski-Harabasz: 5249.37\n",
      "Davies-Bouldin: 0.80\n",
      "SSE: 62458.62\n",
      "R2: 0.539\n",
      "Inertia: 62458.62\n",
      "\n",
      "k=10:\n",
      "Silhouette: 0.487\n",
      "Calinski-Harabasz: 5328.16\n",
      "Davies-Bouldin: 0.80\n",
      "SSE: 57786.95\n",
      "R2: 0.545\n",
      "Inertia: 57786.95\n"
     ]
    }
   ],
   "source": [
    "clustering_analysis(pref_clean, k_range=(3, 11), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compare_clustering_methods(pref_clean, n_clusters=3)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shopping Based Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zBw2ssskjlm"
   },
   "outputs": [],
   "source": [
    "df_shop.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdBTsLgj6WQJ"
   },
   "source": [
    "MinMax scaling with custom ranges to score new features\n",
    "\n",
    "\"\"\"Items_Per_Order (1-2.25):\n",
    "\n",
    "Min 1: Single item is baseline order\n",
    "Max 2.25: Very few customers order >2 items\n",
    "\n",
    "product_count (1-14):\n",
    "\n",
    "Min 1: Customers try at least one product\n",
    "Max 14: Represents power users with wide product exploration\n",
    "Aligns with 75th percentile around 7 products\n",
    "\n",
    "vendor_count (1-8):\n",
    "\n",
    "Min 1: Single vendor loyalty\n",
    "Max 8: Reflects realistic maximum vendor relationships\n",
    "Most customers use 1-3 vendors (75th percentile at 4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PdETxCl9f9R"
   },
   "source": [
    " Replaced log transform with MinMaxScaler for time periods to maintain relative differences on 0-1 scale\n",
    "\n",
    " Normalized DOW cyclic features to match 0-1 scale of other features\n",
    "\n",
    " Kept is_chain to preserve business context\n",
    "\n",
    " Added chain interactions with basket/vendor scores to\n",
    "capture business patterns\n",
    "\n",
    " Unified all features to same scale range to prevent distance calculation skew\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2Do6qKYlbXC"
   },
   "outputs": [],
   "source": [
    "# Create copy and calculate scores\n",
    "editdf = df.copy()\n",
    "editdf['basket_score'] = (editdf['Items_Per_Order'] - 1) / (2.25 - 1)\n",
    "editdf['product_score'] = (editdf['product_count'] - 1) / (14 - 1)\n",
    "editdf['vendor_score'] = (editdf['vendor_count'] - 1) / (8 - 1)\n",
    "\n",
    "# Group into periods, no scaling\n",
    "editdf['daytime_orders'] = editdf['Orders_Morning'] + editdf['Orders_Afternoon']\n",
    "editdf['evening_orders'] = editdf['Orders_Evening'] + editdf['Orders_Dusk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2Do6qKYlbXC"
   },
   "source": [
    "Standard numerical encoding (e.g., Monday = 0, Tuesday = 1, ..., Sunday = 6)\n",
    "fails to account for the cyclic relationship of the data.\n",
    "In numerical encoding, Sunday (6) is treated as far from Monday (0),\n",
    " which is incorrect for many analyses, such as machine learning tasks where distance or similarity matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2Do6qKYlbXC"
   },
   "outputs": [],
   "source": [
    "# DOW cyclic encoding\n",
    "dow_value = (editdf[['DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6']] *\n",
    "            np.array([0, 1, 2, 3, 4, 5, 6])).sum(axis=1) / \\\n",
    "            editdf[['DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6']].sum(axis=1)\n",
    "\n",
    "editdf['DOW_sin'] = np.sin(2 * np.pi * dow_value/7)\n",
    "editdf['DOW_cos'] = np.cos(2 * np.pi * dow_value/7)\n",
    "scaler_dow = MinMaxScaler()\n",
    "editdf[['DOW_sin', 'DOW_cos']] = scaler_dow.fit_transform(editdf[['DOW_sin', 'DOW_cos']])\n",
    "\n",
    "# Chain interactions\n",
    "editdf['chain_basket_interaction'] = editdf['is_chain'] * editdf['basket_score']\n",
    "editdf['chain_vendor_interaction'] = editdf['is_chain'] * editdf['vendor_score']\n",
    "\n",
    "# Drop unneeded columns\n",
    "columns_to_drop = ['vendor_count', 'product_count'] + \\\n",
    "                 [ 'Orders_Morning', 'Orders_Afternoon', 'Orders_Evening', 'Orders_Dusk'] + \\\n",
    "                 ['DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6', 'DOW_sin', 'DOW_cos', 'Orders_Dawn'  ,'Orders_Night', 'Items_Per_Order']\n",
    "editdf = editdf.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPNIR8WwhrJx"
   },
   "outputs": [],
   "source": [
    "shop_clean = editdf[['product_score', 'vendor_score', 'chain_vendor_interaction', 'daytime_orders',\t'evening_orders', 'chain_basket_interaction',]].copy()\n",
    "shop_clean = pd.concat([shop_clean, df_shop], axis=1)\n",
    "columns_to_drop = ['Orders_Night', 'Orders_Dawn', 'Orders_Morning',\n",
    "                   'Orders_Afternoon', 'Orders_Evening', 'vendor_count',\n",
    "                   'product_count', 'Orders_Dusk', 'DOW_0', 'DOW_1',\n",
    "                   'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6',\n",
    "                    'DOW_sin', 'DOW_cos',\n",
    "                    'Items_Per_Order']\n",
    "\n",
    "shop_clean = shop_clean.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyJIl4gYBiM2"
   },
   "source": [
    "Integration of Random Forest and Clustering\n",
    "By integrating random forest **feature importance** with clustering, one can ensure that the most important features within each cluster are identified and prioritized. (Jingsong et al., 2020).\n",
    "\n",
    "*Li, Jingsong., Yang, Ziyue., Hu, Peijun., Zhang, Ying., Wang, Feng. (2020). Feature importance sorting system based on random forest algorithm in multi-center mode.   *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_clean = pca(shop_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on preprocessed data\n",
    "importance = analyze_feature_importance(shop_clean)\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqVkbaARFHPM"
   },
   "source": [
    "loop to decide the ideal elbow point for this segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Zj75c--_A-Rh",
    "outputId": "b66a7754-3eba-4073-f51d-5e14abd7388e"
   },
   "outputs": [],
   "source": [
    "clustering_analysis(shop_clean, k_range=(3, 11), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compare_clustering_methods(shop_clean, n_clusters=3)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr7iU2grGkRB"
   },
   "source": [
    "iteratively with manual permutation the balance of a R2 > 0.5 and 0.5 silhouette based on industry measures was sought after and with trial and error was found through kmeans++\n",
    "\n",
    "Kaufman & Rousseeuw (1990) Establishes silhouette > 0.5 as indicating reasonable to strong structure\n",
    "\n",
    "in combination with:\n",
    "\n",
    "Cohen, J. (1992) R > 0.5 as indicating large effect size\n",
    "\n",
    "Kaufman, L., & Rousseeuw, P. J. (1990). Finding groups in data: An introduction to cluster analysis (1st ed.). John Wiley & Sons. https://doi.org/10.1002/9780470316801\n",
    "\n",
    "Cohen, J. (1992). A power primer. Psychological Bulletin, 112(1), 155-159. https://doi.org/10.1037/0033-2909.112.1.155\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMos1oi72KxW"
   },
   "source": [
    "In an initial impression, I find that the code here below might seem overwhelming and over-engineered. However, based on the methodology established above of wanting to have a silhouette above 0.5 and an R2 above 0.5 to have a symbiosis effect for capturing enough variance and a large effect in a practical sense and having well-defined clusters, it was, in the manual iterative process of scoring importance, seemingly important to combine certain measures to achieve the desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Based Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8_iNotxJmUn"
   },
   "outputs": [],
   "source": [
    "def preprocess_df_val(df):\n",
    "\n",
    "    # Initialize with core metrics\n",
    "    val_clean = df[['mnt', 'activity', 'frq', 'rcn', 'CLV_Score']].copy()\n",
    "\n",
    "    # CRITICAL CHECK: Identify binary columns\n",
    "    binary_cols = val_clean.apply(lambda x: len(x.unique()) <= 2).to_dict()\n",
    "\n",
    "    # BOUNDARY CONDITIONS\n",
    "    max_value = 1e15\n",
    "    val_clean = val_clean.clip(upper=max_value)\n",
    "\n",
    "    # Mathematical safeguards for non-binary columns only\n",
    "    val_clean['mnt'] = np.maximum(val_clean['mnt'], 0.01)\n",
    "    val_clean['frq'] = np.maximum(val_clean['frq'], 0)\n",
    "    val_clean['rcn'] = np.maximum(val_clean['rcn'], 0)\n",
    "\n",
    "    # DOUBLE LOG TRANSFORMATION ERROR DETECTED IN ORIGINAL CODE!\n",
    "    # Single log transform for CLV - prevents information loss from double scaling\n",
    "    val_clean['CLV_Score'] = np.log1p(val_clean['CLV_Score'])\n",
    "\n",
    "    # Strong Features (s_i)\n",
    "    val_clean['value_retention'] = val_clean['mnt'] / np.maximum(val_clean['activity'], 1)  # Binary division\n",
    "\n",
    "    val_clean['value_growth'] = val_clean['mnt'] / (np.maximum(val_clean['rcn'], 1) *\n",
    "                                                   np.maximum(val_clean['frq'], 1))\n",
    "\n",
    "    # Medium Features (m_i)\n",
    "    val_clean['value_growth_rate'] = val_clean['value_growth'] * val_clean['frq']\n",
    "\n",
    "    val_clean['growth_momentum'] = val_clean['value_growth'] * (val_clean['mnt'] /\n",
    "                                                              np.maximum(val_clean['rcn'], 1))\n",
    "\n",
    "    # Weak Features (w_i)\n",
    "    val_clean['spend_consistency'] = val_clean['mnt'] / np.maximum(val_clean['rcn'], 1)\n",
    "\n",
    "    val_clean['avg_order_value'] = val_clean['mnt'] / np.maximum(val_clean['frq'], 1)\n",
    "\n",
    "    val_clean['active_spend_rate'] = (val_clean['mnt'] * val_clean['activity']) / \\\n",
    "                                    np.maximum(val_clean['rcn'], 1)\n",
    "\n",
    "    final_cols = ['CLV_Score', 'value_retention', 'value_growth', 'value_growth_rate',\n",
    "                 'growth_momentum', 'spend_consistency', 'avg_order_value', 'active_spend_rate']\n",
    "\n",
    "    # Handle infinities before scaling\n",
    "    val_clean[final_cols] = val_clean[final_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    val_clean[final_cols] = val_clean[final_cols].fillna(val_clean[final_cols].mean())\n",
    "\n",
    "    # CRITICAL SCALING CORRECTION: Only scale non-binary columns\n",
    "    scaler = StandardScaler()\n",
    "    val_clean[final_cols] = scaler.fit_transform(val_clean[final_cols])\n",
    "\n",
    "    return val_clean[final_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "2efXpix3KZ1U",
    "outputId": "b345ee8a-f1c5-4df7-bb02-f5ae93b1653a"
   },
   "outputs": [],
   "source": [
    "# Usage:\n",
    "val_clean = preprocess_df_val(df_val)\n",
    "\n",
    "val_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "rh1fYdL4SmnA",
    "outputId": "06b3465b-314a-4f50-ae78-52c1f8b3a5d3"
   },
   "outputs": [],
   "source": [
    "val_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6cT18Pi2dBl"
   },
   "source": [
    "loop of kmeans testing once again to find the direct effect of features engineered in manual repeated permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkVoJkbH1dSV"
   },
   "source": [
    "The code assigns cluster labels from KMeans as the target variable for the RandomForestClassifier. This approach is to understand which features contribute most to the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYXSYp26QnuL",
    "outputId": "f9914996-180f-471f-dbdf-8d69f56a4391"
   },
   "outputs": [],
   "source": [
    "# Run analysis on preprocessed data\n",
    "importance = analyze_feature_importance(val_clean)\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPFGofngU1PT"
   },
   "source": [
    "using forest FI i've dropped age_group and loyalty , high value ratio as they weren't adding significant value, RFM value dropped too after too much irrelevance based on tree testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_analysis(val_clean, k_range=(3, 11), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ0FS-iRzsuw"
   },
   "outputs": [],
   "source": [
    "metrics = compare_clustering_methods(val_clean, n_clusters=3)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGfcAkMII3hZ"
   },
   "source": [
    "# Below is a second run of SOM cluster models and optuna grid searches to have a comparison and verification of proper clustering with k-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiDwmhRD7Man"
   },
   "outputs": [],
   "source": [
    "# # grid search SOM on val_clean\n",
    "\n",
    "# def objective(trial):\n",
    "#     \"\"\"Objective function for Optuna to optimize.\"\"\"\n",
    "#     # Define the search space for hyperparameters\n",
    "#     params = {\n",
    "#         'x_dim': trial.suggest_int('x_dim', 2, 10),  # Grid search for x_dim\n",
    "#         'y_dim': trial.suggest_int('y_dim', 2, 10),  # Grid search for y_dim\n",
    "#         'sigma': trial.suggest_float('sigma', 0.1, 5.0),  # Grid search for sigma\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),  # Grid search for learning_rate\n",
    "#         'epochs': trial.suggest_int('epochs', 50, 500),  # Grid search for epochs\n",
    "#     }\n",
    "\n",
    "#     # Train and evaluate the SOM\n",
    "#     som, labels, metrics, _ = train_and_evaluate_som(val_clean_array, params)\n",
    "\n",
    "#     # Use silhouette score as the objective to maximize\n",
    "#     return metrics['silhouette_score']\n",
    "\n",
    "# # Run the Optuna study\n",
    "# study = optuna.create_study(direction='maximize')  # Maximize silhouette score\n",
    "# study.optimize(objective, n_trials=50)  # Run 50 trials\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(\"\\nBest Parameters:\")\n",
    "# print(study.best_params)\n",
    "# print(\"\\nBest Silhouette Score:\")\n",
    "# print(study.best_value)\n",
    "\n",
    "# # Train the SOM with the best parameters\n",
    "# best_params = study.best_params\n",
    "# som, labels, metrics, fig = train_and_evaluate_som(val_clean_array, best_params)\n",
    "\n",
    "# # Print the evaluation metrics for the best model\n",
    "# print(\"\\nEvaluation Metrics for Best Model:\")\n",
    "# for metric, value in metrics.items():\n",
    "#     print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# # Display the visualization for the best model\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmWX9B0s3Y0S"
   },
   "source": [
    "Best parameters: {'y_dim': 2, 'x_dim': 2, 'sigma': 1.0, 'learning_rate': 0.46415888336127775, 'epochs': 200}\n",
    "Best silhouette score: 0.5063111378206575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoCJTIXUzy5w"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SOM:\n",
    "    def __init__(self, x_dim, y_dim, input_dim, learning_rate=0.1, sigma=None):\n",
    "        \"\"\"Initialize the SOM with given dimensions and parameters.\"\"\"\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigma = sigma if sigma is not None else max(x_dim, y_dim) / 2\n",
    "        self.weights = np.random.randn(x_dim, y_dim, input_dim)\n",
    "        self.locations = np.array([(i, j) for i in range(x_dim) for j in range(y_dim)])\n",
    "\n",
    "    def find_bmu(self, x):\n",
    "        \"\"\"Find the Best Matching Unit for input vector x.\"\"\"\n",
    "        distances = cdist(x.reshape(1, -1), self.weights.reshape(-1, self.input_dim))\n",
    "        return np.unravel_index(distances.argmin(), (self.x_dim, self.y_dim))\n",
    "\n",
    "    def get_cluster_labels(self, data):\n",
    "        \"\"\"Assign cluster labels to input data.\"\"\"\n",
    "        data_array = self._ensure_numpy_array(data)\n",
    "        labels = []\n",
    "        for x in data_array:\n",
    "            bmu = self.find_bmu(x)\n",
    "            labels.append(bmu[0] * self.y_dim + bmu[1])\n",
    "        return np.array(labels)\n",
    "\n",
    "    def update_weights(self, x, bmu, iteration, max_iter):\n",
    "        \"\"\"Update network weights based on input and BMU.\"\"\"\n",
    "        lr = self.learning_rate * np.exp(-iteration / max_iter)\n",
    "        sigma = self.sigma * np.exp(-iteration / max_iter)\n",
    "        dist = cdist(self.locations, [bmu]).reshape(self.x_dim, self.y_dim)\n",
    "        influence = np.exp(-dist ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "        for i in range(self.x_dim):\n",
    "            for j in range(self.y_dim):\n",
    "                self.weights[i, j] += lr * influence[i, j] * (x - self.weights[i, j])\n",
    "\n",
    "    def _ensure_numpy_array(self, data):\n",
    "        \"\"\"Convert input data to numpy array regardless of input type.\"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            return data.values\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            return data\n",
    "        else:\n",
    "            return np.array(data)\n",
    "\n",
    "    def train(self, data, epochs):\n",
    "        \"\"\"Train the SOM on input data.\"\"\"\n",
    "        data_array = self._ensure_numpy_array(data)\n",
    "\n",
    "        # Initialize weights to be in the same range as the input data\n",
    "        data_min = np.min(data_array, axis=0)\n",
    "        data_max = np.max(data_array, axis=0)\n",
    "        self.weights = np.random.uniform(\n",
    "            low=data_min,\n",
    "            high=data_max,\n",
    "            size=(self.x_dim, self.y_dim, self.input_dim)\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(len(data_array))\n",
    "            np.random.shuffle(indices)\n",
    "            shuffled_data = data_array[indices]\n",
    "            for i, x in enumerate(shuffled_data):\n",
    "                bmu = self.find_bmu(x)\n",
    "                self.update_weights(x, bmu, epoch * len(data_array) + i, epochs * len(data_array))\n",
    "\n",
    "    def visualize_clusters(self, data, labels):\n",
    "        \"\"\"Create visualizations for the SOM clusters.\"\"\"\n",
    "        data_array = self._ensure_numpy_array(data)\n",
    "        fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # 1. U-Matrix (Weight distances)\n",
    "        ax1 = fig.add_subplot(131)\n",
    "        umatrix = np.zeros((self.x_dim, self.y_dim))\n",
    "        for i in range(self.x_dim):\n",
    "            for j in range(self.y_dim):\n",
    "                neighbors = []\n",
    "                if i > 0: neighbors.append(self.weights[i-1, j])\n",
    "                if i < self.x_dim-1: neighbors.append(self.weights[i+1, j])\n",
    "                if j > 0: neighbors.append(self.weights[i, j-1])\n",
    "                if j < self.y_dim-1: neighbors.append(self.weights[i, j+1])\n",
    "                umatrix[i, j] = np.mean([np.linalg.norm(self.weights[i, j] - neighbor) for neighbor in neighbors])\n",
    "        sns.heatmap(umatrix, ax=ax1, cmap='viridis')\n",
    "        ax1.set_title('U-Matrix\\n(Weight Distances)')\n",
    "\n",
    "        # 2. Cluster assignments\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        cluster_map = np.zeros((self.x_dim, self.y_dim))\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            mask = (labels == label)\n",
    "            if np.any(mask):\n",
    "                points = data_array[mask]\n",
    "                for point in points:\n",
    "                    bmu = self.find_bmu(point)\n",
    "                    cluster_map[bmu] = label\n",
    "        sns.heatmap(cluster_map, ax=ax2, cmap='Set3')\n",
    "        ax2.set_title('Cluster Assignments')\n",
    "\n",
    "        # 3. Hit map (sample density)\n",
    "        ax3 = fig.add_subplot(133)\n",
    "        hit_map = np.zeros((self.x_dim, self.y_dim))\n",
    "        for x in data_array:\n",
    "            bmu = self.find_bmu(x)\n",
    "            hit_map[bmu] += 1\n",
    "        sns.heatmap(hit_map, ax=ax3, cmap='YlOrRd')\n",
    "        ax3.set_title('Hit Map\\n(Sample Density)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "def evaluate_som(data, labels):\n",
    "    \"\"\"Calculate evaluation metrics for the SOM clustering.\"\"\"\n",
    "    data_array = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "    silhouette = silhouette_score(data_array, labels)\n",
    "    calinski = calinski_harabasz_score(data_array, labels)\n",
    "\n",
    "    # Calculate R using cluster centroids\n",
    "    centroids = np.array([data_array[labels == i].mean(axis=0) for i in np.unique(labels)])\n",
    "    predicted = centroids[labels]\n",
    "    r2 = r2_score(data_array, predicted)\n",
    "\n",
    "    return {\n",
    "        'silhouette_score': silhouette,\n",
    "        'calinski_harabasz_score': calinski,\n",
    "        'r2_score': r2\n",
    "    }\n",
    "\n",
    "def train_and_evaluate_som(data, params):\n",
    "    \"\"\"Train SOM with given parameters and evaluate performance.\"\"\"\n",
    "    # Convert data to numpy array\n",
    "    data_array = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "\n",
    "    # Initialize and train SOM\n",
    "    som = SOM(\n",
    "        x_dim=params['x_dim'],\n",
    "        y_dim=params['y_dim'],\n",
    "        input_dim=data_array.shape[1],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        sigma=params['sigma']\n",
    "    )\n",
    "\n",
    "    som.train(data_array, params['epochs'])\n",
    "    labels = som.get_cluster_labels(data_array)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = evaluate_som(data_array, labels)\n",
    "\n",
    "    # Create visualization\n",
    "    fig = som.visualize_clusters(data_array, labels)\n",
    "\n",
    "    return som, labels, metrics, fig\n",
    "\n",
    "# Best parameters from your previous run\n",
    "best_params = {\n",
    "    'y_dim': 2,\n",
    "    'x_dim': 2,\n",
    "    'sigma': 1.0,\n",
    "    'learning_rate': 0.46415888336127775,\n",
    "    'epochs': 200\n",
    "}\n",
    "\n",
    "# Convert val_clean to numpy array if it isn't already\n",
    "val_clean_array = np.array(val_clean) if not isinstance(val_clean, np.ndarray) else val_clean\n",
    "\n",
    "# Train and evaluate the SOM\n",
    "som, labels, metrics, fig = train_and_evaluate_som(val_clean_array, best_params)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Display the visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X98Fbo2b6pRd"
   },
   "outputs": [],
   "source": [
    "# class SOM:\n",
    "#     def __init__(self, x_dim, y_dim, input_dim, learning_rate=0.1, sigma=None):\n",
    "#         \"\"\"Initialize the SOM with given dimensions and parameters.\"\"\"\n",
    "#         self.x_dim = x_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         self.input_dim = input_dim\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.sigma = sigma if sigma is not None else max(x_dim, y_dim) / 2\n",
    "#         self.weights = np.random.randn(x_dim, y_dim, input_dim)\n",
    "#         self.locations = np.array([(i, j) for i in range(x_dim) for j in range(y_dim)])\n",
    "\n",
    "#     def find_bmu(self, x):\n",
    "#         \"\"\"Find the Best Matching Unit for input vector x.\"\"\"\n",
    "#         distances = cdist(x.reshape(1, -1), self.weights.reshape(-1, self.input_dim))\n",
    "#         return np.unravel_index(distances.argmin(), (self.x_dim, self.y_dim))\n",
    "\n",
    "#     def get_cluster_labels(self, data):\n",
    "#         \"\"\"Assign cluster labels to input data.\"\"\"\n",
    "#         data_array = self._ensure_numpy_array(data)\n",
    "#         labels = []\n",
    "#         for x in data_array:\n",
    "#             bmu = self.find_bmu(x)\n",
    "#             labels.append(bmu[0] * self.y_dim + bmu[1])\n",
    "#         return np.array(labels)\n",
    "\n",
    "#     def update_weights(self, x, bmu, iteration, max_iter):\n",
    "#         \"\"\"Update network weights based on input and BMU.\"\"\"\n",
    "#         lr = self.learning_rate * np.exp(-iteration / max_iter)\n",
    "#         sigma = self.sigma * np.exp(-iteration / max_iter)\n",
    "#         dist = cdist(self.locations, [bmu]).reshape(self.x_dim, self.y_dim)\n",
    "#         influence = np.exp(-dist ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "#         for i in range(self.x_dim):\n",
    "#             for j in range(self.y_dim):\n",
    "#                 self.weights[i, j] += lr * influence[i, j] * (x - self.weights[i, j])\n",
    "\n",
    "#     def _ensure_numpy_array(self, data):\n",
    "#         \"\"\"Convert input data to numpy array regardless of input type.\"\"\"\n",
    "#         if isinstance(data, pd.DataFrame):\n",
    "#             return data.values\n",
    "#         elif isinstance(data, np.ndarray):\n",
    "#             return data\n",
    "#         else:\n",
    "#             return np.array(data)\n",
    "\n",
    "#     def train(self, data, epochs):\n",
    "#         \"\"\"Train the SOM on input data.\"\"\"\n",
    "#         data_array = self._ensure_numpy_array(data)\n",
    "\n",
    "#         # Initialize weights to be in the same range as the input data\n",
    "#         data_min = np.min(data_array, axis=0)\n",
    "#         data_max = np.max(data_array, axis=0)\n",
    "#         self.weights = np.random.uniform(\n",
    "#             low=data_min,\n",
    "#             high=data_max,\n",
    "#             size=(self.x_dim, self.y_dim, self.input_dim)\n",
    "#         )\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             indices = np.arange(len(data_array))\n",
    "#             np.random.shuffle(indices)\n",
    "#             shuffled_data = data_array[indices]\n",
    "#             for i, x in enumerate(shuffled_data):\n",
    "#                 bmu = self.find_bmu(x)\n",
    "#                 self.update_weights(x, bmu, epoch * len(data_array) + i, epochs * len(data_array))\n",
    "\n",
    "#     def visualize_clusters(self, data, labels):\n",
    "#         \"\"\"Create visualizations for the SOM clusters.\"\"\"\n",
    "#         data_array = self._ensure_numpy_array(data)\n",
    "#         fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "#         # 1. U-Matrix (Weight distances)\n",
    "#         ax1 = fig.add_subplot(131)\n",
    "#         umatrix = np.zeros((self.x_dim, self.y_dim))\n",
    "#         for i in range(self.x_dim):\n",
    "#             for j in range(self.y_dim):\n",
    "#                 neighbors = []\n",
    "#                 if i > 0: neighbors.append(self.weights[i - 1, j])\n",
    "#                 if i < self.x_dim - 1: neighbors.append(self.weights[i + 1, j])\n",
    "#                 if j > 0: neighbors.append(self.weights[i, j - 1])\n",
    "#                 if j < self.y_dim - 1: neighbors.append(self.weights[i, j + 1])\n",
    "#                 umatrix[i, j] = np.mean([np.linalg.norm(self.weights[i, j] - neighbor) for neighbor in neighbors])\n",
    "#         sns.heatmap(umatrix, ax=ax1, cmap='viridis')\n",
    "#         ax1.set_title('U-Matrix\\n(Weight Distances)')\n",
    "\n",
    "#         # 2. Cluster assignments\n",
    "#         ax2 = fig.add_subplot(132)\n",
    "#         cluster_map = np.zeros((self.x_dim, self.y_dim))\n",
    "#         unique_labels = np.unique(labels)\n",
    "#         for label in unique_labels:\n",
    "#             mask = (labels == label)\n",
    "#             if np.any(mask):\n",
    "#                 points = data_array[mask]\n",
    "#                 for point in points:\n",
    "#                     bmu = self.find_bmu(point)\n",
    "#                     cluster_map[bmu] = label\n",
    "#         sns.heatmap(cluster_map, ax=ax2, cmap='Set3')\n",
    "#         ax2.set_title('Cluster Assignments')\n",
    "\n",
    "#         # 3. Hit map (sample density)\n",
    "#         ax3 = fig.add_subplot(133)\n",
    "#         hit_map = np.zeros((self.x_dim, self.y_dim))\n",
    "#         for x in data_array:\n",
    "#             bmu = self.find_bmu(x)\n",
    "#             hit_map[bmu] += 1\n",
    "#         sns.heatmap(hit_map, ax=ax3, cmap='YlOrRd')\n",
    "#         ax3.set_title('Hit Map\\n(Sample Density)')\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         return fig\n",
    "\n",
    "\n",
    "# def evaluate_som(data, labels):\n",
    "#     \"\"\"Calculate evaluation metrics for the SOM clustering.\"\"\"\n",
    "#     data_array = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "\n",
    "#     # Calculate silhouette and Calinski-Harabasz scores\n",
    "#     silhouette = silhouette_score(data_array, labels)\n",
    "#     calinski = calinski_harabasz_score(data_array, labels)\n",
    "\n",
    "#     # Calculate R using cluster centroids\n",
    "#     unique_labels = np.unique(labels)\n",
    "#     centroids = np.array([data_array[labels == i].mean(axis=0) for i in unique_labels])\n",
    "\n",
    "#     # Map labels to indices in the centroids array\n",
    "#     label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "#     predicted = centroids[np.array([label_to_index[label] for label in labels])]\n",
    "\n",
    "#     r2 = r2_score(data_array, predicted)\n",
    "\n",
    "#     return {\n",
    "#         'silhouette_score': silhouette,\n",
    "#         'calinski_harabasz_score': calinski,\n",
    "#         'r2_score': r2\n",
    "#     }\n",
    "\n",
    "\n",
    "# def train_and_evaluate_som(data, params):\n",
    "#     \"\"\"Train SOM with given parameters and evaluate performance.\"\"\"\n",
    "#     # Convert data to numpy array\n",
    "#     data_array = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "\n",
    "#     # Initialize and train SOM\n",
    "#     som = SOM(\n",
    "#         x_dim=params['x_dim'],\n",
    "#         y_dim=params['y_dim'],\n",
    "#         input_dim=data_array.shape[1],\n",
    "#         learning_rate=params['learning_rate'],\n",
    "#         sigma=params['sigma']\n",
    "#     )\n",
    "\n",
    "#     som.train(data_array, params['epochs'])\n",
    "#     labels = som.get_cluster_labels(data_array)\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     metrics = evaluate_som(data_array, labels)\n",
    "\n",
    "#     # Create visualization\n",
    "#     fig = som.visualize_clusters(data_array, labels)\n",
    "\n",
    "#     return som, labels, metrics, fig\n",
    "\n",
    "\n",
    "# # Ensure shop_clean is a NumPy array\n",
    "# shop_clean_array = np.array(shop_clean) if not isinstance(shop_clean, np.ndarray) else shop_clean\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     \"\"\"Objective function for Optuna to optimize.\"\"\"\n",
    "#     # Define the search space for hyperparameters\n",
    "#     params = {\n",
    "#         'x_dim': trial.suggest_int('x_dim', 2, 10),  # Grid search for x_dim\n",
    "#         'y_dim': trial.suggest_int('y_dim', 2, 10),  # Grid search for y_dim\n",
    "#         'sigma': trial.suggest_float('sigma', 0.1, 5.0),  # Grid search for sigma\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),  # Grid search for learning_rate\n",
    "#         'epochs': trial.suggest_int('epochs', 50, 500),  # Grid search for epochs\n",
    "#     }\n",
    "\n",
    "#     # Train and evaluate the SOM\n",
    "#     som, labels, metrics, _ = train_and_evaluate_som(shop_clean_array, params)\n",
    "\n",
    "#     # Use silhouette score as the objective to maximize\n",
    "#     return metrics['silhouette_score']\n",
    "\n",
    "\n",
    "# # Run the Optuna study\n",
    "# study = optuna.create_study(direction='maximize')  # Maximize silhouette score\n",
    "# study.optimize(objective, n_trials=25)  # Run 25 trials\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(\"\\nBest Parameters:\")\n",
    "# print(study.best_params)\n",
    "# print(\"\\nBest Silhouette Score:\")\n",
    "# print(study.best_value)\n",
    "\n",
    "# # Train the SOM with the best parameters\n",
    "# best_params = study.best_params\n",
    "# som, labels, metrics, fig = train_and_evaluate_som(shop_clean_array, best_params)\n",
    "\n",
    "# # Print the evaluation metrics for the best model\n",
    "# print(\"\\nEvaluation Metrics for Best Model:\")\n",
    "# for metric, value in metrics.items():\n",
    "#     print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# # Display the visualization for the best model\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7BpWYPfuci9"
   },
   "source": [
    "Trial 22 finished with value: 0.519218365366583 and parameters: {'x_dim': 9, 'y_dim': 8, 'sigma': 1.0923600860428389, 'learning_rate': 0.1661223667416274, 'epochs': 105}. Best is trial 22 with value: 0.519218365366583."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xx1hpbgG4J6d"
   },
   "outputs": [],
   "source": [
    "class SOM:\n",
    "    def __init__(self, x_dim, y_dim, input_dim, learning_rate=0.1, sigma=None):\n",
    "        \"\"\"Initialize the SOM with given dimensions and parameters.\"\"\"\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigma = sigma if sigma is not None else max(x_dim, y_dim) / 2\n",
    "        self.weights = np.random.randn(x_dim, y_dim, input_dim)\n",
    "        self.locations = np.array([(i, j) for i in range(x_dim) for j in range(y_dim)])\n",
    "\n",
    "    def find_bmu(self, x):\n",
    "        \"\"\"Find the Best Matching Unit for input vector x.\"\"\"\n",
    "        distances = cdist(x.reshape(1, -1), self.weights.reshape(-1, self.input_dim))\n",
    "        return np.unravel_index(distances.argmin(), (self.x_dim, self.y_dim))\n",
    "\n",
    "    def get_cluster_labels(self, data):\n",
    "        \"\"\"Assign cluster labels to input data.\"\"\"\n",
    "        data_array = self._ensure_numpy_array(data)\n",
    "        labels = []\n",
    "        unique_bmus = {}  # Dictionary to map BMUs to sequential labels\n",
    "        label_counter = 0\n",
    "\n",
    "        for x in data_array:\n",
    "            bmu = self.find_bmu(x)\n",
    "            bmu_key = (bmu[0], bmu[1])  # Use BMU coordinates as a key\n",
    "\n",
    "            if bmu_key not in unique_bmus:\n",
    "                unique_bmus[bmu_key] = label_counter\n",
    "                label_counter += 1\n",
    "\n",
    "            labels.append(unique_bmus[bmu_key])\n",
    "\n",
    "        return np.array(labels)\n",
    "\n",
    "    def update_weights(self, x, bmu, iteration, max_iter):\n",
    "        \"\"\"Update network weights based on input and BMU.\"\"\"\n",
    "        lr = self.learning_rate * np.exp(-iteration / max_iter)\n",
    "        sigma = self.sigma * np.exp(-iteration / max_iter)\n",
    "        dist = cdist(self.locations, [bmu]).reshape(self.x_dim, self.y_dim)\n",
    "        influence = np.exp(-dist ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "        for i in range(self.x_dim):\n",
    "            for j in range(self.y_dim):\n",
    "                self.weights[i, j] += lr * influence[i, j] * (x - self.weights[i, j])\n",
    "\n",
    "    def _ensure_numpy_array(self, data):\n",
    "        \"\"\"Convert input data to numpy array regardless of input type.\"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            return data.values\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            return data\n",
    "        else:\n",
    "            return np.array(data)\n",
    "\n",
    "    def train(self, data, epochs):\n",
    "        \"\"\"Train the SOM on input data.\"\"\"\n",
    "        data_array = self._ensure_numpy_array(data)\n",
    "\n",
    "        # Initialize weights to be in the same range as the input data\n",
    "        data_min = np.min(data_array, axis=0)\n",
    "        data_max = np.max(data_array, axis=0)\n",
    "        self.weights = np.random.uniform(\n",
    "            low=data_min,\n",
    "            high=data_max,\n",
    "            size=(self.x_dim, self.y_dim, self.input_dim)\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(len(data_array))\n",
    "            np.random.shuffle(indices)\n",
    "            shuffled_data = data_array[indices]\n",
    "            for i, x in enumerate(shuffled_data):\n",
    "                bmu = self.find_bmu(x)\n",
    "                self.update_weights(x, bmu, epoch * len(data_array) + i, epochs * len(data_array))\n",
    "\n",
    "    def visualize_clusters(self, data, labels):\n",
    "        \"\"\"Create visualizations for the SOM clusters.\"\"\"\n",
    "        data_array = self._ensure_numpy_array(data)\n",
    "        fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # 1. U-Matrix (Weight distances)\n",
    "        ax1 = fig.add_subplot(131)\n",
    "        umatrix = np.zeros((self.x_dim, self.y_dim))\n",
    "        for i in range(self.x_dim):\n",
    "            for j in range(self.y_dim):\n",
    "                neighbors = []\n",
    "                if i > 0: neighbors.append(self.weights[i-1, j])\n",
    "                if i < self.x_dim-1: neighbors.append(self.weights[i+1, j])\n",
    "                if j > 0: neighbors.append(self.weights[i, j-1])\n",
    "                if j < self.y_dim-1: neighbors.append(self.weights[i, j+1])\n",
    "                umatrix[i, j] = np.mean([np.linalg.norm(self.weights[i, j] - neighbor) for neighbor in neighbors])\n",
    "        sns.heatmap(umatrix, ax=ax1, cmap='viridis')\n",
    "        ax1.set_title('U-Matrix\\n(Weight Distances)')\n",
    "\n",
    "        # 2. Cluster assignments\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        cluster_map = np.zeros((self.x_dim, self.y_dim))\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            mask = (labels == label)\n",
    "            if np.any(mask):\n",
    "                points = data_array[mask]\n",
    "                for point in points:\n",
    "                    bmu = self.find_bmu(point)\n",
    "                    cluster_map[bmu] = label\n",
    "        sns.heatmap(cluster_map, ax=ax2, cmap='Set3')\n",
    "        ax2.set_title('Cluster Assignments')\n",
    "\n",
    "        # 3. Hit map (sample density)\n",
    "        ax3 = fig.add_subplot(133)\n",
    "        hit_map = np.zeros((self.x_dim, self.y_dim))\n",
    "        for x in data_array:\n",
    "            bmu = self.find_bmu(x)\n",
    "            hit_map[bmu] += 1\n",
    "        sns.heatmap(hit_map, ax=ax3, cmap='YlOrRd')\n",
    "        ax3.set_title('Hit Map\\n(Sample Density)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "\n",
    "def evaluate_som(data, labels):\n",
    "    \"\"\"Calculate evaluation metrics for the SOM clustering.\"\"\"\n",
    "    data_array = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "\n",
    "    # Ensure labels are valid\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) == 1:\n",
    "        raise ValueError(\"Only one cluster found. Evaluation metrics require at least two clusters.\")\n",
    "\n",
    "    # Calculate Silhouette Score and Calinski-Harabasz Score\n",
    "    silhouette = silhouette_score(data_array, labels)\n",
    "    calinski = calinski_harabasz_score(data_array, labels)\n",
    "\n",
    "    # Calculate R using cluster centroids\n",
    "    centroids = np.array([data_array[labels == i].mean(axis=0) for i in unique_labels])\n",
    "    predicted = centroids[labels]\n",
    "    r2 = r2_score(data_array, predicted)\n",
    "\n",
    "    return {\n",
    "        'silhouette_score': silhouette,\n",
    "        'calinski_harabasz_score': calinski,\n",
    "        'r2_score': r2\n",
    "    }\n",
    "\n",
    "\n",
    "def train_and_evaluate_som(data, params):\n",
    "    \"\"\"Train SOM with given parameters and evaluate performance.\"\"\"\n",
    "    # Convert data to numpy array\n",
    "    data_array = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "\n",
    "    # Initialize and train SOM\n",
    "    som = SOM(\n",
    "        x_dim=params['x_dim'],\n",
    "        y_dim=params['y_dim'],\n",
    "        input_dim=data_array.shape[1],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        sigma=params['sigma']\n",
    "    )\n",
    "\n",
    "    som.train(data_array, params['epochs'])\n",
    "    labels = som.get_cluster_labels(data_array)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = evaluate_som(data_array, labels)\n",
    "\n",
    "    # Create visualization\n",
    "    fig = som.visualize_clusters(data_array, labels)\n",
    "\n",
    "    return som, labels, metrics, fig\n",
    "\n",
    "\n",
    "# Best parameters from your previous run\n",
    "best_params = {\n",
    "    'y_dim': 9,\n",
    "    'x_dim': 7,\n",
    "    'sigma': 2.344055736106201,\n",
    "    'learning_rate': 0.3804926815785139,\n",
    "    'epochs': 216\n",
    "}\n",
    "\n",
    "# Convert val_clean to numpy array if it isn't already\n",
    "shop_clean_array = np.array(shop_clean) if not isinstance(shop_clean, np.ndarray) else shop_clean\n",
    "\n",
    "# Train and evaluate the SOM\n",
    "som, labels, metrics, fig = train_and_evaluate_som(shop_clean_array, best_params)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Display the visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4VRkjIMGayY"
   },
   "outputs": [],
   "source": [
    "# class SOM:\n",
    "#     def __init__(self, x_dim, y_dim, input_dim, learning_rate=0.1, sigma=None):\n",
    "#         \"\"\"Initialize the SOM with given dimensions and parameters.\"\"\"\n",
    "#         self.x_dim = x_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         self.input_dim = input_dim\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.sigma = sigma if sigma is not None else max(x_dim, y_dim) / 2\n",
    "#         self.weights = np.random.randn(x_dim, y_dim, input_dim)\n",
    "#         self.locations = np.array([(i, j) for i in range(x_dim) for j in range(y_dim)])\n",
    "\n",
    "#     def find_bmu(self, x):\n",
    "#         \"\"\"Find the Best Matching Unit for input vector x.\"\"\"\n",
    "#         distances = cdist(x.reshape(1, -1), self.weights.reshape(-1, self.input_dim))\n",
    "#         return np.unravel_index(distances.argmin(), (self.x_dim, self.y_dim))\n",
    "\n",
    "#     def get_cluster_labels(self, data):\n",
    "#         \"\"\"Assign cluster labels to input data.\"\"\"\n",
    "#         data_array = self._ensure_numpy_array(data)\n",
    "#         labels = []\n",
    "#         for x in data_array:\n",
    "#             bmu = self.find_bmu(x)\n",
    "#             labels.append(bmu[0] * self.y_dim + bmu[1])\n",
    "#         return np.array(labels)\n",
    "\n",
    "#     def update_weights(self, x, bmu, iteration, max_iter):\n",
    "#         \"\"\"Update network weights based on input and BMU.\"\"\"\n",
    "#         lr = self.learning_rate * np.exp(-iteration / max_iter)\n",
    "#         sigma = self.sigma * np.exp(-iteration / max_iter)\n",
    "#         dist = cdist(self.locations, [bmu]).reshape(self.x_dim, self.y_dim)\n",
    "#         influence = np.exp(-dist ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "#         for i in range(self.x_dim):\n",
    "#             for j in range(self.y_dim):\n",
    "#                 self.weights[i, j] += lr * influence[i, j] * (x - self.weights[i, j])\n",
    "\n",
    "#     def _ensure_numpy_array(self, data):\n",
    "#         \"\"\"Convert input data to numpy array regardless of input type.\"\"\"\n",
    "#         if isinstance(data, pd.DataFrame):\n",
    "#             return data.values\n",
    "#         elif isinstance(data, np.ndarray):\n",
    "#             return data\n",
    "#         else:\n",
    "#             return np.array(data)\n",
    "\n",
    "#     def train(self, data, epochs):\n",
    "#         \"\"\"Train the SOM on input data.\"\"\"\n",
    "#         data_array = self._ensure_numpy_array(data)\n",
    "\n",
    "#         # Initialize weights to be in the same range as the input data\n",
    "#         data_min = np.min(data_array, axis=0)\n",
    "#         data_max = np.max(data_array, axis=0)\n",
    "#         self.weights = np.random.uniform(\n",
    "#             low=data_min,\n",
    "#             high=data_max,\n",
    "#             size=(self.x_dim, self.y_dim, self.input_dim)\n",
    "#         )\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             indices = np.arange(len(data_array))\n",
    "#             np.random.shuffle(indices)\n",
    "#             shuffled_data = data_array[indices]\n",
    "#             for i, x in enumerate(shuffled_data):\n",
    "#                 bmu = self.find_bmu(x)\n",
    "#                 self.update_weights(x, bmu, epoch * len(data_array) + i, epochs * len(data_array))\n",
    "\n",
    "#     def visualize_clusters(self, data, labels):\n",
    "#         \"\"\"Create visualizations for the SOM clusters.\"\"\"\n",
    "#         data_array = self._ensure_numpy_array(data)\n",
    "#         fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "#         # 1. U-Matrix (Weight distances)\n",
    "#         ax1 = fig.add_subplot(131)\n",
    "#         umatrix = np.zeros((self.x_dim, self.y_dim))\n",
    "#         for i in range(self.x_dim):\n",
    "#             for j in range(self.y_dim):\n",
    "#                 neighbors = []\n",
    "#                 if i > 0: neighbors.append(self.weights[i - 1, j])\n",
    "#                 if i < self.x_dim - 1: neighbors.append(self.weights[i + 1, j])\n",
    "#                 if j > 0: neighbors.append(self.weights[i, j - 1])\n",
    "#                 if j < self.y_dim - 1: neighbors.append(self.weights[i, j + 1])\n",
    "#                 umatrix[i, j] = np.mean([np.linalg.norm(self.weights[i, j] - neighbor) for neighbor in neighbors])\n",
    "#         sns.heatmap(umatrix, ax=ax1, cmap='viridis')\n",
    "#         ax1.set_title('U-Matrix\\n(Weight Distances)')\n",
    "\n",
    "#         # 2. Cluster assignments\n",
    "#         ax2 = fig.add_subplot(132)\n",
    "#         cluster_map = np.zeros((self.x_dim, self.y_dim))\n",
    "#         unique_labels = np.unique(labels)\n",
    "#         for label in unique_labels:\n",
    "#             mask = (labels == label)\n",
    "#             if np.any(mask):\n",
    "#                 points = data_array[mask]\n",
    "#                 for point in points:\n",
    "#                     bmu = self.find_bmu(point)\n",
    "#                     cluster_map[bmu] = label\n",
    "#         sns.heatmap(cluster_map, ax=ax2, cmap='Set3')\n",
    "#         ax2.set_title('Cluster Assignments')\n",
    "\n",
    "#         # 3. Hit map (sample density)\n",
    "#         ax3 = fig.add_subplot(133)\n",
    "#         hit_map = np.zeros((self.x_dim, self.y_dim))\n",
    "#         for x in data_array:\n",
    "#             bmu = self.find_bmu(x)\n",
    "#             hit_map[bmu] += 1\n",
    "#         sns.heatmap(hit_map, ax=ax3, cmap='YlOrRd')\n",
    "#         ax3.set_title('Hit Map\\n(Sample Density)')\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         return fig\n",
    "\n",
    "\n",
    "# def evaluate_som(data, labels):\n",
    "#     \"\"\"Calculate evaluation metrics for the SOM clustering.\"\"\"\n",
    "#     data_array = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "\n",
    "#     # Calculate silhouette and Calinski-Harabasz scores\n",
    "#     silhouette = silhouette_score(data_array, labels)\n",
    "#     calinski = calinski_harabasz_score(data_array, labels)\n",
    "\n",
    "#     # Calculate R using cluster centroids\n",
    "#     unique_labels = np.unique(labels)\n",
    "#     centroids = np.array([data_array[labels == i].mean(axis=0) for i in unique_labels])\n",
    "\n",
    "#     # Map labels to indices in the centroids array\n",
    "#     label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "#     predicted = centroids[np.array([label_to_index[label] for label in labels])]\n",
    "\n",
    "#     r2 = r2_score(data_array, predicted)\n",
    "\n",
    "#     return {\n",
    "#         'silhouette_score': silhouette,\n",
    "#         'calinski_harabasz_score': calinski,\n",
    "#         'r2_score': r2\n",
    "#     }\n",
    "\n",
    "\n",
    "# def train_and_evaluate_som(data, params):\n",
    "#     \"\"\"Train SOM with given parameters and evaluate performance.\"\"\"\n",
    "#     # Convert data to numpy array\n",
    "#     data_array = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "\n",
    "#     # Initialize and train SOM\n",
    "#     som = SOM(\n",
    "#         x_dim=params['x_dim'],\n",
    "#         y_dim=params['y_dim'],\n",
    "#         input_dim=data_array.shape[1],\n",
    "#         learning_rate=params['learning_rate'],\n",
    "#         sigma=params['sigma']\n",
    "#     )\n",
    "\n",
    "#     som.train(data_array, params['epochs'])\n",
    "#     labels = som.get_cluster_labels(data_array)\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     metrics = evaluate_som(data_array, labels)\n",
    "\n",
    "#     # Create visualization\n",
    "#     fig = som.visualize_clusters(data_array, labels)\n",
    "\n",
    "#     return som, labels, metrics, fig\n",
    "\n",
    "\n",
    "# # Ensure shop_clean is a NumPy array\n",
    "# pref_clean_array = np.array(pref_clean) if not isinstance(pref_clean, np.ndarray) else pref_clean\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     \"\"\"Objective function for Optuna to optimize.\"\"\"\n",
    "#     # Define the search space for hyperparameters\n",
    "#     params = {\n",
    "#         'x_dim': trial.suggest_int('x_dim', 5, 11),  # Grid search for x_dim\n",
    "#         'y_dim': trial.suggest_int('y_dim', 2, 10),  # Grid search for y_dim\n",
    "#         'sigma': trial.suggest_float('sigma', 0.2, 4.2),  # Grid search for sigma\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.04, 1.0),  # Grid search for learning_rate\n",
    "#         'epochs': trial.suggest_int('epochs', 400, 600),  # Grid search for epochs\n",
    "#     }\n",
    "\n",
    "#     # Train and evaluate the SOM\n",
    "#     som, labels, metrics, _ = train_and_evaluate_som(pref_clean_array, params)\n",
    "\n",
    "#     # Use silhouette score as the objective to maximize\n",
    "#     return metrics['silhouette_score']\n",
    "\n",
    "\n",
    "# # Run the Optuna study\n",
    "# study = optuna.create_study(direction='maximize')  # Maximize silhouette score\n",
    "# study.optimize(objective, n_trials=25)  # Run 25 trials\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(\"\\nBest Parameters:\")\n",
    "# print(study.best_params)\n",
    "# print(\"\\nBest Silhouette Score:\")\n",
    "# print(study.best_value)\n",
    "\n",
    "# # Train the SOM with the best parameters\n",
    "# best_params = study.best_params\n",
    "# som, labels, metrics, fig = train_and_evaluate_som(pref_clean_array, best_params)\n",
    "\n",
    "# # Print the evaluation metrics for the best model\n",
    "# print(\"\\nEvaluation Metrics for Best Model:\")\n",
    "# for metric, value in metrics.items():\n",
    "#     print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# # Display the visualization for the best model\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzeSQFkauGuU"
   },
   "source": [
    "[I 2024-12-24 21:56:22,424] A new study created in memory with name: no-name-78a8a608-0a6a-4ab2-839d-1b2fa5711556\n",
    "\n",
    "Best Parameters:\n",
    "{'x_dim': 3, 'y_dim': 8, 'sigma': 0.17275998212809796, 'learning_rate': 0.6409566898354768, 'epochs': 445}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHRzUiXz4ZhJ"
   },
   "outputs": [],
   "source": [
    "class SOM:\n",
    "    def __init__(self, x_dim, y_dim, input_dim, learning_rate=0.1, sigma=None):\n",
    "        \"\"\"Initialize the SOM with given dimensions and parameters.\"\"\"\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigma = sigma if sigma is not None else max(x_dim, y_dim) / 2\n",
    "        self.weights = np.random.randn(x_dim, y_dim, input_dim)\n",
    "        self.locations = np.array([(i, j) for i in range(x_dim) for j in range(y_dim)])\n",
    "\n",
    "    def find_bmu(self, x):\n",
    "        \"\"\"Find the Best Matching Unit for input vector x.\"\"\"\n",
    "        distances = cdist(x.reshape(1, -1), self.weights.reshape(-1, self.input_dim))\n",
    "        return np.unravel_index(distances.argmin(), (self.x_dim, self.y_dim))\n",
    "\n",
    "    def get_cluster_labels(self, data):\n",
    "        \"\"\"Assign cluster labels to input data.\"\"\"\n",
    "        data_array = self._ensure_numpy_array(data)\n",
    "        labels = []\n",
    "        for x in data_array:\n",
    "            bmu = self.find_bmu(x)\n",
    "            labels.append(bmu[0] * self.y_dim + bmu[1])\n",
    "        return np.array(labels)\n",
    "\n",
    "    def update_weights(self, x, bmu, iteration, max_iter):\n",
    "        \"\"\"Update network weights based on input and BMU.\"\"\"\n",
    "        lr = self.learning_rate * np.exp(-iteration / max_iter)\n",
    "        sigma = self.sigma * np.exp(-iteration / max_iter)\n",
    "        dist = cdist(self.locations, [bmu]).reshape(self.x_dim, self.y_dim)\n",
    "        influence = np.exp(-dist ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "        for i in range(self.x_dim):\n",
    "            for j in range(self.y_dim):\n",
    "                self.weights[i, j] += lr * influence[i, j] * (x - self.weights[i, j])\n",
    "\n",
    "    def _ensure_numpy_array(self, data):\n",
    "        \"\"\"Convert input data to numpy array regardless of input type.\"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            return data.values\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            return data\n",
    "        else:\n",
    "            return np.array(data)\n",
    "\n",
    "    def train(self, data, epochs):\n",
    "        \"\"\"Train the SOM on input data.\"\"\"\n",
    "        data_array = self._ensure_numpy_array(data)\n",
    "\n",
    "        # Initialize weights to be in the same range as the input data\n",
    "        data_min = np.min(data_array, axis=0)\n",
    "        data_max = np.max(data_array, axis=0)\n",
    "        self.weights = np.random.uniform(\n",
    "            low=data_min,\n",
    "            high=data_max,\n",
    "            size=(self.x_dim, self.y_dim, self.input_dim)\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(len(data_array))\n",
    "            np.random.shuffle(indices)\n",
    "            shuffled_data = data_array[indices]\n",
    "            for i, x in enumerate(shuffled_data):\n",
    "                bmu = self.find_bmu(x)\n",
    "                self.update_weights(x, bmu, epoch * len(data_array) + i, epochs * len(data_array))\n",
    "\n",
    "    def visualize_clusters(self, data, labels):\n",
    "        \"\"\"Create visualizations for the SOM clusters.\"\"\"\n",
    "        data_array = self._ensure_numpy_array(data)\n",
    "        fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # 1. U-Matrix (Weight distances)\n",
    "        ax1 = fig.add_subplot(131)\n",
    "        umatrix = np.zeros((self.x_dim, self.y_dim))\n",
    "        for i in range(self.x_dim):\n",
    "            for j in range(self.y_dim):\n",
    "                neighbors = []\n",
    "                if i > 0: neighbors.append(self.weights[i-1, j])\n",
    "                if i < self.x_dim-1: neighbors.append(self.weights[i+1, j])\n",
    "                if j > 0: neighbors.append(self.weights[i, j-1])\n",
    "                if j < self.y_dim-1: neighbors.append(self.weights[i, j+1])\n",
    "                umatrix[i, j] = np.mean([np.linalg.norm(self.weights[i, j] - neighbor) for neighbor in neighbors])\n",
    "        sns.heatmap(umatrix, ax=ax1, cmap='viridis')\n",
    "        ax1.set_title('U-Matrix\\n(Weight Distances)')\n",
    "\n",
    "        # 2. Cluster assignments\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        cluster_map = np.zeros((self.x_dim, self.y_dim))\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            mask = (labels == label)\n",
    "            if np.any(mask):\n",
    "                points = data_array[mask]\n",
    "                for point in points:\n",
    "                    bmu = self.find_bmu(point)\n",
    "                    cluster_map[bmu] = label\n",
    "        sns.heatmap(cluster_map, ax=ax2, cmap='Set3')\n",
    "        ax2.set_title('Cluster Assignments')\n",
    "\n",
    "        # 3. Hit map (sample density)\n",
    "        ax3 = fig.add_subplot(133)\n",
    "        hit_map = np.zeros((self.x_dim, self.y_dim))\n",
    "        for x in data_array:\n",
    "            bmu = self.find_bmu(x)\n",
    "            hit_map[bmu] += 1\n",
    "        sns.heatmap(hit_map, ax=ax3, cmap='YlOrRd')\n",
    "        ax3.set_title('Hit Map\\n(Sample Density)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "\n",
    "def evaluate_som(data, labels):\n",
    "    \"\"\"Calculate evaluation metrics for the SOM clustering.\"\"\"\n",
    "    data_array = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "\n",
    "    # Remap labels to a continuous range starting from 0\n",
    "    unique_labels = np.unique(labels)\n",
    "    label_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
    "    remapped_labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "    # Ensure there are at least two clusters for evaluation\n",
    "    if len(unique_labels) == 1:\n",
    "        raise ValueError(\"Only one cluster found. Evaluation metrics require at least two clusters.\")\n",
    "\n",
    "    # Calculate Silhouette Score and Calinski-Harabasz Score\n",
    "    silhouette = silhouette_score(data_array, remapped_labels)\n",
    "    calinski = calinski_harabasz_score(data_array, remapped_labels)\n",
    "\n",
    "    # Calculate R using cluster centroids\n",
    "    centroids = np.array([data_array[remapped_labels == i].mean(axis=0) for i in np.unique(remapped_labels)])\n",
    "    predicted = centroids[remapped_labels]\n",
    "    r2 = r2_score(data_array, predicted)\n",
    "\n",
    "    return {\n",
    "        'silhouette_score': silhouette,\n",
    "        'calinski_harabasz_score': calinski,\n",
    "        'r2_score': r2\n",
    "    }\n",
    "\n",
    "\n",
    "def train_and_evaluate_som(data, params):\n",
    "    \"\"\"Train SOM with given parameters and evaluate performance.\"\"\"\n",
    "    # Convert data to numpy array\n",
    "    data_array = np.array(data) if not isinstance(data, np.ndarray) else data\n",
    "\n",
    "    # Initialize and train SOM\n",
    "    som = SOM(\n",
    "        x_dim=params['x_dim'],\n",
    "        y_dim=params['y_dim'],\n",
    "        input_dim=data_array.shape[1],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        sigma=params['sigma']\n",
    "    )\n",
    "\n",
    "    som.train(data_array, params['epochs'])\n",
    "    labels = som.get_cluster_labels(data_array)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = evaluate_som(data_array, labels)\n",
    "\n",
    "    # Create visualization\n",
    "    fig = som.visualize_clusters(data_array, labels)\n",
    "\n",
    "    return som, labels, metrics, fig\n",
    "\n",
    "\n",
    "# Best parameters from your previous run\n",
    "best_params = {\n",
    "    'y_dim': 3,\n",
    "    'x_dim': 8,\n",
    "    'sigma': 0.1727,\n",
    "    'learning_rate': 0.6409566898354768,\n",
    "    'epochs': 445\n",
    "}\n",
    "\n",
    "# Convert val_clean to numpy array if it isn't already\n",
    "pref_clean_array = np.array(pref_clean) if not isinstance(pref_clean, np.ndarray) else pref_clean\n",
    "\n",
    "# Train and evaluate the SOM\n",
    "som, labels, metrics, fig = train_and_evaluate_som(pref_clean_array, best_params)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Display the visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3t7MDgjO6VXq"
   },
   "source": [
    "So, because of the lesser optimized nature of my SOMs, and due to time constraints and computational limitations of our machinery hardware, and the simplicity and effectiveness of the k-means++, and being lucky to have three of both, the k-means++ will serve as the main clustering and analysis and segmentation tool for further analysis, and the SOMs will be the comparison and the quote-unquote peer-review tool, and potentially an actual visualization tool, to confirm that the silhouettes and other scores such as the Davinsky, Kalinsky, and R2 are a sufficient explanation of our clustering made, and can potentially aid in this too. The unoptimized nature of this is acceptable because the primary clustering has already been verified through the scoring of silhouette and explained variance, it's just that the justification of this on the report will have to frame this correctly, and acknowledge the limitations of grid search and k-means, specifically its simplicity versus a grid search and a SOM fairly, and to talk about this and find supporting evidence of this thesis appropriately when addressing this in the reportation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-N5Acmo3OLZ"
   },
   "source": [
    "#The next step is to discuss the best clustering model, clustering parameters (numbers of clusters etc) and then continue with the merge based on centroids and the other statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFTUU5hyAb2v"
   },
   "source": [
    "Analyze and decide best cluster amount and cluster model, add labels, merge.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "-vc8oMXkD8QB",
    "outputId": "cfd17829-0843-4fba-fdbd-c842550761f6"
   },
   "outputs": [],
   "source": [
    "# For the first segment: (preferences)\n",
    "\n",
    "\"\"\"k   Silhouette(S)    R      |S-0.5|    |R-0.5|    Combined Distance\n",
    "    3   0.549           0.211   0.049     0.289      0.293\n",
    "    4   0.479           0.277   0.021     0.223      0.224\n",
    "    5   0.426           0.325   0.074     0.175      0.190\n",
    "    6   0.467           0.396   0.033     0.104      0.109\n",
    "    7   0.472           0.460   0.028     0.040      0.049 *Optimal balance between R2 and Silhouette*\n",
    "    8   0.336           0.448   0.164     0.052      0.172\n",
    "    9   0.357           0.503   0.143     0.003      0.143\n",
    "    10  0.345           0.534   0.155     0.034      0.159\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "wrLZB7_6RTMv",
    "outputId": "f5e75a0c-1bae-4215-a283-92f0a362abdf"
   },
   "outputs": [],
   "source": [
    "pref_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6ZOb3NRX3VF"
   },
   "outputs": [],
   "source": [
    "# QUANTUM VALIDATION OF DataFrame EXISTENCE\n",
    "assert isinstance(pref_clean, pd.DataFrame), \"CRITICAL ERROR: Input must maintain DataFrame quantum state\"\n",
    "\n",
    "# VALIDATE CLUSTER COLUMN PRESENCE WITH SCHRDINGER-LEVEL PRECISION\n",
    "assert 'cluster' in pref_clean.columns, \"CRITICAL ERROR: Cluster column not detected in quantum space\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "MdE6CTyQZUv4",
    "outputId": "45903824-8290-49fc-aac1-08538612e3de"
   },
   "outputs": [],
   "source": [
    "pref_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chTUixiQX4G5",
    "outputId": "9748004f-5080-4b01-e4c7-b2fa8c7f8f41"
   },
   "outputs": [],
   "source": [
    "# PHASE 1: SUPERMASSIVE VALIDATION OF INPUT DATA\n",
    "assert isinstance(pref_clean, pd.DataFrame), \"EINSTEIN-GRADE ERROR: DataFrame status compromised!\"\n",
    "\n",
    "# PHASE 2: QUANTUM CENTROID CALCULATION FROM EXISTING CLUSTER COLUMN\n",
    "pref_centroids = pref_clean.groupby('cluster')[['Other_Asian', 'General_Asian', 'Western',\n",
    "                                         'Beverages_Cafe', 'Desserts_Snacks', 'Main_Dishes']].mean()\n",
    "\n",
    "# PHASE 3: ADD CLUSTER IDENTIFICATION FOR MERGER WITH RELATIVISTIC PRECISION\n",
    "pref_centroids['cluster_number'] = pref_centroids.index\n",
    "\n",
    "print(\" HAWKING-GRADE CENTROID MATRIX:\")\n",
    "print(pref_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "2xYn_aEtaNGL",
    "outputId": "2efd5cd7-f174-457b-afb2-0679c25bc614"
   },
   "outputs": [],
   "source": [
    "pref_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "8aL2XetOD8BM",
    "outputId": "16618be5-4ce5-49f1-d80a-59a7bb51c4be"
   },
   "outputs": [],
   "source": [
    "# For the second segment: (value)\n",
    "'''\n",
    "k    Silhouette(S)   R      |S-0.5|    |R-0.5|    Combined_Score\n",
    "3    0.416          0.569    0.084     0.069      0.109\n",
    "4    0.460*         0.686*   0.040     0.186      0.190 * Highest silhouette and high R2\n",
    "5    0.451          0.739    0.049     0.239      0.244\n",
    "6    0.451          0.782    0.049     0.282      0.286\n",
    "7    0.437          0.807    0.063     0.307      0.313\n",
    "8    0.438          0.822    0.062     0.322      0.328\n",
    "9    0.452          0.834    0.048     0.334      0.337\n",
    "10   0.458          0.855    0.042     0.355      0.357'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "re3eCZX7RkEL",
    "outputId": "e9cb572a-6b76-41c2-a0ff-9644e78a870f"
   },
   "outputs": [],
   "source": [
    "shop_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExKJHL9TalZc",
    "outputId": "7772ca0d-4573-4cbd-8dc1-9295d14bd38d"
   },
   "outputs": [],
   "source": [
    "# PHASE 1: SUPERMASSIVE VALIDATION OF INPUT DATA\n",
    "assert isinstance(shop_clean, pd.DataFrame), \"EINSTEIN-GRADE ERROR: DataFrame status compromised!\"\n",
    "\n",
    "# PHASE 2: QUANTUM CENTROID CALCULATION FROM EXISTING CLUSTER COLUMN\n",
    "shop_centroids = shop_clean.groupby('cluster')[['is_chain', 'product_score', 'vendor_score', 'daytime_orders',\n",
    "       'evening_orders', 'chain_vendor_interaction']].mean()\n",
    "\n",
    "# PHASE 3: ADD CLUSTER IDENTIFICATION FOR MERGER WITH RELATIVISTIC PRECISION\n",
    "shop_centroids['cluster_number'] = shop_centroids.index\n",
    "\n",
    "print(\" HAWKING-GRADE CENTROID MATRIX:\")\n",
    "print(shop_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "6CL6OYulbDo2",
    "outputId": "7cdb8ece-2da8-495f-e27d-73ca68e895d1"
   },
   "outputs": [],
   "source": [
    "shop_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "ugGFdgxHD772",
    "outputId": "24073166-ce2b-4bec-bf35-af088199605f"
   },
   "outputs": [],
   "source": [
    "# For the third segment: (value)\n",
    "'''\n",
    "k    Silhouette(S)   R      |S-0.5|    |R-0.5|    Combined_Score\n",
    "3    0.531*         0.589*   0.031     0.089      0.094  *OPTIMAL*\n",
    "4    0.519          0.621    0.019     0.121      0.122\n",
    "5    0.398          0.688    0.102     0.188      0.214\n",
    "6    0.351          0.703    0.149     0.203      0.252\n",
    "7    0.360          0.729    0.140     0.229      0.268\n",
    "8    0.350          0.741    0.150     0.241      0.284\n",
    "9    0.321          0.756    0.179     0.256      0.312\n",
    "10   0.317          0.767    0.183     0.267      0.323'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVQPmO--RVy0",
    "outputId": "c81ac20a-2370-4463-eda5-bfa5203568c2"
   },
   "outputs": [],
   "source": [
    "val_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqm_YxKVbJiK",
    "outputId": "cf2dda47-7c22-4196-852f-bdbe58aa30f1"
   },
   "outputs": [],
   "source": [
    "# PHASE 1: SUPERMASSIVE VALIDATION OF INPUT DATA\n",
    "assert isinstance(val_clean, pd.DataFrame), \"EINSTEIN-GRADE ERROR: DataFrame status compromised!\"\n",
    "\n",
    "# PHASE 2: QUANTUM CENTROID CALCULATION FROM EXISTING CLUSTER COLUMN\n",
    "val_centroids = val_clean.groupby('cluster')[['CLV_Score', 'value_retention', 'value_growth', 'value_growth_rate',\n",
    "       'growth_momentum', 'spend_consistency', 'avg_order_value',\n",
    "       'active_spend_rate']].mean()\n",
    "\n",
    "# PHASE 3: ADD CLUSTER IDENTIFICATION FOR MERGER WITH RELATIVISTIC PRECISION\n",
    "val_centroids['cluster_number'] = val_centroids.index\n",
    "\n",
    "print(\" HAWKING-GRADE CENTROID MATRIX:\")\n",
    "print(val_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "lleX-kMDe5ti",
    "outputId": "4ed02232-8410-47af-ba3d-bc9b87f904c0"
   },
   "outputs": [],
   "source": [
    "val_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eBu0AHA5PdJB",
    "outputId": "094b0679-4abe-404c-d531-41a191538f0c"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import numpy as np\n",
    "\n",
    "# PHASE 1: MERGE THE EXACT CENTROID DATAFRAMES WITH QUANTUM PRECISION\n",
    "merged_centroids = pd.merge(\n",
    "    shop_centroids.assign(key=1),\n",
    "    val_centroids.assign(key=1),\n",
    "    on='key'\n",
    ").merge(\n",
    "    pref_centroids.assign(key=1),\n",
    "    on='key'\n",
    ").drop('key', axis=1)\n",
    "\n",
    "# PHASE 2: REMOVE CLUSTER COLUMNS WITH ABSOLUTE PRECISION\n",
    "cluster_cols = [col for col in merged_centroids.columns if 'cluster' in col.lower()]\n",
    "feature_cols = [col for col in merged_centroids.columns if col not in cluster_cols]\n",
    "\n",
    "# PHASE 3: COMPUTE LINKAGE MATRIX WITH HEISENBERG-GRADE PRECISION\n",
    "X = merged_centroids[feature_cols]\n",
    "Z = linkage(X, method='ward', metric='euclidean')\n",
    "\n",
    "# PHASE 4: QUANTUM-PRECISE DENDROGRAM VISUALIZATION\n",
    "plt.figure(figsize=(15, 10))\n",
    "dendrogram(Z, leaf_rotation=90)\n",
    "plt.title('Multi-Dimensional Hierarchical Clustering of Shop-Val-Pref Centroids')\n",
    "plt.xlabel('Centroid Combinations')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.axhline(y=5, color='r', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PHASE 5: ULTRA-PRECISE ELBOW ANALYSIS\n",
    "last_merge = Z[:, 2]\n",
    "acceleration = np.diff(last_merge, 2)\n",
    "elbow_idx = np.argmax(acceleration) + 2\n",
    "suggested_clusters = len(merged_centroids) - elbow_idx\n",
    "\n",
    "print(f\" QUANTUM ANALYSIS SUGGESTS {suggested_clusters} OPTIMAL CLUSTERS\")\n",
    "print(\"\\n CRITICAL VALIDATION METRICS:\")\n",
    "print(f\"Total centroid combinations: {len(merged_centroids)}\")\n",
    "print(f\"Features used in clustering: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "tZnDV2VugIRg",
    "outputId": "b2400e8c-d920-4808-d06d-21023f0ef4d2"
   },
   "outputs": [],
   "source": [
    "merged_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6YeVBXCgjTl"
   },
   "outputs": [],
   "source": [
    "# PHASE 1: MERGE ORIGINAL DFs - QUANTUM INTEGRITY MAINTAINED\n",
    "original_merged = pd.merge(\n",
    "    shop_clean,\n",
    "    val_clean,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    suffixes=('_shop', '_val')\n",
    ").merge(\n",
    "    pref_clean,\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ")\n",
    "\n",
    "# PHASE 2: CALCULATE CENTROIDS WITH HEISENBERG-GRADE PRECISION\n",
    "merged_centroids = original_merged.groupby(['cluster_shop', 'cluster_val', 'cluster']).mean()\n",
    "\n",
    "# PHASE 3: FEATURE SELECTION WITH QUANTUM VALIDATION\n",
    "feature_cols = [col for col in merged_centroids.columns\n",
    "               if 'cluster' not in col.lower()]\n",
    "X = merged_centroids[feature_cols]\n",
    "\n",
    "# PHASE 4: HIERARCHICAL CLUSTERING WITH PLANCK-LENGTH PRECISION\n",
    "Z = linkage(X, method='ward', metric='euclidean')\n",
    "\n",
    "#  CRITICAL SCIENTIFIC CORRECTION HERE \n",
    "final_clusters = fcluster(Z, t=3, criterion='maxclust')  # Changed from 'distance' to 'maxclust' and t=4 to t=5\n",
    "\n",
    "# PHASE 5: MAPPING CREATION WITH QUANTUM INTEGRITY\n",
    "cluster_mapping = dict(zip(merged_centroids.index, final_clusters))\n",
    "\n",
    "# PHASE 6: CLUSTER ASSIGNMENT WITH RELATIVISTIC PRECISION\n",
    "original_merged['final_cluster'] = original_merged.apply(\n",
    "    lambda row: cluster_mapping.get(\n",
    "        (row['cluster_shop'], row['cluster_val'], row['cluster']),\n",
    "        -1  # Quantum error detection maintained\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#  CRITICAL SCIENTIFIC VALIDATION\n",
    "assert original_merged['final_cluster'].nunique() <= 5, \"QUANTUM ERROR: Too many clusters detected!\"\n",
    "assert -1 not in original_merged['final_cluster'].unique(), \"QUANTUM ERROR: Unmapped clusters detected!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "DlbE2voGh2P_",
    "outputId": "a268bfca-2004-464e-8450-6558e08c4d8b"
   },
   "outputs": [],
   "source": [
    "original_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrJwDpsNFi1o"
   },
   "source": [
    "# Profiling with unused / categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88giH4zAlnx6",
    "outputId": "912e7210-f7ba-47e3-d325-0eba1f8ae8ac"
   },
   "outputs": [],
   "source": [
    "# PHASE 1: QUANTUM VALIDATION OF CATEGORICAL COLUMNS\n",
    "assert all(col in df.columns for col in categorical_cols), \"QUANTUM ERROR: Missing categorical columns!\"\n",
    "\n",
    "# PHASE 1: LOCATE CATEGORICAL COLUMNS IN ORIGINAL DATA\n",
    "categorical_data = df[['last_promo', 'payment_method',\n",
    "                      'customer_region_0', 'customer_region_1',\n",
    "                      'customer_region_2', 'customer_region_3']]\n",
    "\n",
    "# PHASE 2: QUANTUM-PRECISE MERGE WITH ORIGINAL_MERGED\n",
    "enhanced_merged = pd.merge(\n",
    "    original_merged,\n",
    "    categorical_data,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    validate='1:1'  # CRITICAL: Ensures perfect 1-to-1 mapping\n",
    ")\n",
    "\n",
    "# PHASE 3: HEISENBERG-GRADE VALIDATION\n",
    "assert len(enhanced_merged) == len(original_merged), \"CRITICAL ERROR: Row count mismatch!\"\n",
    "assert all(col in enhanced_merged.columns for col in categorical_cols), \"CRITICAL ERROR: Missing columns!\"\n",
    "\n",
    "# PHASE 4: DISPLAY QUANTUM-VERIFIED RESULTS\n",
    "print(\" QUANTUM-PRECISE VALIDATION METRICS:\")\n",
    "print(f\"Original rows: {len(original_merged)}\")\n",
    "print(f\"Enhanced rows: {len(enhanced_merged)}\")\n",
    "print(\"\\nCategorical Column Statistics:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col} unique values: {enhanced_merged[col].nunique()}\")\n",
    "    print(enhanced_merged[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "TjCBr6OxmdcI",
    "outputId": "2991e93e-1c3d-47e3-8192-8fffe58aef69"
   },
   "outputs": [],
   "source": [
    "enhanced_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRGdR9YoFgMb"
   },
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hpb-0GfliEUt",
    "outputId": "7b7489a2-7445-4e6e-85f1-c07af084a157"
   },
   "outputs": [],
   "source": [
    "# PHASE 1: QUANTUM-PRECISE FUNCTION DEFINITION\n",
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Multi-dimensional cluster profiling with quantum-grade precision.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "\n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2, figsize=figsize, squeeze=False)\n",
    "\n",
    "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
    "        # Quantum filtering\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "\n",
    "        # Heisenberg-precise centroid calculation\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "\n",
    "        # Relativistic visualization\n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=sns.color_palette(), ax=ax[0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1])\n",
    "\n",
    "        # Quantum layout optimization\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [f\"Cluster {i}\" for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight='heavy')\n",
    "        ax[0].legend(handles, cluster_labels)\n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(f\"Cluster Means - {len(handles)} Clusters\", fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=-20)\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(f\"Cluster Sizes - {len(handles)} Clusters\", fontsize=13)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profiling\", fontsize=23)\n",
    "    plt.show()\n",
    "\n",
    "# PHASE 2: PREPARE DATA WITH QUANTUM INTEGRITY\n",
    "metric_features = [col for col in enhanced_merged.columns\n",
    "                  if 'cluster' not in col.lower()]\n",
    "\n",
    "# PHASE 3: EXECUTE VISUALIZATION WITH HEISENBERG-GRADE PRECISION\n",
    "cluster_profiles(\n",
    "    df=enhanced_merged,\n",
    "    label_columns=['cluster_shop', 'cluster_val', 'cluster', 'final_cluster'],\n",
    "    figsize=(40, 32),\n",
    "    compar_titles=[\"Shopping Clusters\", \"Value Clusters\",\n",
    "                  \"Preference Clusters\", \"Final Hierarchical Clusters\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bJ7E6qFDlTWy",
    "outputId": "4752ed25-fc9c-48ae-8419-0e9fb48c7d91"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    r2_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def quantum_cluster_validation(shop_clean, val_clean, pref_clean, enhanced_merged, figsize=(20, 24)):\n",
    "    \"\"\"QUANTUM-PRECISE VALIDATION\"\"\"\n",
    "\n",
    "    def calculate_metrics(X, labels):\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        calinski = calinski_harabasz_score(X, labels)\n",
    "        davies = davies_bouldin_score(X, labels)\n",
    "\n",
    "        # Calculate SSE and R without dimensional mixing\n",
    "        sse = 0\n",
    "        cluster_centers = {}\n",
    "        cluster_sizes = {}\n",
    "\n",
    "        for cluster in np.unique(labels):\n",
    "            mask = labels == cluster\n",
    "            cluster_points = X[mask]\n",
    "            cluster_sizes[cluster] = len(cluster_points)\n",
    "            center = cluster_points.mean(axis=0)\n",
    "            cluster_centers[cluster] = center\n",
    "            sse += np.sum((cluster_points - center) ** 2)\n",
    "\n",
    "        total_center = X.mean(axis=0)\n",
    "        total_ss = np.sum((X - total_center) ** 2)\n",
    "        r2 = 1 - (sse / total_ss)\n",
    "\n",
    "        return {\n",
    "            'silhouette': silhouette,\n",
    "            'calinski': calinski,\n",
    "            'davies': davies,\n",
    "            'sse': sse,\n",
    "            'r2': r2,\n",
    "            'sizes': cluster_sizes\n",
    "        }\n",
    "\n",
    "    # Calculate metrics for each space SEPARATELY\n",
    "    results = {}\n",
    "\n",
    "    # Shop metrics\n",
    "    X_shop = shop_clean.drop('cluster', axis=1).values\n",
    "    results['shop'] = calculate_metrics(X_shop, shop_clean['cluster'])\n",
    "\n",
    "    # Value metrics\n",
    "    X_val = val_clean.drop('cluster', axis=1).values\n",
    "    results['value'] = calculate_metrics(X_val, val_clean['cluster'])\n",
    "\n",
    "    # Preference metrics\n",
    "    X_pref = pref_clean.drop('cluster', axis=1).values\n",
    "    results['preference'] = calculate_metrics(X_pref, pref_clean['cluster'])\n",
    "\n",
    "    # Final merged metrics\n",
    "    X_merged = enhanced_merged[[col for col in enhanced_merged if 'cluster' not in col]].values\n",
    "    results['final'] = calculate_metrics(X_merged, enhanced_merged['final_cluster'])\n",
    "\n",
    "    # Separate visualization function for safety\n",
    "    plot_validation_results(results, figsize)\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_validation_results(results, figsize):\n",
    "    \"\"\"QUANTUM-SAFE VISUALIZATION\"\"\"\n",
    "    fig, axes = plt.subplots(len(results), 2, figsize=figsize)\n",
    "    fig.suptitle(\"QUANTUM-GRADE CLUSTER VALIDATION METRICS\", y=0.95)\n",
    "\n",
    "    for idx, (name, metrics) in enumerate(results.items()):\n",
    "        # LEFT: Bar plot of metrics\n",
    "        ax1 = axes[idx, 0]\n",
    "        metric_values = [metrics['silhouette'],\n",
    "                        metrics['calinski']/max(1, metrics['calinski']),\n",
    "                        1/(1 + metrics['davies']),\n",
    "                        metrics['r2']]\n",
    "        metric_names = ['Silhouette', 'Calinski (norm)', 'Davies (inv)', 'R']\n",
    "\n",
    "        sns.barplot(x=metric_names, y=metric_values, ax=ax1)\n",
    "        ax1.set_title(f\"{name.title()} Metrics\")\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # RIGHT: Cluster size distribution\n",
    "        ax2 = axes[idx, 1]\n",
    "        sizes = pd.Series(metrics['sizes'])\n",
    "        sns.barplot(x=sizes.index, y=sizes.values, ax=ax2)\n",
    "        ax2.set_title(f\"{name.title()} Cluster Sizes\")\n",
    "\n",
    "        # Add metrics text\n",
    "        text = (\n",
    "            f\"Silhouette: {metrics['silhouette']:.3f}\\n\"\n",
    "            f\"Calinski: {metrics['calinski']:.0f}\\n\"\n",
    "            f\"Davies: {metrics['davies']:.3f}\\n\"\n",
    "            f\"R: {metrics['r2']:.3f}\"\n",
    "        )\n",
    "        ax2.text(1.05, 0.5, text, transform=ax2.transAxes,\n",
    "                bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return metrics\n",
    "\n",
    "# PHASE 1: EXECUTE QUANTUM VALIDATION\n",
    "validation_metrics = quantum_cluster_validation(\n",
    "    shop_clean=shop_clean,\n",
    "    val_clean=val_clean,\n",
    "    pref_clean=pref_clean,\n",
    "    enhanced_merged=enhanced_merged,\n",
    "    figsize=(20, 24)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o34fPo1YjtFf",
    "outputId": "0d2db572-e427-4b85-9518-3fe925e9b54c"
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "CmIOKAKIjrGo",
    "outputId": "dcb9adb2-136b-4f65-a68f-0318eaf70cce"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def quantum_clean_manifold_analysis(enhanced_merged, random_state=42):\n",
    "    \"\"\"\n",
    "    QUANTUM-PRECISE MANIFOLD ANALYSIS FOR PRE-CLEANED DATA\n",
    "    No outliers = Pure dimensional analysis with absolute precision\n",
    "    \"\"\"\n",
    "\n",
    "    # PHASE 1: PURE FEATURE EXTRACTION\n",
    "    feature_cols = [col for col in enhanced_merged.columns if 'cluster' not in col]\n",
    "    X = enhanced_merged[feature_cols].values\n",
    "    labels = enhanced_merged['final_cluster'].values\n",
    "\n",
    "    # PHASE 2: MANIFOLD LEARNING WITH THEORETICAL PHYSICS PRECISION\n",
    "    # PCA in clean space\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    var_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "    # Enhanced UMAP for clean data\n",
    "    umap_optimal = umap.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=30,     # Optimal for clean data\n",
    "        min_dist=0.3,       # Better separation for clean clusters\n",
    "        metric='euclidean', # Pure distance in clean space\n",
    "        random_state=random_state\n",
    "    )\n",
    "    X_umap = umap_optimal.fit_transform(X)\n",
    "\n",
    "    # PHASE 3: QUANTUM-PRECISE VISUALIZATION\n",
    "    # plt.style.use('seaborn')  # Enhanced aesthetics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # PCA with enhanced visualization\n",
    "    scatter1 = ax1.scatter(\n",
    "        X_pca[:, 0], X_pca[:, 1],\n",
    "        c=labels, cmap='viridis',\n",
    "        alpha=0.7, s=70,    # Increased visibility\n",
    "        edgecolor='white',  # Enhanced point definition\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    ax1.set_title(f'PCA Projection (Variance Explained: {var_ratio[0]:.2f}, {var_ratio[1]:.2f})',\n",
    "                  fontsize=12, pad=20)\n",
    "    ax1.set_xlabel('Principal Component 1', fontsize=10)\n",
    "    ax1.set_ylabel('Principal Component 2', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.2)\n",
    "\n",
    "    # UMAP with enhanced visualization\n",
    "    scatter2 = ax2.scatter(\n",
    "        X_umap[:, 0], X_umap[:, 1],\n",
    "        c=labels, cmap='viridis',\n",
    "        alpha=0.7, s=70,\n",
    "        edgecolor='white',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    ax2.set_title('UMAP Projection', fontsize=12, pad=20)\n",
    "    ax2.set_xlabel('UMAP Component 1', fontsize=10)\n",
    "    ax2.set_ylabel('UMAP Component 2', fontsize=10)\n",
    "    ax2.grid(True, alpha=0.2)\n",
    "\n",
    "    # Enhanced legends\n",
    "    for ax, scatter in [(ax1, scatter1), (ax2, scatter2)]:\n",
    "        legend = ax.legend(\n",
    "            *scatter.legend_elements(),\n",
    "            title=\"Clusters\",\n",
    "            title_fontsize=10,\n",
    "            bbox_to_anchor=(1.05, 1),\n",
    "            loc='upper left'\n",
    "        )\n",
    "        ax.add_artist(legend)\n",
    "\n",
    "    plt.tight_layout(w_pad=4)\n",
    "    plt.show()\n",
    "\n",
    "    # PHASE 4: QUANTUM-PRECISE ANALYSIS\n",
    "    print(\"\\nQUANTUM-GRADE MANIFOLD ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Feature contribution analysis\n",
    "    feature_importance = np.abs(pca.components_)\n",
    "    top_features_pc1 = np.argsort(feature_importance[0])[-5:]\n",
    "    top_features_pc2 = np.argsort(feature_importance[1])[-5:]\n",
    "\n",
    "    print(\"PCA VARIANCE EXPLANATION:\")\n",
    "    print(f\"PC1: {var_ratio[0]:.3f} ({var_ratio[0]*100:.1f}%)\")\n",
    "    print(f\"PC2: {var_ratio[1]:.3f} ({var_ratio[1]*100:.1f}%)\")\n",
    "    print(f\"Total: {sum(var_ratio):.3f} ({sum(var_ratio)*100:.1f}%)\")\n",
    "\n",
    "    print(\"\\nTOP CONTRIBUTING FEATURES:\")\n",
    "    print(\"PC1:\", [feature_cols[i] for i in top_features_pc1[::-1]])\n",
    "    print(\"PC2:\", [feature_cols[i] for i in top_features_pc2[::-1]])\n",
    "\n",
    "    return {\n",
    "        'pca_embedding': X_pca,\n",
    "        'umap_embedding': X_umap,\n",
    "        'pca_variance': var_ratio,\n",
    "        'feature_importance': {\n",
    "            'pc1': dict(zip(feature_cols, feature_importance[0])),\n",
    "            'pc2': dict(zip(feature_cols, feature_importance[1]))\n",
    "        }\n",
    "    }\n",
    "\n",
    "# EXECUTE QUANTUM-GRADE ANALYSIS\n",
    "manifold_results = quantum_clean_manifold_analysis(enhanced_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "GW9Ry8V5DmRG",
    "outputId": "db73ba9a-66f4-47a2-c92c-8a693c4e9519"
   },
   "outputs": [],
   "source": [
    "val_clean\n",
    "pref_clean\n",
    "shop_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYn_DtdplQqc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5eG8AfpFJE2"
   },
   "source": [
    "https://github.com/fpontejos/Data-Mining-23-24/blob/main/notebooks_solutions/lab14_cluster_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7avKfSYFewV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNbZBCPGF3Kg"
   },
   "source": [
    "# Assess feature importance and reclassify outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9h5p-6vF4u2"
   },
   "source": [
    "# Using a Decision Tree we get the normalized total reduction of the criterion (gini or entropy) brought by that feature (also known as Gini importance)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP2FIHhuUljq1Qv44uibX27",
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
